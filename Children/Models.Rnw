%==============================
\chapter{Spatial Models}
%==============================

In the preceding sections, we reviewed fundamental concepts of spatial econometrics, such as spatial dependency and spatial autocorrelation. This chapter advances our understanding by examining the formulation of spatial models.

In Section \ref{sec:taxonomy}, we establish a comprehensive taxonomy of spatial models, encompassing the Spatial Lag Model, Spatial Durbin Model, Spatial Error Model, and the Spatial Autocorrelation Model. Each model is motivated and illustrated with examples to provide a clear understanding.

Moving forward to Section \ref{sec:interpretation}, we explore the concept of ``spillover'' effects within the spatial model framework. Additionally, we delve into the interpretation of marginal effects, enhancing our ability to extract meaningful insights from spatial econometric analyses.

%*******************************
\section{Taxonomy of Models}\label{sec:taxonomy}
%*******************************

As demonstrated in the previous chapter, particularly in Section~\ref{sec:spatial_dependence}, traditional econometric methods are not equipped to handle spatial dependencies. The primary challenge arises from the fact that we often have more parameters than observations. However, by using the spatial weight matrix, we can address this issue by reducing the number of parameters to just one. This is achieved through the weighted average of the dependent variable values, $\vy$, in the neighborhood of unit $i$.

%-----------------------------------------------------------
\subsection{Spatial Lag Model}\index{Spatial lag model}
%-----------------------------------------------------------

Given the problem of insufficient degrees of freedom, a natural question arises: how can we model a situation where the dependent variable is influenced by spatially lagged values? Instead of relying on a full system of equations, we can capture the spatial dependence with a model that includes a spatially lagged dependent variable. Specifically, we model the spatial dependence as follows:
\begin{equation}\label{eq:slm_model_1}
y_i = \alpha +\rho \sum_{j = 1}^n w_{ij}y_j + \epsilon_i, \quad i = 1,\ldots,n,
\end{equation}
%
where $w_{ij}$ is the $(i,j)$th element of the spatial weight matrix $\mW$ matrix (refer to Definition \ref{def:W}). The variable $y_i$ denotes the dependent variable for spatial unit $i$, and $\sum_{j = 1}^n w_{ij} y_j$ is the weighted average of the dependent variable for the neighbors of $i$ (referred to as the spatial lag). The error term $\epsilon_i$ satisfies $\E(\epsilon_i) = 0$, and $\rho$ is the spatial autoregressive parameter that quantifies the strength of spatial interdependence. Specifically, a positive value of $\rho$ ($\rho > 0$) indicates positive spatial dependence, while a negative value ($\rho < 0$) suggests negative spatial dependence. If $\rho = 0$, the model reduces to the traditional linear regression model.

By introducing a spatially lagged variable, we explicitly account for spatial spillover effects, such as those arising from geographical proximity. This data-generating process is known as a \emph{Spatial Autoregressive Process}\index{Spatial autoregressive process} (SAR) or, equivalently, the \emph{Spatial Lag Model} (SLM). In the case where the model only includes the spatial lag and no explanatory variables, it is referred to as the pure Spatial Lag Model or SAR model.

\begin{figure}[ht]
	\caption{The SLM for two tegions}\label{figure:slm}
		\centering
		\begin{tikzpicture}
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (y1) at (-1,0) {$y_1$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (x1) at (-1,2) {$x_1$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (e1) at (-1,-2) {$\epsilon_1$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (y2) at (1, 0) {$y_2$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (x2) at (1,2) {$x_2$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (e2) at (1,-2) {$\epsilon_2$};
			
			\draw[->, solid] (x1) to (y1);
			\draw[->, solid] (x2) to (y2);
			\draw[->, solid] (e1) to (y1);
			\draw[->, solid] (e2) to (y2);
			\draw[dashed, ->] (y1) to[bend right] (y2);
			\draw[dashed, ->] (y2) to[bend right] (y1);

			% Legend
       \node (leyend) at (4, 0){
    \begin{tabular}{l@{: }l}
      \multicolumn{2}{c}{\textbf{Effects}}              \\
      $\rightarrow$                        & non-spatial effects    \\
      $\dashrightarrow$                     & spatial effects  \\
      \end{tabular}
      };
		\end{tikzpicture}
\end{figure}

Figure~\ref{figure:slm} visually illustrates the spatial autoregressive model from Equation~\eqref{eq:slm_model_1}, focusing on two regions. In this representation, the variables $(x_1, x_2)$ and the unobserved terms $(\epsilon_1, \epsilon_2)$ directly influence the dependent variables $y_1$ and $y_2$ in their respective regions. The spatial spillover effects are captured through the influence of $y_1$ on $y_2$ and vice versa. This structure reflects \emph{simultaneity} in the model, highlighting the presence of spatial autocorrelation and spillover effects.

It is important to have different notation forms for the SLM. In vector form, the model can be expressed as:
\begin{equation*}
y_i = \alpha + \rho \underset{(1\times n)}{\vw_i}^\top \underset{(n\times 1)}{\vy} + \epsilon_i, \quad i = 1,\ldots,n,
\end{equation*}
%
where $\vw_i$ is the $i$th row of $\mW$. Additionally, a comprehensive Spatial Lag Model (SLM) specification, considering covariates in matrix form, takes the shape:
\begin{equation}\label{eq:Sar}
\vy  =  \alpha \vones_n+ \rho \mW\vy + \mX\vbeta + \vepsi,
\end{equation}
%
where $\vy$ is a $n\times 1$ vector containing observations on the dependent variable, $\mX$ is an $n\times k$ matrix of observations on the explanatory variables; $\vbeta$ is the $k\times 1$ vector of parameters and $\alpha$ is the constant; and $\vones_n$ is a $n\times 1$ vector of ones.

%====================================
\subsection{Spatial Durbin Model}
%====================================

The Spatial Durbin Model (SDM) is defined as:\index{Spatial durbin model}
\begin{equation}\label{eq:Sdm}
\vy  =  \rho \mW\vy + \alpha \vones_n + \mX\vbeta + \mW\mX\vgamma + \vepsi.
\end{equation}
%
where $\vy$ is the dependent variable, $\rho$ is the spatial autoregressive parameter, $\mW$ is the spatial weights matrix, $\vones_n$ is an $n$-dimensional vector of ones to account for the intercept $\alpha$, $\mX$ is the matrix of explanatory variables, $\mW\mX$ represents the spatially lagged explanatory variables, and $\vepsi$ is the error term.

The SDM extends traditional spatial autoregressive models by including not only the spatially lagged dependent variable ($\mW\vy$) and explanatory variables ($\mX$) but also the spatially lagged explanatory variables ($\mW\mX$). This means that $\vy$ depends on the local (own-region) factors from $\mX$ as well as the same factors averaged over neighboring regions, introducing an additional layer of spatial dependence.

This concept is illustrated in Figure \ref{figure:sdm}. The diagram shows that Region 1 affects Region 2 not only through the spatially lagged dependent variable ($\vy$) but also through the spatially lagged independent and exogenous variables ($\vx$).

\begin{figure}[ht]
	\centering
	\caption{The SDM for Two Regions}\label{figure:sdm}	
		\begin{tikzpicture}
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (y1) at (-1,0) {$y_1$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (x1) at (-1,2) {$\vx_1$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (e1) at (-1,-2) {$\epsilon_1$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (y2) at (1, 0) {$y_2$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (x2) at (1,2) {$\vx_2$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (e2) at (1,-2) {$\epsilon_2$};

			\draw[->, solid] (x1) to (y1);
			\draw[->, solid] (x2) to (y2);
			\draw[->, solid] (e1) to (y1);
			\draw[->, solid] (e2) to (y2);
			\draw[dashed, ->] (y1) to[bend right] (y2);
			\draw[dashed, ->] (y2) to[bend right] (y1);
			\draw[dashed, ->] (x1) to (y2);
			\draw[dashed, ->] (x2) to (y1);
			% Legend
       \node (leyend) at (4, 0){
    \begin{tabular}{l@{: }l}
      \multicolumn{2}{c}{\textbf{Effects}}              \\
      $\rightarrow$                        & non-spatial effects    \\
      $\dashrightarrow$                     & spatial effects  \\
      \end{tabular}
      };
		\end{tikzpicture}
\end{figure}

Consider an example where $\vy$ represents air pollution levels in different regions. The term $\mW\vy$ implies that air pollution in Region 1 affects pollution in Region 2, and vice versa. If $\mX$ includes population density, the spatially lagged variable $\mW\mX$ captures the effect of population density in Region 1 (or Region 2) on air pollution in the neighboring region.

The SDM is particularly advantageous for calculating marginal effects, as it allows for the decomposition of direct, indirect, and total effects, which will be explored in later sections.

%====================================
\subsection{Spatial Error Model}\label{sec:tax_SEM}
%====================================

Another form of spatial dependence occurs when it operates through the error process, meaning that the errors from different regions exhibit spatial autocorrelation. The Spatial Error Model (SEM) is formulated as\index{Spatial error model}:
\begin{equation}\label{eq:sem_model_comp}
  \begin{aligned}
\vy &= \alpha \vones + \mX\vbeta + \vu, \\
\vu &= \lambda \mW\vu + \vepsi. \\
 \end{aligned}
\end{equation}
%
where $\lambda$ is the autoregressive parameter capturing the spatial dependence in the error lag $\mW\vu$, $\vepsi$ is i.i.d. noise, and $\mW$ is the spatial weights matrix. The parameter $\lambda$ distinguishes the spatial error dependence from the spatial autoregressive coefficient $\rho$ in spatial lag models.

Figure \ref{figure:sem} illustrates the SEM for two regions. The diagram highlights that the error terms ($\epsilon_1$ and $\epsilon_2$) of both regions are interconnected, and the spatial effect operates solely through this relationship.	

\begin{figure}[ht]
	\centering
	\caption{The SEM for two regions}\label{figure:sem}	
		\begin{tikzpicture}
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (y1) at (-1,0) {$y_1$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (x1) at (-1,2) {$\vx_1$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (e1) at (-1,-2) {$\epsilon_1$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (y2) at (1, 0) {$y_2$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (x2) at (1,2) {$\vx_2$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (e2) at (1,-2) {$\epsilon_2$};

			\draw[->, solid] (x1) to (y1);
			\draw[->, solid] (x2) to (y2);
			\draw[->, solid] (e1) to (y1);
			\draw[->, solid] (e2) to (y2);
			\draw[dashed, <->] (e1) to (e2);

			% Legend
       \node (leyend) at (4, 0){
    \begin{tabular}{l@{: }l}
      \multicolumn{2}{c}{\textbf{Effects}}              \\
      $\rightarrow$                        & non-spatial effects    \\
      $\dashrightarrow$                     & spatial effects  \\
      \end{tabular}
      };
		\end{tikzpicture}
\end{figure}

As noted by \cite{AnselinBera1998}, spatial error dependence can be viewed as a nuisance, with $\lambda$ acting as a nuisance parameter. This reflects spatial autocorrelation in measurement errors or in unaccounted variables that may influence the dependent variable. For instance, it may capture spillovers of omitted variables across spatial units.

Unlike models focusing on explicit spatial or social interaction processes, the SEM accommodates situations where the determinants of the dependent variable are spatially autocorrelated but not explicitly modeled. It can also represent scenarios where unobserved shocks exhibit a spatial pattern. This flexibility makes the SEM useful in dealing with spatially structured errors that arise from omitted variables or other latent spatial processes.
% The spatial diffusion of this model can be analyzed if we consider the reduced form equation. If the matrix $(\mI_n - \lambda\mW)$ is not singular, then \eqref{eq:sem_model_comp} can be written under the following reduced form\index{Spatial error model!reduced form}:
% 
% \begin{equation}
%   \vy =  \alpha \vones + \mX\vbeta + (\mI_n - \lambda\mW)^{-1}\vepsi.
% \end{equation}
% 
% This expression leads to a global spatial diffusion effect, but there is not spatial multiplier effect. The variance-covariance matrix is given by:
% 
% \begin{equation}
%   \begin{aligned}
% \E\left(\left.\vy\vy^\top\right|\mW, \mX\right)  & = \E\left(\left.\vepsi\vepsi^\top\right|\mW, \mX\right) \\
%                  & = \left(\mI_n- \lambda\mW\right)^{-1}\E\left(\left.\vepsi\vepsi^\top\right|\mW, \mX\right)\left(\mI_n- \lambda\mW^\top\right)^{-1}
% \end{aligned}
% \end{equation}

\begin{remark}
Interaction effects among the unobserved terms may also be interpreted to reflect a mechanism to correct rent-seeking politicians for unanticipated fiscal policy changes. See for example \cite{allers2005tax}.
\end{remark}

%=========================
\subsection{Spatial Autocorrelation Model}
%=========================

The Spatial Autocorrelation Model (SAC) is a more general framework that incorporates key features of both the spatial lag and spatial error models discussed earlier. Its structural representation is given by:
\begin{equation*}
  \begin{aligned}
    y_i & = \alpha + \rho    \sum_{j = 1}^n w_{ij} y_j + \sum_{k=1}^K x_{ik}\beta_k + u_i \\
    u_i & = \lambda \sum_{j = 1}^n m_{ij} u_j + \epsilon_i
  \end{aligned}
\end{equation*}
%
or more compactly in matrix form, 
\begin{equation*}
  \begin{aligned}
\vy &= \alpha \vones_n + \rho \mW\vy + \mX\vbeta + \vu, \\
\vu &= \lambda \mM\vu + \vepsi, \\
  \end{aligned}
\end{equation*}
%
where the matrix $\mW$ and $\mM$ are $n\times n$ spatial-weighting matrices.\footnote{This model is also known as SARAR(1, 1) model or Cliff-Ord models because of the impact that \cite{cliff1973spatial}  had on the subsequent literature. Note that SARAR(1, 1) is  a special case of the more general SARAR(p, q) model.} In this model, spatial interactions in the dependent variable and the disturbances are considered. As standard, the spatial weight matrices $\mW$ and $\mM$ are taken to be known and nonsthocastic. These matrices are part of the model definition, and in many applications, $\mM = \mW$. When $\rho = 0$, the model reduces to the SEM. When $\lambda = 0$ the model reduces to the SLM (SAR) specification. Setting $\rho = 0$ and $\lambda = 0$ causes the model to reduce to a linear regression model with exogenous variables. 

A broader taxonomy of spatial models is shown in Figure \ref{fig:taxonomy}. The most comprehensive model in this framework is the General Nesting Spatial Model (GNS or Manski's Model), which includes spatial dependence in the dependent variable, the explanatory variables, and the error term. Different restrictions on the GNS model yield various spatial model specifications, as detailed below.

\begin{figure}[h]
\caption{Taxonomy of spatial models}
\label{fig:taxonomy}
\centering
\begin{tikzpicture}[scale = 0.8]
			\node[rectangle, text  width = 5cm, fill = red!10, align = center] (gns) at (0,0) {\scriptsize General nesting spatial model \\
			\begin{eqnarray*}
			\vy &=& \rho\mW\vy + \mX\vbeta + \mW\mX\vgamma + \vu \\
			\vu &=& \lambda\mW\vu + \vepsi
			\end{eqnarray*}};
			
			\node[rectangle, text  width = 3cm, fill = red!10, align = center] (sac) at (6,4) {\scriptsize  SAC \\
			\begin{eqnarray*}
			\vy &=& \rho\mW\vy + \mX\vbeta + \vu \\
			\vu &=& \lambda\mW\vu + \vepsi
			\end{eqnarray*}};
			
			\node[rectangle, text  width = 3cm, fill = red!10, align = center] (sdm) at (6,0) {\scriptsize  Spatial durbin model \\
			\begin{equation*}
			\vy = \rho\mW\vy + \mX\vbeta + \mW\mX\vgamma + \vepsi
			\end{equation*}};
			
			\node[rectangle, text  width = 3cm, fill = red!10, align = center] (sde) at (6,-4) {\scriptsize Spatial durbin error model \\
			\begin{eqnarray*}
			\vy &=& \mX\vbeta + \mW\mX\vgamma + \vu \\
			\vu &=& \lambda\mW\vu + \vepsi
			\end{eqnarray*}};
			
			\node[rectangle, text  width = 3cm, fill = red!10, align = center] (slm) at (12,4) {\scriptsize Spatial Lag Model \\
			\begin{equation*}
			\vy = \rho\mW\vy + \mX\vbeta + \vepsi
			\end{equation*}};
			
			\node[rectangle, text  width = 3cm, fill = red!10, align = center] (slx) at (12,0) {\scriptsize SLX \\
			\begin{equation*}
			\vy = \mX\vbeta + \mW\mX\vgamma + \vepsi
			\end{equation*}};
			
			\node[rectangle, text  width = 3cm, fill = red!10, align = center] (sem) at (12,-4) {\scriptsize Spatial error model \\
			\begin{eqnarray*}
			\vy &=& \mX\vbeta + \vu \\
			\vu &=& \lambda\mW\vu + \vepsi
			\end{eqnarray*}};
			
			\node[rectangle, text  width = 2cm, fill = red!10, align = center] (ols) at (16,0) {\scriptsize OLS \\
			\begin{equation*}
			\vy = \mX\vbeta + \vepsi
			\end{equation*}};

			\draw (gns) -- (sac) node [->, solid, midway, above, sloped] {\scriptsize $\vgamma = 0$};
			\draw (gns) -- (sdm) node [->, solid, midway, above, sloped] {\scriptsize $\lambda = 0$};
			\draw (gns) -- (sde) node [->, solid, midway, above, sloped] {\scriptsize $\rho = 0$};
			\draw (sac) -- (slm) node [->, solid, midway, above, sloped] {\scriptsize $\lambda = 0$};
			\draw (sac) -- (sem) node [->, solid, near end, above, sloped] {\scriptsize $\rho = 0$};
			\draw (sdm) -- (slx) node [->, solid, near end, above, sloped] {\scriptsize $\rho = 0$};
			\draw (sde) -- (slx) node [->, solid, near start, above, sloped] {\scriptsize $\lambda = 0$};
			\draw (sdm) -- (slm) node [->, solid, midway, above, sloped] {\scriptsize $\gamma = 0$};
			\draw (sdm) -- (sem) node [->, solid, midway, above, sloped] {\scriptsize $\gamma = -\rho\beta$};
			\draw (sde) -- (sem) node [->, solid, midway, above, sloped] {\scriptsize $\gamma = 0$};
			\draw (slm) -- (ols) node [->, solid, midway, above, sloped] {\scriptsize $\rho = 0$};
			\draw (slx) -- (ols) node [->, solid, midway, above, sloped] {\scriptsize $\gamma = 0$};
			\draw (sem) -- (ols) node [->, solid, midway, above, sloped] {\scriptsize $\lambda = 0$};
\end{tikzpicture}
\end{figure}

Starting with the GNS model:

\begin{itemize}
  \item Imposing the restriction $\vgamma = \vzeros$ leads to the SAC model that includes both a spatial lag for the dependent variable and spatial lag for the error term, but excludes the influence of the spatially lagged explanatory variables. 
  \item Imposing the restriction $\lambda = 0$ leads to the SDM.
  \item Imposing the restriction $\rho = 0$ leads to the Spatial Durbin Error Model (SDEM). 
\end{itemize}

Starting with the SDM:

\begin{itemize}
  \item The so-called common factor parameter restrictions $(\vgamma = -\rho\vbeta)$ yields the spatial error regression model (SEM) specification that assumes that externalities across spatial unites are mostly a nuisance spatial dependence problem caused by the regional transmission of random shocks. 
  \item Imposing the restriction $\gamma = 0$ leads to the spatial lag model (SLM), whereas the restriction $\rho = 0$ results in a least-squares spatially lagged $\mX$ regression model (labeled SLX) that assumes independence between regions in the dependent variable, but includes characteristics from neighboring regions in the form of spatially lagged explanatory variables.  
\end{itemize}

Finally, if $\rho = \lambda = 0$ and $\vtheta = 0$, then we obtain the traditional linear regression model. 

%-------------------------------------------------------------------------
\section{Reduced Form and Parameter Space}\index{Reduced form!Spatial lag model}
%-------------------------------------------------------------------------

An important distinction in the context of spatial models is the difference between structural and reduced form model. The reduced form expresses the endogenous variables as functions of the exogenous variables. In contrast, the structural form is the `behavioral model' that defines the relationship between the vaiables in the system.  

For example, Equation \eqref{eq:Sar} represents the structural model for the SLM, where both exogenous and endogenous variables are related to the dependent variable $\vy$. This form reflects the theoretical framework that captures the underlying relationship between the variables. 

However, it is often more insightful to consider how the dependent variable is generated, which is referred to as the \textbf{data generating process} (DGP). The DGP represents the empirical process that gives rise to the observed data. By solving the structural model in Equation \eqref{eq:Sar} for the endogenous variables, $\vy$, yields the reduced-form model. 

The implied DGP or ``reduced form equation'' for the SLM given in Equation \eqref{eq:Sar} is:
\begin{equation}\label{eq:Sar_dgp}
\vy  =  \left(\mI_n - \rho \mW\right)^{-1}\left(\alpha \vones_n + \mX\vbeta\right) + \left(\mI_n - \rho \mW\right)^{-1}\vepsi,
\end{equation}
%
which shows that any spatially lagged dependent variable is no longer present on the right-hand side. This equation illustrate the simultaneous nature of the spatial autoregressive process.

\begin{remark}
The \textbf{reduced form} of a system of equations is the result of solving the system for the \textbf{endogenous variables}. This gives the latter as functions of the exogenous variables, if any. For example, the general expression of a structural form is $f(\vy, \mX, \vepsi) = \vzeros$, whereas the reduced form of this model is given by $\vy = g(\mX, \vepsi)$, with $g$ as function. 
\end{remark}

Without restrictions on $(\mI_n-\rho\mW)$---and $\left(\alpha \vones_n + \mX\vbeta\right)$---the coefficients cannot be identified from data. In other words, to identify the true coefficients from data, we need $(\mI_n-\rho\mW)$ to be invertible. From linear algebra, a matrix $\mA$ is invertible if $\det(\mA)\neq 0$. Thus, for the reduced from to be valid we require that $\det(\mI_n -\rho\mW)\neq 0$. The question then becomes: for which values of $\rho$ lead to a non-singular $(\mI_n -\rho\mW)$? 

%For \textbf{symmetric} matrices, the compact open interval for $\rho \in \left( \omega_{min}^{-1}, \omega_{max}^{-1}\right)$ will lead to a symmetric positive definite $(\mI_n -\rho\mW)$, where $\omega_{min}$ and $\omega_{max}$ are the minimum and maximum eigen value of $\mW$, respectively. This gives rise to the following Theorem:

The following Lemma from \cite{kelejian2010specification} establishes a bound for $\rho$:

%---------------------------
\begin{lemma}[Bounds for $\rho$ \citep{kelejian2010specification}]\label{lemma:bound-rho-kp}
Let $\tau$ denote the spectral radius of $\mW$, defined as:
\begin{equation*}
\tau = \max\left\lbrace |\omega_1|, \ldots, |\omega_n| \right\rbrace,
\end{equation*}
%
where $\omega_1, \ldots, \omega_n$ denote the eigenvalues of $\mW$. Then $\mI_n - \rho\mW$ is nonsingular for all $\rho$ in the interval $(-1/\tau, 1/\tau)$. 
\end{lemma}
%---------------------------

When $\rho$ is restricted to the interval $(-1/\tau, 1/\tau)$, all eigenvalues of $\rho\mW$ are guaranteed to have absolute values less than one. A related result, frequently used in the literature, is presented below:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma}[Invertibility]\index{Weight matrix!Invertibility}\index{eigen values}\label{lemma:inveribility}
Let $\mW$ be a weighting matrix, such that $w_{ii} = 0$ for all $i = 1,\ldots,n$, and assume that all of the roots of $\mW$ are real. Assume also that $\mW$ is not row normalized. Let $\omega_{min}$ and $\omega_{max}$ be the minimum and maximum eigen value of $\mW$. Assume also that $\omega_{max} > 0$ and $\omega_{min} < 0$. Then $\left(\mI_n - \rho\mW\right)$ is nonsingular for all:
\begin{equation*}
  \omega_{min}^{-1} < \rho < \omega_{max}^{-1}
\end{equation*}
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As \cite{kelejian2010specification} explain, Lemma \ref{lemma:inveribility} holds only when all eigenvalues of $\mW$ are real. However, non-symmetric matrices typically have complex eigenvalues, making Lemma \ref{lemma:bound-rho-kp} more general.

To facilitate interpretation, $\mW$ is often normalized so that each row sums to unity. This row-normalization ensures nonnegative weights between 0 and 1, which allows $\mW$ to be interpreted as a row-stochastic matrix that averages neighboring values. According to Theorem \ref{teo:eigen_values} (Eigenvalues of Row-Stochastic Matrix), the eigenvalues of such a row-stochastic matrix $\mW$ have absolute values less than or equal to one. This result, combined with Lemma \ref{lemma:bound-rho-kp}, implies $\rho \in (1/\omega_{\textrm{min}}, 1)$.

However, interpreting $\rho$ as a conventional correlation coefficient between $\vy$ and its spatial lag $\mW\vy$ can be misleading. The parameter space of $\rho$ depends on the normalization of $\mW$. For instance, standardization methods other than row-normalization may yield different bounds for $\rho$. 

%----------------------------
\begin{lemma}[Invertibility of Row-Normalized $\mW$ matrix]\label{lemma:invert-W-row}
If $\mW$ is row-normalized, then  $\left(\mI_n - \rho\mW\right)^{-1}$ exists for all $\left|\rho \right| < 1$
\end{lemma}
%----------------------------

Despite its popularity, row-normalization has drawbacks. As discussed in Section \ref{sec:spatial_lag_var}, it alters the internal structure of $\mW$, complicating inter-row comparisons. To address this, scalar normalization (multiplying $\mW$ by a constant $a$) can be applied. This approach preserves the relationships between rows and removes unit-of-measure effects. Let:
\begin{equation*}
  \begin{aligned}
    a & = \min \left\lbrace r, c \right\rbrace \\
    r & = \max_i \sum_j \left|w_{ij}\right|\quad \mbox{maximal row sum of the absolute values} \\
    c & = \max_j \sum_i \left|w_{ij}\right|\quad \mbox{maximal column sum of the absolute values}.
  \end{aligned}
\end{equation*}

Then, assuming that the elements of $\mW$ are nonegative, $(\mI_n - \rho\mW)$ will be nonsingular for all $\left|\rho\right| < 1/a$. Note that this normalization has the advantage of ensuring that the resulting spatial weights, $w_{ij}$, are all between 0 and 1, and hence can still be interpreted as relative influence intensities. This could be taken as the parameter space.\index{Parameter space} 

This is an important result because a model which has a weighting matrix which is not row normalized can always be normalized in such a way that the inverse needed to solve the model will exists in an easily established region. 


\begin{remark}
For further details on normalizing $\mW$ and the parameter space of $\rho$ see \citet[][section 2.4]{elhorst2014spatial} and \citet[][section 2.2]{kelejian2010specification}
\end{remark}

When $\mI_n - \rho\mW$ is nonsingular, it can be expressed as an infinite series, commonly referred to as the Leontief expansion:

%----------------------
\begin{lemma}[Leontief Expansion]\label{lemma:Leontief}\index{Leontief expansion}
If $\mI - \rho \mW$ is nonsingular, then
	\begin{equation*}
	(\mI - \rho \mW)^{-1} = \sum_{i = 0} ^{\infty}(\rho \mW)^{i}
	\end{equation*}
\end{lemma}
%----------------------

Using Lemma \ref{lemma:Leontief} (Leontief Expansion), the reduced form for the SLM in Equation \eqref{eq:Sar_dgp} can be written as:
\begin{equation}\label{eq:infinite_series_slm}
  \begin{aligned}
      \vy  = &\left(\mI_n + \rho\mW + \rho^2\mW^2 + \ldots \right)\left(\alpha \vones_n + \mX\vbeta\right) + \left(\mI_n + \rho\mW + \rho^2\mW^2 + \ldots\right)\vepsi, \\
           = & \alpha\vones_n + \rho\mW\vones_n\alpha + \rho^2\mW^2\vones_n\alpha + \ldots + \mX\vbeta + \rho\mW\mX\vbeta + \rho^2\mW^2\mX\vbeta + \ldots \\
          & + \vepsi + \rho\mW\vepsi + \rho^2\mW^2\vepsi.
  \end{aligned}
\end{equation}

Expression \eqref{eq:infinite_series_slm} can be simplified since the infinite series:
\begin{equation*}
\alpha\vones_n + \rho\mW\vones_n\alpha + \rho^2\mW^2\vones_\alpha + \ldots \to \frac{\vones_n\alpha}{\left(1 - \rho\right)},
\end{equation*}
%
since $\alpha$ is a scalar, the parameter $\left|\rho\right|  < 1$, and $\mW$ is row-stochastic. By definition $\mW\vones_n = \vones_n$ and therefore $\mW\mW\vones_n = \mW\vones_n = \vones$. Consequently, $\mW^l\vones_n = \vones_n$ for $l\geq 0$ (recall that $\mW^0 = \mI_n$). This allows us to write:
\begin{equation*}
\vy = \frac{1}{(1-\rho)}\vones_n\alpha + \mX\vbeta + \rho\mW\mX\vbeta + \rho^2\mW^2\mX\vbeta + \ldots + \vepsi + \rho\mW\vepsi + \rho^2\mW^2\vepsi + \ldots
\end{equation*}

This expansion reveals two effects: a multiplier effect affecting the explanatory variables and a spatial diffusion effect affecting the error terms. With respect to the explanatory variables, this expression means that, on average, the value of $\vy$ at one location $i$ is not only explained by the values of the explanatory variables associated to this location but also by those associated to all other locations (neighbors or not) via the inverse spatial transformation $\left(\mI_n - \rho \mW\right)^{-1}$. This spatial multiplier effect decreases with distance. This can be seen if we consider the powers of $\mW$ in the series expansion of $\left(\mI_n - \rho \mW\right)^{-1}$.\index{Multiplier effect} 

With respect to the error process, this expression means that a random (unobserved) shock in a location $i$ not only affects the value of $y$ in this location but also has an impact on the values of $y$ in all other locations via the same spatial inverse transformation. To see this, recall that $\mW^2$ will reflect second-order contiguous neighbors, those that are neighbors to the first-order neighbors (review Section \ref{sec:HSO}). Since the neighbor of the neighbor (second-order neighbor) to an observation $i$ includes observation $i$ itself, $\mW^2$ has positive elements on the diagonal when each observations has at least one neighbor. That is, higher-order spatial lags can lead to a connectivity relation for an observations $i$ such that $\mW\vepsi$ will extract observations from the vector $\vepsi$ that point back to the observation $i$ itself. This implies that there exists a simultaneous feedback. This diffusion effect also declines with distance. We will explore this mechanism more deeply in Section \ref{sec:interpretation}.

Considering the reduced form Equation (\ref{eq:Sar_dgp}), we might be able to find the mean and variance-covariance matrix of the complete system as function of exogenous variables. The expectation is given by:
\begin{equation}\label{eq:expected_value_slm}
  \begin{aligned}
\E(\vy|\mX,\mW) & = \E\left[\left.\left(\mI_n - \rho \mW\right)^{-1}\left(\alpha \vones_n + \mX\vbeta\right) + \left(\mI_n - \rho \mW\right)^{-1}\vepsi\right|\mX,\mW\right] \\
                & = \left(\mI_n - \rho \mW\right)^{-1}\left(\alpha \vones_n + \mX\vbeta\right).
\end{aligned}
\end{equation}

From Equation \eqref{eq:Sar_dgp}, we derive the variance-covariance matrix of $\vy$:
\begin{equation*}
  \begin{aligned}
    \var\left(\left.\vy\right|\mW, \mX\right) & = \E\left(\left.\vy\vy^\top\right|\mW,\mX\right) \\ 
                               & = \left(\mI_n - \rho\mW\right)^{-1}\E\left(\left.\vepsi\vepsi^\top\right|\mW, \mX\right)\left(\mI_n- \rho\mW^\top\right)
  \end{aligned}
\end{equation*}

This $n\times n$ variance-covariance matrix is full, which implies that each location is correlated with every other location in the system. However, this correlation decreases with distance. Since we have not assumed anything about the error variance, we can say that $\E\left(\left.\vepsi\vepsi^\top\right|\mW, \mX\right)$ is a full matrix, say $\mOmega_{\epsilon}$. This covers the possibility of heteroskedasticity, spatial autocorrelation, or both. In absence of either of these complications, the variance matrix simplifies to the usual $\sigma^2\mI_n$.

\begin{example}[County homicide rates in US]
In the criminology literature there has been a great emphasis of spatial diffusion of crime. The idea is that criminal violence may spread geographically via a diffusion process. For example, some researchers suggests that certain social processes such as illegal drug markets and gang rivalries may be important for explaining the pattern and mechanisms of the spread of homicides \citep{cohen1999diffusion}. 

In particular, empirical literature has focused on homicide rates and their determinants using the following OLS specification:

\begin{equation*}
y_i = \vx_i^\top\vbeta + \epsilon_i\quad i = 1, ..., n,
\end{equation*}
%
where $y_i$ is the homicide rate in spatial unit $i$ and $\vx_i$ is a $k\times 1$ set of covariates that explain homicide rates across spatial units. However, this model does not allow capturing the idea of spatial diffusion and spatial effects of homicide rates. For example, \cite{baller2001structural}, after rejecting the null hypothesis of spatial randomness on homicide rates, propose (among other spatial models) the following SLM process for modeling homicide rates using a county-level data for the decennial years in the 1960 to 1990 time period:

\begin{equation*}
\vy  =  \alpha \vones_n + \rho \mW\vy + \mX\vbeta + \epsilon,
\end{equation*}
%
where $\vy$ is the homicide rates for the US counties, $\mX$ includes a deprivation, population density, median age, the unemployment rate, percent divorced, and a Southern dummy variable based on census definitions.  As explained by \cite{baller2001structural}, if homicides rates are determined solely by the structural factors included in the $\mX$ matrix, there should be no spatial patterning of homicide beyond that created by socio-demographic similarities of geographically proximate counties. If this is the case, once all $x_k$ are included in the model, the spatial relationship between $y_i$ and $y_j$ will become nonsignificant. This implies that $\rho = 0$.

This is the model most compatible with common notions of diffusion processes because it implies an influence of neighbors' homicide rates that is not simply an artifact of measured or unmeasured independent variables. Rather, homicide events in one place actually increase the likelihood of homicides in nearby locales. 
\end{example}

%-------------------------------------------------------------------------
\subsection{Eigenvalues in R}
%-------------------------------------------------------------------------

Computing eigenvalues is a crucial step in the estimation of spatial models, particularly when using Maximum Likelihood procedures. In this section, we demonstrate how to compute the eigenvalues of a spatial weight matrix $\mW$ in R, along with some of their properties.

First, we create an artificial spatial weight matrix using functions from the \pkg{spdep} package:

<<Wl.queen>>=
# Create a queen W matrix
library("spdep")
Wl.queen <- cell2nb(3, 3, type = "queen")
summary(Wl.queen)
@

The function \code{cell2nb} from \pkg{spdep} package generates a list of neighbors for a grid of cells. Here, we define a grid with 3 rows and 3 columns, resulting in a total of 9 regions. Since \code{type = "queen"} is specified, the neighbors include those sharing either an edge or a vertex, consistent with a regular grid spatial structure. The resulting object is of class \code{nb}, representing a list of neighbors.

To proceed with matrix computations, we convert this \code{nb} object into a spatial weight matrix of class \code{matrix}:
<<W.queen>>=
# From nb to matrix
W.queen <- nb2mat(Wl.queen)
round(W.queen, 3)
@

The resulting matrix is row-normalized, meaning that the sum of each row equals one. We can verify this as follows:
<<check-rowsums>>=
# Compute row-sums
rowSums(W.queen)
@

Additionally, note that this matrix is not symmetric. We can confirm this property using:
<<is.sym>>=
# Check whether the matrix is symmetric
is.sym <- all(W.queen == t(W.queen))
is.sym
@

The \code{eigen} function in R computes the eigenvalues and eigenvectors of a matrix. It returns a named list with two components: \code{values} (the eigenvalues) and \code{vectors} (the eigenvectors). To illustrate, we compute the eigenvalues of the previously defined spatial weight matrix:

<<eigen-values>>=
# Obtain eigenvalues
values <- eigen(W.queen, symmetric = is.sym)$values
values
range(values)
@

The eigenvalues are returned in decreasing order. For the spatial weight matrix \code{W.queen}, all eigenvalues are real. The minimum eigenvalue is approximately -0.45, while the maximum eigenvalue is 1, corroborating the fact that if $\mW$ is row-normalized, then $|\vomega| \leq 1$.

Since $\mW$ is row-normalized, and  according to Lemma \ref{lemma:invert-W-row}, $\mI_n - \rho\mW$ is invertible if $|\rho| < 1$. Note that if $\rho = 1$, then the matrix is nonsingular as shown in the following lines
<<check.inv>>=
solve(diag(9) - 1 * W.queen)
@


Next, we compute the eigenvalues for a rook spatial weight matrix:
<<eigen-rook>>=
W.rook <- nb2mat(cell2nb(3, 3, type = "rook"))
is.sym <- all(W.rook == t(W.rook))
values <- eigen(W.rook, symmetric = is.sym)$values
range(values)
@

For the rook spatial weight matrix, the minimum eigenvalue is -1, and the maximum eigenvalue is 1. This highlights a difference in the spectral properties of rook versus queen spatial weight matrices.

Another approach to constructing spatial weight matrices, particularly for Monte Carlo studies, is the $k$-ahead and $k$-behind criterion in a circular world, as proposed by \citet{kelejian1999generalized}. In this framework, each spatial unit is assumed to have $k$ neighbors ahead of it and $k$ neighbors behind it in the sample order. For example, we can create such a matrix with $k = 2$ using the \code{circular} function from the \pkg{sphet} package:
<<w-circ, message = FALSE>>=
library("sphet")
W.cir <- nb2mat(circular(3, 3, 2))
W.cir
all(W.cir == t(W.cir))
values <- eigen(W.cir, symmetric = all(W.cir == t(W.cir)))$values
values
range(values)
@

In this example, each spatial unit has $2k$ neighbors (two ahead and two behind). The eigenvalues for this matrix can provide insights into the connectivity and spatial structure defined by this criterion.

%-------------------------------------------------------------------------
\subsection{Generate spatial DGP in R}
%-------------------------------------------------------------------------

To construct a DGP of a spatial model, we first define the reduced form of the model. Consider the SLM with
\begin{equation*}
\vy = \left(\mI_n - \rho\mW\right)^{-1}\left(\beta_0\vones_n + \beta_1\vx + \vepsi\right), 
\end{equation*}
%
where $\vones_n$ is an $n\times 1$ vector of ones and the elements of the vector $\vx$ are normally distributed with mean 0 and standard deviation 1. The parameter $\beta_0$ is set to 1, while $\beta_1 = 2$. The error term is normally distributed with mean zero and standard deviation one. The spatial autoregressive parameter is set to $\rho = 0.6$. 
The following lines create this DGP for a sample size of $n = 49$ using a circular spatial weight matrix:
<<create-dgp>>=
# Set the random seed
set.seed(1)

# True parameters
b0  <- 1
b1  <- 2
rho <- 0.6

# Sample size and W matrix
n     <- 49
W.cir <- nb2mat(circular(sqrt(n), sqrt(n), 3))
                
# Generate random variables
x <- rnorm(n, mean = 0, sd = 1)
epsilon <- rnorm(n, mean = 0, sd = 1)

# Generate dependent variable
y <- solve(diag(n) -  rho * W.cir) %*% (b0 + b1 * x + epsilon)
@

%==============================================
\section{Motivation of Spatial Models}\label{sec:motivation}
%==============================================

\subsection{SLM as a Long-run Equilibrium}\label{sec:SLM_lre}

A Spatial Lag Model (SLM) can be interpreted as a system of simultaneous dependencies over time that converges to a new steady-state equilibrium, even when using a cross-sectional dataset \citep{lesage2010introduction}. To demonstrate this, consider the vector of the dependent variable at time $t$, denoted by $\vy_t$. Assume that this variable is determined by a spatial autoregressive process, where the current value depends on the spatially lagged values of the dependent variable from neighboring observations. This introduces a time lag in the average values of the neighboring dependent variables observed during the previous period, $\mW\vy_{t-1}$. We can also include the current period's own-region characteristics $\mX_t$ in the model. If the characteristics of regions remain relatively fixed over time, we can express $\mX_t = \mX$.

For illustration, consider a model where pollution is the dependent variable $\vy_t$, which depends on the past period's pollution values from neighboring regions, $\mW\vy_{t-1}$. The appropriate model in this case would be:
\begin{equation}\label{eq:slm_with_time}
\vy_t = \rho \mW\vy_{t-1} + \mX\vbeta + \vepsi_t.
\end{equation}

We can substitute $\vy_{t-1}$ from the right-hand side of equation \eqref{eq:slm_with_time} as:
\begin{equation*}
\vy_{t-1} = \rho \mW\vy_{t-2} + \mX\vbeta + \vepsi_{t-1},
\end{equation*}
%
resulting in:
\begin{equation}\label{eq:recur_2}
\begin{aligned}
\vy_t  = & \mX\vbeta +\rho\mW\left(\mX\vbeta + \rho \mW\vy_{t-2} + \vepsi_{t-1}\right) + \vepsi_t\\
=& \mX\vbeta +\rho\mW\mX\vbeta + \rho^2\mW^2\vy_{t-2} + \epsilon_t + \rho\mW\vepsi_{t-1}.
\end{aligned}
\end{equation}

By recursively substituting past values of $\vy_{t-r}$ on the right-hand side of Equation \eqref{eq:recur_2} over $q$ periods, we obtain:
\begin{equation*}
  \begin{aligned}
    \vy_t &= \left(\mI_n + \rho\mW + \rho^2\mW^2 + \ldots + \rho^{q-1}\mW^{q-1}\right)\mX\vbeta + \rho^q\mW^q\vy_{t-q} + \vu,\\
    \vu  & = \vepsi_{t} + \rho\mW\vepsi_{t-1} + \rho^2\mW^2\vepsi_{t-2} + \ldots + \rho^{q-1}\mW^{q-1}\vepsi_{t - (q-1)}. 
  \end{aligned}
\end{equation*}

The expected value of this spatial process is:
\begin{equation}\label{eq:expectation_time}
  \E\left(\vy_t\right) = \left(\mI_n + \rho\mW + \rho^2\mW^2 + \ldots + \rho^{q-1}\mW^{q-1}\right)\mX\vbeta + \rho^q\mW^q\vy_{t-q},
\end{equation}
%
where we use the fact that $\E(\vepsi_{t-r}) = 0, r = 0, \ldots, q-1$, which also implies that $\E(\vu) = \vzeros$. 

Finally, taking the limit of equation (\ref{eq:expectation_time}) as $q \to \infty$, we obtain:
\begin{equation}\label{eq:steady_state}
\lim_{q\to \infty} \E\left(\vy_t\right) = \left(\mI_n - \rho\mW\right)^{-1}\mX\vbeta.
\end{equation}

Note that the magnitude of $\rho^q\mW^q\vy_{t-q}$ tends to zero as $q$ increases, under the assumption that $\lvert \rho \rvert < 1$ and that $\mW$ is row-stochastic (i.e., it has a principal eigenvalue of 1).

Equation \eqref{eq:steady_state} indicates that we can interpret the observed cross-sectional relationship as the outcome or expectation of a long-run equilibrium or steady state. This provides a dynamic motivation for the data generating process (DGP) of the cross-sectional SLM, which is commonly used in spatial regression modeling. In other words, a cross-sectional SLM relationship can emerge from the time-dependent decisions of economic agents located at different points in space, where decisions are influenced by neighboring regions.


\subsection{SEM and Omitted Variables Motivation}

Consider the following process:
\begin{equation*}
\vy = \vx\beta + \vz\theta,
\end{equation*}
%
where $\vx$ and $\vz$ are \textbf{uncorrelated} vectors of dimension $n\times 1$, and the vector $\vz$ follows the following spatial autoregressive process:
\begin{eqnarray*}
  \vz &=& \rho \mW\vz + \vr \\
  \vz &=& \left(\mI_n - \rho\mW\right)^{-1}\vr
\end{eqnarray*}
%
where $\vr \sim \rN(0, \sigma^2_{\epsilon}\mI_n)$. Examples of $\vz$ are culture, social capital, or neighborhood prestige. 

If $\vz$ is not observed, then:
\begin{equation}\label{eq:sem_omited_moti}
  \begin{aligned}
    \vy & = \vx\beta + \vu \\
    \vu & = \left(\mI_n - \rho\mW\right)^{-1}\vepsi
  \end{aligned}
\end{equation}
%
where $\vepsi = \theta\vr$. Then, we have the DGP for the SEM.

\subsection{SDM and Omitted Variables Motivation}

Now suppose that $\mX$ and $\vepsi$ from (\ref{eq:sem_omited_moti}) are correlated, given by the following process:
\begin{equation}\label{eq:cor_x_epsi}
\begin{aligned}
  \vepsi & = \vx \gamma + \vv \\
  \vv    & \sim \rN(0, \sigma^2\mI_n) 
\end{aligned}
\end{equation}
%
where the scalar parameters $\gamma$ and $\sigma^2$ govern the strength of the relationship between $\mX$ and $\vz = (\mI_n-\rho\mW)^{-1}\vr$. Inserting (\ref{eq:cor_x_epsi}) into (\ref{eq:sem_omited_moti}), we obtain:
\begin{equation}
  \begin{aligned}
    \vy & = \vx\beta + \left(\mI_n - \rho\mW\right)^{-1}\vepsi \\
        & = \vx\beta + \left(\mI_n - \rho\mW\right)^{-1}\left(\vx \gamma + \vv\right) \\
        & = \vx\beta + \left(\mI_n - \rho\mW\right)^{-1}\vx \gamma + \left(\mI_n - \rho\mW\right)^{-1}\vv \\
       \left(\mI_n - \rho\mW\right)\vy & = \left(\mI_n - \rho\mW\right)\vx\beta  + \vv \\
       \vy & = \rho\mW\vy + \vx\left(\beta + \gamma\right) + \mW\vx(-\rho\beta) + \vv
  \end{aligned}
\end{equation}

This is the Spatial Durbin Model (SDM), which includes a spatial lag of the dependent variable $\vy$, as well as the explanatory variables $\vx$.


%==============================================
\section{Interpreting Spatial Models}\label{sec:interpretation}
%==============================================

\subsection{Measuring Spillovers}

A central concern in regional science is the measurement of spatial spillovers. In a spatial context, spillovers can be defined as the impacts that changes in one region have on neighboring regions \citep{LeSage2014}. Examples of spatial spillovers include:
\begin{itemize}
  \item Changes in the tax rate in one region may influence tax rate decisions in neighboring regions, a phenomenon known as tax mimicking or yardstick competition among local governments. 
  \item Home improvements made by one homeowner may increase the selling prices of nearby homes.
  \item Innovations by university researchers may diffuse to nearby firms. 
  \item Air or water pollution generated in one region may spill over to neighboring regions.  
\end{itemize}

The models discussed in the previous section can be used to formally define spatial spillovers and, more importantly, to estimate their quantitative magnitude and test their statistical significance. However, it is important to distinguish between global and local spillovers, a distinction explored in \cite{anselin2003spatial} and \cite{LeSage2014}.

We begin by formally defining global spillovers:
%------------------------------------------------------
\begin{definition}[Global Spillovers]\index{Spillover effects!Global spillovers}
	 Global spillovers occur when changes in a characteristic of one region affect the outcomes of all other regions. This includes impacts on the region itself, as changes can propagate through neighboring regions and back to the original region (feedback). Specifically, global spillovers influence not only direct neighbors but also neighbors of neighbors, neighbors of neighbors of neighbors, and so on. 
\end{definition}
%-----------------------------------------

The endogenous interactions produced by global spillovers lead to a scenario where changes in one region trigger a sequence of adjustments across potentially all regions in the sample, ultimately resulting in a new long-run steady-state equilibrium \citep{lesage2014regional}. As \cite{lesage2014regional} explains, global spillovers may arise from interactions between local policies. For example: \emph{``It seems plausible that changes in the levels of public assistance (cigarette taxes) in state $A$ would lead neighboring states (e.g., state $B$) to adjust their levels of assistance (taxes), which in turn triggers a feedback response from state $A$, and also responses from states $C$ that are neighbors to state $B$, and so on.''}

The following definition describes local spillovers:

%------------------------------------------------------
 \begin{definition}[Local Spillovers]\index{Spillover effects!Local spillovers}
 	Local spillovers occur when the impact is confined to nearby or immediate neighbors, with the effect diminishing before it reaches regions further removed, such as neighbors of neighbors.
 \end{definition}	
%------------------------------------------------------

As indicated by these definitions, the key difference between global and local spillovers is that feedback or endogenous interactions are only possible in the case of global spillovers.

%-------------------------------------------------------------------------
\subsection{Marginal Effects}\index{Spillover effects!Marginal effects}
%-------------------------------------------------------------------------

Mathematically, the notion of spillover can be thought as the derivative $\partial y_i/ \partial x_j$. This represents how changes in an explanatory variable in region $i$ influence the dependent variable in another region $j\neq i$.

As an illustration, consider the SDM, which can be re-written as:
\begin{equation*}
\begin{aligned}
(\mI_n - \rho\mW)\vy & = \mX\vbeta + \mW\mX\vtheta + \vepsi, \\
\vy & =   (\mI_n - \rho\mW)^{-1}\mX\vbeta + (\mI_n - \rho\mW)^{-1}\mW\mX\vtheta + (\mI_n - \rho\mW)^{-1}\vepsi, \\
\vy & =   \mA(\mW)^{-1}\mX\vbeta + \mA(\mW)^{-1}\mW\mX\vtheta + \mA(\mW)^{-1}\vepsi,\quad \mbox{since $\mA(\mW) = (\mI_n - \rho\mW)^{-1}$}\\
\vy & =   \mA(\mW)^{-1}\left(\mX\vbeta + \mW\mX\vtheta\right) + \mA(\mW)^{-1}\vepsi, \\
\vy & =   \sum_{r=1}^K\mA(\mW)^{-1}\left(\mI_n\beta_r + \mW\theta_r\right)\vx_r + \mA(\mW)^{-1}\vepsi, \\
\underbrace{\vy}_{(n\times 1)} & =   \sum_{r=1}^K\underbrace{\mS_r(\mW)}_{(n\times n)}\underbrace{\vx_r}_{n\times 1} + \underbrace{\mA(\mW)^{-1}}_{(n\times n)}\underbrace{\vepsi}_{n\times 1}
\end{aligned}
\end{equation*}
%
where $\mS_r = \mA(\mW)^{-1}\left(\mI_n\beta_r + \mW\theta_r\right)$, and
\begin{equation*}
 \vx_r = \begin{pmatrix}
          x_{r1} \\
          x_{r2} \\
          \vdots \\
          x_{rn}
        \end{pmatrix}.
\end{equation*}

Assuming that $\E(\epsilon_i) = 0$, then :
\begin{equation}\label{eq:system_expected}
\begin{pmatrix}
\E(y_1) \\ \E(y_2) \\ \vdots \\ \E(y_n)
\end{pmatrix}
=
\sum_{r=1}^K\begin{pmatrix}
\mS_r(\mW)_{11} & \mS_r(\mW)_{12} & \hdots & \mS_r(\mW)_{1n} \\
\mS_r(\mW)_{21} & \mS_r(\mW)_{22} & \hdots & \mS_r(\mW)_{2n} \\
\vdots & \vdots & \ddots & \vdots \\ 
\mS_r(\mW)_{n1} & \mS_r(\mW)_{n2} & \hdots & \mS_r(\mW)_{nn} 
\end{pmatrix}
\begin{pmatrix}
x_{1r} \\ x_{2r} \\ \vdots \\ x_{nr} 
\end{pmatrix}.
\end{equation}

For the dependent variable for spatial unit $i$, Equation (\ref{eq:system_expected}) would be:
\begin{equation}
\E(y_i) = \sum_{r=1}^k\left[\mS_r(\mW)_{i1}x_{1r} + \mS_r(\mW)_{i2}x_{2r} + \ldots + \mS_r(\mW)_{in}x_{nr}\right].
\end{equation}

So, the impact on the expected value of location $i$ given a change in the explanatory variable $x_r$ in location $j$ is\index{Spillover effects!Indirect effects}
\begin{equation}\label{eq:indirect_impact}
\frac{\partial \E(y_i)}{\partial x_{jr}} = \mS_r(\mW)_{ij}
\end{equation}
%
where $\mS_r(\mW)_{ij}$ is this equation represents the $i,j$th element of the matrix $\mS_r(\mW)$. This result implies that, unlike the OLS model, a change in some variable in certain region will potentially affect the expected value of the dependent variable in all other regions. Given this characteristic, this type of effect is known as \textbf{indirect effect}.

The impact of the expected value of region $i$, given a change in certain variable for the same region is given by\index{Spillover effects!Direct effects}
\begin{equation}\label{eq:direct_impact}
\frac{\partial \E(y_i)}{\partial x_{ir}} = \mS_r(\mW)_{ii}.
\end{equation}

This impact includes the \textbf{effect of feedback loops} where observation $i$ affects observation $j$ and observation $j$ also affects observation $i$: a change in $x_{ir}$ will affect the expected value of dependent variable in $i$, then will pass through the neighbors of $i$ and back to the region itself. To shed more light on this, let us write the all the marginal effects in matrix notation as follows:
\begin{equation}\label{eq:matrix_marginal_effects}
\begin{aligned}
 \underset{(n \times n)}{\begin{pmatrix}
  \frac{\partial \E(\vy)}{\partial x_{1r}} & \frac{\partial \E(\vy)}{\partial x_{2r}} & \hdots & \frac{\partial \E(\vy)}{\partial x_{nr}} 
   \end{pmatrix}} & = 
  \begin{pmatrix}
  \frac{\partial \E(y_1)}{\partial x_{1r}} & \frac{\partial \E(y_1)}{\partial x_{2r}} & \hdots & \frac{\partial \E(y_1)}{\partial x_{nr}} \\
  \frac{\partial \E(y_2)}{\partial x_{1r}} & \frac{\partial \E(y_2)}{\partial x_{2r}} & \hdots & \frac{\partial \E(y_2)}{\partial x_{nr}} \\
  \vdots & \vdots & \ddots & \vdots \\
  \frac{\partial \E(y_n)}{\partial x_{1r}} & \frac{\partial \E(y_n)}{\partial x_{2r}} & \hdots & \frac{\partial \E(y_n)}{\partial x_{nr}} 
  \end{pmatrix} \\
  & = \mA(\mW)^{-1}\left(\mI_n\beta_r + \mW\theta_r\right) = \mS_r(\mW) \\
  & = (\mI_n - \rho\mW)^{-1}
  \begin{pmatrix}
    \beta_r  & w_{12} \theta_r  & \hdots & w_{1n}\theta_r \\
    w_{21} \theta_r  & \beta_r   & \hdots & w_{2n}\theta_r \\
    \vdots & \vdots & \ddots & \vdots \\
    w_{n1} \theta_r  & w_{n2} \theta_r  & \hdots  & \beta_r \\
  \end{pmatrix}
\end{aligned}  
\end{equation}

This expression is somewhat difficult to understand. To provide a better understanding we follow \cite{elhorst2010applied} and consider a model with 3 regions arranged linearly\footnote{Unit 1 is neighbor of unit 2, unit 2 is a neighbor of both units 1 and 3, and unit 3 is a neighbor of unit 2.} with the following matrices:
\begin{equation}\label{eq:w_mat_ex_spill}
  \mW = \begin{pmatrix}
          0      & 1 & 0 \\
          w_{21} & 0 & w_{23} \\
          0      & 1 & 0
        \end{pmatrix}
\end{equation}
%
and
\begin{equation}\label{eq:w_Imat_ex_spill}
  \mA(\mW)^{-1} = \frac{1}{1 - \rho^2}
  \begin{pmatrix}
          1 - w_{23}\rho^2     & \rho & \rho^2 w_{23}\\
          \rho w_{21} & 1 & \rho w_{23} \\
          \rho^2 w_{21}      & \rho & 1 - w_{21}\rho^2 
        \end{pmatrix}
\end{equation}
%
where $w_{12} = w_{31}= 1$ since units 1 and 3 have only one neighbor, and $w_{21} + w_{23} = 1$, so we explicitly consider a row-standardized matrix.  Substituting Equations (\ref{eq:w_mat_ex_spill}) and (\ref{eq:w_Imat_ex_spill}) into Equation (\ref{eq:matrix_marginal_effects}) we get:
\begin{equation*}
  \begin{pmatrix}
  \frac{\partial \E(\vy)}{\partial x_{1r}} & \frac{\partial \E(\vy)}{\partial x_{2r}} & \frac{\partial \E(\vy)}{\partial x_{3r}} 
   \end{pmatrix} = \frac{1}{1 - \rho^2}
   \begin{pmatrix}
    \left(1 -  w_{23}\rho^2\right)\beta_r + \left(w_{21}\rho\right) \theta_r & \rho \beta_r + \theta_r & \left(w_{23}\rho^2\right)\beta_r + (\rho w_{23})\theta_r \\
    (w_{21}\rho)\beta_r + w_{21}\theta_r & \beta_r + \rho \theta_r & (w_{23}\rho)\beta_r + w_{23}\theta_r \\
    (w_{21}\rho^2)\beta_r + (w_{21}\rho)\theta_r & \rho \beta_r + \theta_r & (1 -  w_{21}\rho^2)\beta_r + (w_{23}\rho)\theta_r
   \end{pmatrix}
\end{equation*}

Every diagonal element of this matrix represents a direct effect. Consequently, indirect effect do not occur if both $\rho = 0$ and $\theta_k = 0$, since all non-diagonal elements will then be zero. Another important insight is that direct and indirect effects are different for different spatial units in the sample. Direct effects are different because the diagonal elements of the matrix $(\mI_n - \rho\mW)^{-1}$ are different for different units, provided that $\rho \neq 0$. Indirect effects are different because both the non-diagonal elements of the matrix $(\mI_n - \rho\mW)^{-1}$ and of the matrix $\mW$ are different for different units, provided that $\rho \neq 0$ and/or $\theta_k\neq 0$. Finally, note that indirect effects that occur if $\theta_k\neq 0$ are \textbf{local effects}, whereas indirect effects that occur if $\rho\neq 0$ are \textbf{global effects}. 

%-----------------------------------------------------------------
\subsubsection{Summary Measures}\label{sec:summary-measures}
%-----------------------------------------------------------------

In general, the change of each variable in each region implies $n^2$ potential marginal effects.  If we have $K$ variables in our model, this implies $K\times n^2$ potential measures. Even for small values of $n$ and $K$, it may already be rather difficult to report these results compactly. To overcome this problem, \citet[][p. 36-37]{lesage2010introduction} propose the following scalar summary measures: 

%-------------------------------------------------------------
\begin{definition}[Average Direct Impact]\label{def:ADI}
Let $\mS_r = \mA(\mW)^{-1}\left(\mI_n\beta_r + \mW\theta_r\right)$ for variable $r$. The impact of changes in the $i$th observation of $x_r$, which is denoted $x_{ir}$, on $y_i$ could be summarized by measuring the average $S_r(\mW)_{ii}$, which equals
	\begin{equation*}
		\mbox{ADI} = \frac{1}{n}\tr\left(\mS_r(\mW)\right)
	\end{equation*}
\end{definition}
%-------------------------------------------------------------

Averaging over the direct impact associated with all observations $i$ is similar in spirit to typical regression coefficient interpretations that represent average response of the dependent to independent variables over the sample of observations. 	

%----------------------------------------------------
\begin{definition}[Average Total Impact to an Observation]\label{def:ATIT}
	Let $\mS_r = \mA(\mW)^{-1}\left(\mI_n\beta_r + \mW\theta_r\right)$ for variable $r$. The sum across the $i$th row of $\mS_r(\mW)$ would be represent the total impact on individual observation $y_i$ resulting from changing the $r$th explanatory variable by the same amount across all $n$ observations. There are $n$ of these sums given by the column vector $\vc_r = \mS_r(\mW)\vones_n$, so an average of these total impacts is:
	\begin{equation}
		\mbox{ATIT} = \frac{1}{n} \vones_n'\vc_r
		\end{equation} 
\end{definition}
%----------------------------------------------------

%----------------------------------------------------
\begin{definition}[Average Total Impact from an Observation]\label{def:ATIF}
	Let $\mS_r = \mA(\mW)^{-1}\left(\mI_n\beta_r + \mW\theta_r\right)$ for variable $r$. The sum down the $j$th column of $\mS_r(\mW)$ would yield the total impact over all $y_i$ from changing the $r$th explanatory variable by an amount in the $j$th observation. There are $n$ of these sums given by the row vector $\vr_r = \vones_n'\mS_{r}(\mW)$, so an average of these total impacts is:
\begin{equation}
				\mbox{ATIF} = \frac{1}{n} \vr_r\vones_n
				\end{equation} 
\end{definition}
%----------------------------------------------------

The definition \ref{def:ATIF} relates how changes in a single observation $j$ influences all observations. In contrast, definition \ref{def:ATIT} considers how changes in all observations influences a single observation $i$. In both cases, averaging over all $n$ observations, leads to the same numerical result. The implication of this interesting result is that the \textbf{average total impact} is the average of all derivatives of $y_i$ with respect to $x_{jr}$ for any $i, j$. 

Therefore:
\begin{eqnarray}
\bar{M}(r)_{\mbox{direct}} & = & n^{-1}\tr\left(\mS_r(\mW)\right) \\
\bar{M}(r)_{\mbox{total}} & = & n^{-1}\vones_n'\mS_r(\mW)\vones_n \\
\bar{M}(r)_{\mbox{indirect}} & = & \bar{M}(r)_{\mbox{total}} - \bar{M}(r)_{\mbox{direct}}
\end{eqnarray}	


Given our example above, we obtain a direct effect of:
\begin{equation*}
  \frac{(3- \rho^2)}{3(1 - \rho^2)}\beta_k + \frac{2p}{3(1-\rho^2)}\theta_k,
\end{equation*}
%
and an indirect effect of
\begin{equation*}
  \frac{3\rho + \rho^2}{(3(1-\rho^2))}\beta_k + \frac{3 + \rho}{3(1- \rho^2)}\theta_k.
\end{equation*}

Unfortunately, since every application will have its own unique number of observations $n$ and spatial weight matrix $(\mW)$, these formulae cannot be generalized. 


\begin{example}[The effect of number of workers on commuting times]\label{example:commuting-Kirby}
\citet{kirby2009changes} use a Spatial Durbin Model (SDM) specification to examine changes in the (logged) number of workers in U.S. census tracts with commuting times exceeding 45 minutes one way, between 1990 and 2000. (See also the related discussion in Section \ref{sec:lesage-example}.) This investigation is motivated by the observation that the percentage of U.S. workers with these long commute times increased from 12.5\% in 1990 to 15.4\% in 2000, representing a rise of over 10\%. 

When selecting the appropriate model, the authors identify two key dynamics:
\begin{itemize}
    \item \textbf{Global spillover impacts}: Congestion effects from increased long-distance commuters in one part of a metropolitan area likely affect travel times across the entire roadway network.
    \item \textbf{Feedback effects}: Congestion from commuting decisions in one tract spills over into neighboring tracts, creating reciprocal feedback effects that further influence congestion in the originating tract.
\end{itemize}

These observations led the authors to specify the following SDM:
\begin{equation*}
    \vy = \rho \mW \vy + \alpha \vones_n + \mX \vbeta + \mW\mX \vtheta + \vepsi,
\end{equation*}
where:
\begin{itemize}
    \item \(\vy\): Logged number of workers with long commute times.
    \item \(\mX\): Variables capturing household location decisions, including age, gender, income distribution, and geographical characteristics of the tract.
    \item \(\mW \mX\): The corresponding variables for neighboring tracts, incorporating spatial interactions.
    \item \(\rho\): Spatial autoregressive parameter capturing dependence in the dependent variable.
    \item \(\mW\): Spatial weight matrix reflecting neighborhood relationships.
\end{itemize}

Using this model, the authors compare \textbf{direct}, \textbf{indirect}, and \textbf{total effects} estimates for the years 1990 and 2000. Their analysis highlights that demographic factors, particularly age and gender distributions, explain much of the variation in long commute times during this period.
 
Based on a comparison of \textbf{direct, indirect} and \textbf{total effects} estimates form the 1990 and 2000 models, they conclude that the suite of variables reflecting the age and gender distribution of population in the tracts represents the primary explanation for changes in the number of workers with long commute times between 1990 and 2000. The spillover impacts of the number of employed females in the 1990 model was positive suggesting that more employed females in a tract produced an increase in long commute times for neighboring tract commuters. In contrast, for the 2000 model, spillovers associated with employed females were negative, so that more employed females in a tract reduced long commute times for workers located in neighboring tracts. 
\end{example}


\begin{example}[Effect of pollution on housing price]\label{example:pollution-kim}
\cite{kim2003measuring} use a spatial-lag hedonic model in order to assess the direct and indirect effect of quality air on housing price.  The main model is the following:
\begin{equation*}
\vp = \rho\mW\vp + \mX_1\vbeta_1 + \mX_2\vbeta_2 + \mX_3\vbeta_3 + \vepsi,
\end{equation*}
%
where $\vp$ is the vector of housing prices, $\rho$ is a spatial autocorrelation parameter, $\mW$ is the $n\times n$ spatial weight matrix, $\mX_1$ is a matrix with observations on structural characteristics, $\mX_2$ is a matrix with observations on neighborhood characteristics, and $\mX_3$ is a matrix with observations on environmental quality (SO$_2$ and NO$_x$). 

The marginal implicit price (marginal benefit) of the hedonic equation is derived as
\begin{equation*}
\begin{pmatrix}
  \frac{\partial \E(\vp)}{\partial x_{1r}} & \frac{\partial \E(\vp)}{\partial x_{2r}} & \hdots & \frac{\partial \E(\vp)}{\partial x_{nr}} 
   \end{pmatrix} = \mA(\mW)^{-1}\mI_n\beta_r\quad \mbox{where}\quad \mA(\mW)^{-1} = (\mI_n - \rho\mW)^{-1}
\end{equation*}

Focusing on the first row the interpretation if the following: the housing price of location $i$ is not only affected by a marginal change air quality of location $i$ but also is affected by marginal changes of air quality in other locations. That is, the total impact of a change in air quality on housing price at location $i$ is the sum of the direct impacts $\partial p_1/\partial x_{1k}$ plus induced impacts $\sum_{i = 2}^n \partial p_1 / \partial x_{ik}$ (See our Definition \ref{def:ATIT}). 

An important point evidenced by \cite{kim2003measuring} is that, if  the row-sums of $\mW$ is less than or equal to one and $\rho$ in the proper parameter space, i.e., $\rho < 1$, then the total average effect can be computed as $\beta_r / (1-\rho)$. To see this note that
\begin{equation}\label{eq:total_effect_simple}
  \begin{aligned}
    n^{-1}\vones^\top\mS_r(\mW)\vones & = n^{-1}\vones^\top\left[\mA(\mW)^{-1}\left(\mI\beta_r\right)\right]\vones \\
                                  & = n^{-1}\vones^\top\left[(\mI_n - \rho\mW)^{-1}\right]\left(\mI\beta_r\right)\vones \\
                                  & =   n^{-1}\vones^\top\left[\mI_n + \rho\mW + \rho^2\mW^2 + \ldots \right]\left(\mI\beta_r\right)\vones \quad\mbox{using Lemma \ref{lemma:Leontief}} \\
                                  & = n^{-1} \vones^\top\left[\mI_n\beta_r + \rho\mW\beta_r + \rho^2\mW^2\beta_r + \ldots \right]\vones \\
                                  & = n^{-1} \vones^\top\left[\mI_n\vones\beta_r + \rho\mW\vones\beta_r + \rho^2\mW(\mW\vones)\beta_r + \rho^3\mW\mW(\mW\vones) \right] \\
                                  & = n^{-1} \vones^\top\left[\beta_r\vones + \rho\beta_r\vones + \rho^2\beta_r\vones + \rho^3\beta_r\vones + \ldots \right]\quad \because \mW^l\vones = \vones \\
                                  & = n^{-1} \vones^\top\left[\beta_r + \rho\beta_r + \rho^2\beta_r + \rho^3\beta_r + \ldots \right]\vones\\
                                  & = n^{-1} \left[\beta_r + \rho\beta_r + \rho^2\beta_r + \rho^3 + \ldots \right] \vones^\top\vones\\
                                  & = n^{-1} \left[\beta_r + \rho\beta_r + \rho^2\beta_r + \rho^3\beta_r + \ldots \right] n \\
                                  & = \frac{\beta_r}{(1 - \rho)}
  \end{aligned}
\end{equation}

The model is estimated in a semi-log functional form, therefore the estimated coefficients can be interpreted as semi-elasticities. In particular, note that the elasticity for SO$_2$ is given by:
\begin{equation}
  \begin{aligned}
    \epsilon_{SO_2} & = \left(\frac{SO_2}{p}\right)\left(\frac{d p}{d SO_2}\right) \\
                      & = \left(\frac{SO_2}{p}\right)\left(\frac{\beta_r}{(1 - \rho)}\cdot p\right)\quad \mbox{since the model is log-lin} \\
                      & = \frac{\beta_r}{(1 - \rho)}\cdot SO_2
  \end{aligned}
\end{equation}

Using the estimated $\widehat{\rho} =  0.549$ and replacing SO$_2$ by its mean value they obtain that the elasticity of housing price from a given small change in air quality is about $0.348 \approx 4\%$. The marginal benefits per household of a permanent 4\% improvement in air quality using $\beta_{SO_2}(\mI_n - \rho\mW)^{-1}\vp$ is about \$2333 (1.43\% of mean house value) for owners. 
\end{example}

\begin{example}[Human capital and labor productivity]
\cite{fischer2009impact} analyze the role of human capital in explaining labor productivity variation among European region. In particular they estimate the following model:

\begin{equation*}
\vy = \rho\mW\vy + \mX\vbeta + \mW\mX\vgamma +\vepsi
\end{equation*}
%
where $\vy$ is the vector of observations on the (log of) labor productivity level at the end of the sample period (2004) and $\mX$ contains (the log of) labor productivity and human capital at the beginning of the sample period (1995). The parameter $\rho$ is expected to be positive indicating that regional productivity levels are positively related to a linear combination of neighboring regions' productivity. The parameter vector $\vgamma$ captures two types of spatial externalities:spatial effects working through the level of labor productivity and spatial effects working through the level of human capital, both at the beginning of the sample period. 

The estimated parameter of the spatial autoregressive parameter is $\widehat{\rho} = 0.664$ providing evidence for the existence of significant spatial effects working through the dependent variable. 

The mean direct impact for the human capital is 0.1317, whereas the indirect impact is -0.1968. They interpret the indirect impact in two ways. First, they argue that the indirect impact reflects how a change in the human capital level of all regions by some constant would impact the labor productivity of a typical region (observation). The sign of the estimated mean indirect impact implies that an increase in the initial level of human capital of all other regions would decrease the productivity level of a typical region. This indirect impact takes into account the fact that the change in initial human capital level negatively impacts other regions' labor productivity, which in turn negatively influences our typical region's labor productivity due to the presence of positive spatial dependence on neighboring regions' labor productivity levels. 

Second \cite{fischer2009impact} measure the cumulative impact of a change in region's $i$ initial level of human capital averaged over all other regions.  The impact from changing a single region's initial level of human capital on each of the other region's labor productivity is small, but cumulatively the impact measures -0.1968.
\end{example}

\begin{remark}
A very good paper for those interesting in making the connection between global/local spillovers and different spatial model specifications is \cite{lesage2014regional}. This is a must-read paper. 
\end{remark}

%%==============================================
%\section{Motivation of Spatial Models}
%==============================================

%\subsection{A Time-Dependence Motivation}

%Economic agents often make current period decisions that are influenced by the behavior of other agents in previous periods. For example, local governments might set tax rates observing rates set by neighboring region in previous time periods.\footnote{This section is heavily based on \cite{lesage2010introduction}} Although the tax rates were set over time by the cross-section of regions representing our sample, the observed cross-sectional tax rates would exhibit a pattern of spatial dependence. 

%Consider the dependent variable vector at time $t$, denoted $\vy_t$, determined using a spatial autoregressive scheme that depends on space-time lagged values of the dependent variable from observations. 

%=================================================================
\subsection{Partitioning Global Effects Estimates Over Space}\label{sec:partitioning-effects}
%=================================================================

It should bear in mind that these scalar summary measures of impact reflect how these changes would work thought the simultaneous dependence system over time to culminate in a new steady state equilibrium. Therefore, they should be considered as those impacts that would take place once all regions reach their equilibrium after the initial change in the variable of interest (See our discussion in Section \ref{sec:SLM_lre}). However one could track the cumulative effects as the impacts pass through neighbors, neighbors of neighbors and so on. 

\begin{remark}
Cross-sectional observations could be viewed as reflecting a (comparative static) slice at one point in time of a long-run steady-sate equilibrium relationship, and the partial derivatives viewed as reflecting a comparative static analysis of changes that represent new steady-state relationship that would arise \citep{lesage2014regional}.
\end{remark}

Intuition tell us that impacts arising from a change in the explanatory variables will influence low-order neighbors more than higher-order neighbors. Therefore, we would expect a decline in the impacts' magnitude as we move from lower- to higher-order neighbors. To get a better idea of this process is necessary to consider the matrix $\mS_r(\mW)$ and recognize, by Lemma \ref{lemma:Leontief}, that this matrix can be expressed as a linear combination of power of the weight matrix $\mW$. In particular, recall that if $\mW$ is a row standardized matrix such that $\rho \in (-1, 1)$, then by Lemma \ref{lemma:Leontief}:

\begin{equation}
  \begin{pmatrix}
  \frac{\partial \E(\vy)}{\partial x_{1r}} & \frac{\partial \E(\vy)}{\partial x_{2r}} & \hdots & \frac{\partial \E(\vy)}{\partial x_{nr}} 
   \end{pmatrix} \approx \left(\mI_n  + \rho \mW + \rho^2\mW^2 + \rho^3\mW^3 + ... + \rho^l\mW^l\right)\mI_n\beta_r  
\end{equation}
    
This expression allow us to observe the impact associated with each power of $\mW$, where these powers corresponds to the observation themselves (zero-order), immediate neighbors (first-order), neighbors of neighbors (second-order), and so on. Using this expansion we could account for both the cumulative effects as marginal and total direct, indirect associated with different order of neighbors.

% %==============================================
% \section{Predictors for Spatial Models}
% %==============================================
% 
% \cite{kelejian2007relative} consider different information sets and define predictors as conditional means based on these information sets. Consider the SAC (SARAR) model:
% 
% \begin{equation}
%   \begin{aligned}
%     \vy & = \rho\mW\vy + \mX\vbeta + \vu, \\
%     \vu & = \lambda\mW\vu + \vepsi. 
%   \end{aligned}
% \end{equation}
% 
% Thus, the reduced form equation is:
% 
% \begin{equation}
% \vy = \left(\mI_n -\rho \mW\right)^{-1}\mX\vbeta + \left(\mI_n -\rho \mW\right)^{-1}\left(\mI_n -\lambda \mW\right)^{-1}\vepsi. \\
% \end{equation}
% 
% Assume that $\vepsi\sim\rN(\vzeros, \sigma^2_{\epsilon}\mI_n)$


%==============================================
\section{Lesage's Book Example}\label{sec:lesage-example}\index{Spillover effects!example}
%==============================================

\subsection{Commuting Times and Congestion}

In this section we use \cite{lesage2010introduction}'s example as an illustration of spatial spillovers.\footnote{This example is further explore in \cite{kirby2009changes} with a real application.} For this purpose consider a set of seven regions show in Figure \ref{fig:lesage-example}, which represent three regions to the west and three to the east of a central business district (CBD). In particular, consider region $R4$ as being the central business district. Since the entire region contains only a single roadway, all commuters share this route to and from the CBD.


\begin{figure}[ht]
\caption{Regions east and west of the CBD}\label{fig:lesage-example}
		    \centering 
		      \includegraphics[width = 8cm, height=6cm]{figure/lesage.png}
\end{figure}

We observe the following set of the sample data for these regions that relates travel times to the CBD (in minutes) contained in the dependent variable vector $\vy$ to distance (in miles) and population density (population per square block) of the regions in the two columns of the matrix $\mX$.

\begin{figure}[H]
		    \centering 
		      \includegraphics[width = 8cm, height=6cm]{figure/lesage2.png}
\end{figure}	

According to \cite{lesage2010introduction}, the pattern of longer travel times for more distant regions R1 and R7 versus nearer R3 and R5 found in vector $\vy$ seems to clearly violate independence, since travel times appear similar for neighboring regions (see also Example \ref{example:commuting-Kirby}). However one can argue that the observed pattern is not due to spatial dependence, but rather it is explained by the variables Distance and Density associated with each region, since these also appear similar for neighboring regions. Note that even for individual residing in the CBD, it takes time to go somewhere else in the CBD. Therefore, the travel time for intra-CBD travel is 26 minutes despite having a distance of 0 miles. 

If we assume that the observed data was collected in a given day and averaged over a 24-hour period, it can be hypothesized that congestion effects that arise from the shared highway can explain the observed patter of travel times.  It is reasonable to claim that longer travel times in one region should lead to longer travel times in neighboring regions on any given day. This is because commuters pass from one region to another as they travel along the highway to the CBD.

Congestion effects represent one type of spatial spillover, which do not occur simultaneously, but require some time for the traffic delay to arise. From a modeling point of view, this effect cannot be captured by OLS model with distance and density as independent variables. These are dynamic feedback effects from travel time on a particular day that impact travel times of neighboring regions in the short time interval required for the traffic delay to occur. Since the explanatory variable distance would not change from day to day, and population density would change very slowly on a daily time scale, these variables would not be capable of explaining daily delay phenomena. 

A better way of explaining congestion is by the following DGP: 

\begin{equation*}
\vy = \rho_0\mW\vy + \mX\vbeta_0 + \vepsi, 
\end{equation*}
%
such that:
\begin{equation*}
\widehat{\vy} = \left(\mI_n - \widehat{\rho}\mW\right)^{-1}\mX\widehat{\vbeta},
\end{equation*}
%
where the estimated parameters are $\widehat{\vbeta} = (0.135, 0.561)'$ and $\widehat{\rho} = 0.640$ (assume that somehow we have estimated these parameters). Note that the estimated spatial autoregressive parameters indicates positive spatial dependence in the commuting times. 

\subsection{Computing Effects in R}

Now think about the following question: What would be the estimated spillovers if region $R2$ doubles its population density? To answer this question we first obtain the predicted values of travel times before the change.\footnote{Note that there is a typo in \cite{lesage2010introduction}, because in their equation (1.19) they double distance, not density.} That is, we first obtain:

\begin{equation*}
\widehat{\vy}^{(1)} = \left(\mI_n - \widehat{\rho}\mW\right)^{-1}\mX\widehat{\vbeta}.
\end{equation*}

<<exam-effect>>=
# Estimated coefficients
b <- c(0.135, 0.561)
rho <- 0.642

# W and X
X <- cbind(c(10, 20, 30, 50, 30, 20, 10),
           c(30, 20, 10, 0, 10, 20, 30))
W <- cbind(c(0, 1, 0, 0, 0, 0, 0),
           c(1, 0, 1, 0, 0, 0, 0),
           c(0, 1, 0, 1, 0, 0, 0),
           c(0, 0, 1, 0, 1, 0, 0),
           c(0, 0, 0, 1, 0, 1, 0),
           c(0, 0, 0, 0, 1, 0, 1),
           c(0, 0, 0, 0, 0, 1, 0))
Ws <- W / rowSums(W)

# Prediction
yhat_1 <- solve(diag(nrow(W)) -  rho * Ws) %*% crossprod(t(X), b)
@

Now we estimate the predicted values of travel times after the change in population density in $R2$ using:

\begin{equation}
\widehat{\vy}^{(2)} = \left(\mI_n - \widehat{\rho}\mW\right)^{-1}\widetilde{\mX}\widehat{\vbeta}
\end{equation}
%
where $\widetilde{\mX}$ is the new matrix reflecting a doubling of the population density of region $R2$.\footnote{For more about prediction in the spatial context see \cite{kelejian2007relative}.} A comparison of predictions $\widehat{\vy}^{(1)}$ and $\widehat{\vy}^{(2)}$ are going to be used to illustrate how the model generates spatial spillovers.  

<<doubling-dens>>=
# Now we double the population density of a single region
X_d <- cbind(c(10, 40, 30, 50, 30, 20, 10),
             c(30, 20, 10, 0, 10, 20, 30))

# Compute predicted value after the change
yhat_2 <- solve(diag(nrow(W)) -  rho * Ws) %*% crossprod(t(X_d), b)

# Results
result <- cbind(yhat_1, yhat_2, yhat_2 - yhat_1)
colnames(result) <- c("y1", "y2", "y2 - y1")
round(result, 2)
sum(yhat_2 - yhat_1)
@

The two set of predictions show that the change in region $R2$ population density has a direct effect that increases the commuting times for residents of region $R2$ by $\approx$4 minutes. It also has an indirect or spillover effect that produces an increase in commuting times for the other six regions. Furthermore, it can be noticed that the increase in commuting times for neighboring regions $R1$ and $R3$ are the greatest and these spillovers decline as we move to regions in the sample that are located farther away from region $R2$ where the change in population density occurred. 

What is the cumulative indirect impacts? Adding up the increased commuting times across all other regions (excluding the own-region change in commuting time), we find that equals $\approx  4.86 (2.56 + 1.45 + 0.53 + 0.19 + 0.08 + 0.05)$ minutes, which is larger than the direct (own-region) impact of 4 minutes. Finally, the total impact of all residents of the seven regions from the change in population density of region $R2$ is the sum of the direct and indirect effects, or 8.85 minutes increase in travel times to the CBD. 

Now assume that the OLS estimates for the example above are: $\widehat{\vbeta}_{OLS} = \left[0.55, 1.25\right]$. Using these estimates we compute the OLS predictions based on the matrices $\mX$ and $\widetilde{\mX}$ as shown above.

<<ols_pred>>=
# Ols prediction
b_ols <- c(0.55, 1.25)
yhat_1 <- crossprod(t(X), b_ols)
yhat_2 <- crossprod(t(X_d), b_ols)
result <- cbind(yhat_1, yhat_2, yhat_2 - yhat_1)
colnames(result) <- c("y1", "y2", "y2 - y1")
round(result, 2)
@

The results show no spatial spillovers. Only the travel time of $R2$ is affected by the change in population density of region $R2$. It can be also observed that OLS prediction is upward bias.  This is the main message here. An OLS model does no allows for spatial spillover impacts and generates biased marginal effects. 

Now we further explore our formulas and definition from previous Section. As we showed in Equation (\ref{eq:direct_impact}), the impact of changes in the $i$th observation of $x_r$ on $y_i$  is $S_r(W)_{ii}$. Given the SLM structure of our example, this is equivalent to 

\begin{equation*}
\frac{\partial \E(\texttt{CT}_i)}{\partial \texttt{density}_i} = \mS_{density}(\mW)_{ii}, \quad \mbox{where $\mS_{\texttt{density}} = (\mI - \rho \mW)^{-1}\mI\beta_{\texttt{density}}$}.
\end{equation*}

We can compute our $\mS_{\texttt{density}}$ in the following way.
 
<<compu-S>>=
# Compute S(W) matrix for density
b_dens <- 0.135
S <- solve(diag(nrow(W)) -  rho * Ws)  %*% diag(nrow(W)) * b_dens 
colnames(S) <- rownames(S) <- c("R1", "R2", "R3", "R4", "R5", "R6", "R7")
@

Then, the direct impact of doubling population density of $R2$ on the expected value of commuting time for $R2$ is given by
   
\begin{equation*}
  \Delta \E(\texttt{CT}_2) = S_{\texttt{density}}(\mW)_{22} \Delta \texttt{density}_{2} = S_{\texttt{density}}(\mW)_{22} \cdot 20
\end{equation*}

In R, this equals :

<<effect1>>=
# Direct impact of R2 on R2
round(S[2,2] * 20, 2)
@

Note that this value is the same as that found using the predicted value procedure:  by doubling population density in  $R2$  increases the commuting times for residents of region $R2$ by $\approx$4 minutes. 

Finding the indirect impact on region $R1$ is similar given Equation \ref{eq:indirect_impact}. The indirect impact on region $R1$ is given by:

\begin{equation*}
  \Delta \E(\texttt{CT}_1) = S_{\texttt{density}}(\mW)_{12} \Delta \texttt{density}_{2} = S_{\texttt{density}}(\mW)_{12} \cdot 20
\end{equation*}

That is:

<<effect2>>=
# Indirect impact of R2 on R1
round(S[1,2] * 20, 2)
@

Again, note that is the same value computed before: An increase of 100\% of population density in $R2$ implies an increase of travel time of region $R1$ to CBD of about 2.56 minutes, after considering all feedback effects. 


An interesting question would be the following: What would be the impact on commuting time on $R1$ if population density increases by 20 in all the Regions? To answer this question, we should recall our definition \ref{def:ATIT} states that the sum across the $i$th row of $\mS_{r}(\mW)$ would be represent the total impact on individual observation $y_i$ resulting from changing the $r$th explanatory variable by the same amount across $n$ observations. 
   
<<effect3>>=
# ATIT
round(sum(S[1, ]) * 20, 2)
@

This number implies that the total impact to R1 will be an increase of commuting time of $\approx 7.5$ minutes. Using the formula for ATIT gives the same result:

<<effect4>>=
# ATIT
n <- nrow(W)
vones <- rep(1, n) 
round(((t(vones) %*% S %*% vones) / n ) * 20, 2)
@

Similarly, we could ask: What would be the impact of increasing density by 20 in $R1$ on all the other regions? This is equivalent to our definition \ref{def:ATIF} which state that the sum down the $j$th column of $\mS_r(\mW)$ would yield the total impact over all $y_i$ from changing the $r$th explanatory variable by an amount in the $j$th observation.

<<effect5>>=
# ATIF
round(sum(S[, 1]) * 20, 2)
@

In words, increasing density by 20 in $R1$ would imply a total effect  in all the regions of about 7.54 minutes.

Imagine that you are a policy maker and you are considering in implementing a policy to reduce population density and hence reduce commuting time in the regions. However, given that resources are scarce, you must select which region to implement this policy. In order to produce a greater effect of policy you could use the estimated spatial model and look for the region that will have the greatest overall impact (considering feedback effects). Basically, this involves calculating the column sum of $\mS_r(\mW)$ for each region in the following way: 

<<>>=
# Computing colsums of S(W)
round(colSums(S), 2)
@ 
   
Note that the impact of decreasing population density by 1 will have a greater reduction in commuting time if applied in regions $R2$ and $R6$ (why?)

Finally, the average direct, indirect and total effects of an increase in 1 in population density in all the regions can be computed as follows. 

<<average-effects>>=
# Average Direct Impact 
ADI <- sum(diag(S)) / nrow(W)
round(ADI, 4)

# Average Total Impact 
Total <- crossprod(rep(1, nrow(W)), S) %*% rep(1, nrow(W)) / nrow(W)
round(Total, 4)

# Average Indirect Impact
round(Total - ADI, 4)
@

Equation (\ref{eq:total_effect_simple}) of Example \ref{example:pollution-kim}, we show that the total effect can be also be computed as $\beta_r / (1 - \rho)$. We know show that this proposition is true for our example

<<check-total>>=
#Check total effect
b_dens / (1 - rho )
@

\subsection{Cumulative Effects}

The main idea of this exercise is to show how the change in some explanatory variable produces changes in the independent variable in all the spatial units by decomposing them into cumulative and marginal impacts for different order of neighbors as explained in Section \ref{sec:partitioning-effects}. 
    
First, we load the package \pkg{expm} which will allow us to compute power of matrices in a loop. Then we create the estimated coefficients along with the $\mW$ matrix:

<<message = FALSE>>=
# Package to compute power of a matrix
library("expm") 
@

In order to create the decomposition for the ADI, AII and ATI, we create the following loop from $q = 0$ to $q = 10$:

<<loop-decomp>>=
## Loop for decomposition
out <- matrix(NA, nrow = 11, ncol = 3)             # Matrix for the results 
colnames(out) <- c("Total", "Direct", "Indirect")  # colnames
rownames(out) <- paste("q", sep = "=", seq(0, 10)) # rownames

for (q in 0:10) {
  if (q == 0) {                                    # If q=0, then Sr = I * beta
    S <- diag(n) * b_dens
  } else {
    S <- (rho ^ q * Ws %^% q)  * b_dens
  }
  q <- q + 1                                       # the row = 0 doesn't exist!
  out[q, 2] <- sum(diag(S)) / n
  out[q, 1] <- crossprod(rep(1, n), S) %*% rep(1, n) / n
  out[q, 3] <- out[q, 1] - out[q, 2]
}
@

The results are the following

<<result-loop>>=
# Print results
round(out, 4)
round(colSums(out), 4)
@

This table shows both the cumulative and partitioned direct, indirect and total impacts associated with orders 0 to 10 for the SLM. The cumulative direct impact from previous section equal to 0.1837, which given the coefficient 0.1350 indicates that \emph{there is a feedback equal to} (0.1837 - 0.1350) = 0.0487 arising from each region impacting neighbors that in turn impacts neighbors to neighbors and so on.

The column sum of the matrix \texttt{out} shows that by the time we reach 10th-order neighbors we have accounted for 0.1834 of the 0.1837 cumulative direct effect. It is important noting that for $\mW^0$ there is no indirect effect, only direct effects, and for $\mW^1$ there is no direct effect, only indirect. To see this, note that when $q=0$ we obtain $\mW^0 = \mI_n$:

<<loop-w0>>=
Ws %^% 0
@

Thus,  we have $\mS_r(\mW) = \mI_n\beta_r = 0.1350 \mI_n$. When $q= 1$ we have only indirect effect since there are zero elements on the diagonal of the matrix $\mW$. This also occurs for  $q = 3, 5, 7, 9$: 

<<loop-w1>>=
Ws %^% 1
Ws %^% 3
@


Also, the row-stochastic nature of $\mW$ leads to an average of the sum of the rows that takes the form $\beta_r \times \rho= 0.135\times 0.642 = 0.0867$, when $q=1$.

The matrix \texttt{out} also shows that both direct and indirect effects fall out as the order of neighbors increases, however the indirect or spatial spillovers effects decay more slowly as we move to higher-order neighbors. 


%**********************************************
%\section{Criticisms of Spatial Econometrics}
%**********************************************

% The concepts of causality and endogeneity have not been elaborated so much in a spatial context.\\
% 
% The goal of spatial dependence is to measure the effects of proximity. Other studies try to isolate the effect of proximity.\\
% 
% Another problem using spatial data analysis is that important variables are highly correlated and no study includes all relevant variables (Problem of correlated missing variables \citep{mcmillen2010issues}).\\
% 
% In some cases when the units of observation can be located in some geographical space it is possible that location is irrelevant for understanding data pertaining to those units. In this case, we might say that the spatial dimension is random. In other cases, including the spatial dimension to the problem is important.\\
% 
% 
% There are three papers that critique spatial econometrics.
% 
% \begin{enumerate}
% 	\item \cite{gibbons2012mostly} state that identification is almost always impossible with standard spatial econometric practice.
% 	\item \cite{mcmillen2012perspectives} posits that spatial econometrics rests on very restrictive assumptions and that identification is difficult. Nonparametric approaches are better.
% 	\item \cite{corrado2012economics} state that standard spatial econometric has too often been misapplied. Namely, there is not enough emphasis on both theory or in simply forming a conceptual framework to understand spatial spillovers.
% \end{enumerate}	
% 
% 
% According to 

%=============================================================
\section{Empirical Example: Growth and Spatial Externalities}
%=============================================================

\cite{ertur2007growth} developed a theoretical growth model that explicitly incorporates technological interdependence among economies and examines the role of spatial spillover effects.

The model is based on an aggregate Cobb-Douglas production function for country $i$ at time $t$, exhibiting constant returns to scale in labor and physical capital:
\begin{equation}\label{eq:prod_func}
Y_i(t) = A_i(t)K_i^{\alpha}(t)L_i^{1- \alpha}(t), 
\end{equation}
where $Y_i(t)$ denotes total output, $K_i(t)$ is the stock of reproducible physical capital, $L_i(t)$ is the labor force, and $A_i(t)$ represents the aggregate level of technology.

The evolution of $A_i(t)$ is modeled as:
\begin{equation}\label{eq:A_growth}
A_i(t) = \Omega(t)k_i^{\phi}(t)\prod_{j\neq i}A_j^{\gamma w_{ij}}(t),
\end{equation}
%
where $k_i(t) = K_i(t)/L_i(t)$ is the capital intensity, and $w_{ij}$ denotes the weight capturing the connectivity between country $i$ and country $j$ in the spatial network.

This formulation reflects three distinct sources of technological progress:
\begin{enumerate}
\item Exogenous Technological Progress: The term $\Omega(t)$ captures the common component of technological change across all countries, assumed to grow at a constant rate $\mu$, i.e., $\Omega(t) = \Omega(0)e^{\mu t}$.
\item Domestic Knowledge Spillovers: The second term, $k_i^{\phi}(t)$, reflects the idea that technology in a country improves with its own capital deepening. The parameter $\phi \in [0,1)$ measures the strength of internal (or home) externalities from physical capital accumulation.
\item International Knowledge Spillovers: The final term captures cross-country technological interdependence. It is modeled as a spatially weighted geometric mean of the technological levels of neighboring countries, where $\gamma \in [0,1)$ measures the strength of international spillovers. While $\gamma$ is assumed to be common across countries, the impact of spatial externalities on a specific country $i$ depends on its relative position and connectivity in the spatial structure, as encoded by the weights $w_{ij}$. The more a given country $i$ is connected to its neighbors, the higher $w_{ij}$ is, and the more country $i$ benefits from spatial externalities. 
\end{enumerate}

In logarithmic and  matrix form, Equation \eqref{eq:A_growth} becomes:
\begin{equation*}
\va = \mOmega + \phi\vk + \gamma \mW\va, 
\end{equation*}
%
where $\va$ is the $n \times 1$ vector of the logarithms of technology levels, $\vk$ is the $n \times 1$ vector of the logarithms of capital per worker, and $\mW$ is the $n \times n$ spatial weight matrix with elements $w_{ij}$. Solving for $\va$---assuming $\gamma \neq 0$ and $1/\gamma$ is not an eigenvalue of $\mW$---yields:
\begin{equation*}
\va = \left(\mI-\gamma \mW\right)^{-1}\mOmega + \phi\left(\mI-\gamma \mW\right)^{-1}\vk.
\end{equation*}

If $|\gamma|<1$, then Equation \eqref{eq:A_growth} can be rewritten as:
\begin{equation}\label{eq:new_A_g}
A_i(t) = \Omega^{\frac{1}{1-\gamma}}k_i^{\phi}\prod_{j = 1}^nk_j^{\phi\sum_{r = 1}^{\infty}\gamma^rw_{ij}^{(r)}}(t),
\end{equation}
%
where $w_{ij}^{(r)}$ denotes the $r$-th order spatial weight between countries $i$ and $j$.

Substituting \eqref{eq:new_A_g} into the per-worker form of the production function \eqref{eq:prod_func} gives:
\begin{equation}\label{eq:ypercapita}
y_i(t) = \Omega^{\frac{1}{1-\gamma}}k_i^{u_{ii}}(t)\prod_{j\neq i}^nk_j^{u_{ij}}(t),
\end{equation}
%
where $u_{ii} = \alpha + \phi\left(1 + \sum_{r = 1}^{\infty}\gamma^rw_{ii}^{(r)}\right)$ and $u_{ij} = \phi\sum_{r = 1}^{\infty}\gamma^rw_{ij}^{(r)}$, and $y_i(t)=Y_i(t)/L_i(t)$. 

This expression implies spatial heterogeneity in the production function parameters. Specifically, if there are no externalities from physical capital (i.e., $\phi = 0$), then $u_{ii} = \alpha$ and $u_{ij} = 0$, and the model collapses to the standard neoclassical production function.

The model also allows for an analysis of the \textit{social elasticity} of income per worker with respect to physical capital. According to Equation \eqref{eq:ypercapita}, when country $i$ increases its own capital per worker, it benefits from a marginal social return of $u_{ii}$. If instead all countries simultaneously raise their capital per worker, the total return for country $i$ increases to:
\[
u_{ii} + \sum_{j \neq i} u_{ij} = \alpha + \frac{\phi}{1 - \gamma}.
\]
To ensure local convergence and avoid explosive or endogenous growth dynamics, it is assumed that social returns are decreasing; that is, $\alpha + \frac{\phi}{1 - \gamma} < 1$.

Assume that a constant fraction $s_i$ of output is saved, labor grows exogenously at rate $n_i$, and the depreciation rate of capital $\delta$ is identical across countries. Then, the dynamics of capital per worker follow the Solow equation: \begin{equation*} \dot{k}_i(t) = s_iy_i(t) - (n_i + \delta)k_i(t), \end{equation*} where the dot denotes a time derivative.

Under these assumptions, \cite{ertur2007growth} derive the steady-state income per worker in country $i$ as:
\begin{equation*}
  \begin{aligned}
    \log y_{it}^* & =  \frac{1}{1 - \alpha -\phi}\log \Omega_t + \frac{\alpha + \phi}{1 - \alpha -\phi}\log s_i - \frac{\alpha + \phi}{1 - \alpha -\phi}\log (n_i + g + \delta) \\
                  & - \frac{\alpha\gamma}{1 - \alpha -\phi}\sum_{j\neq i}^N w_{ij}\ln s_j + \frac{\alpha\gamma}{1 - \alpha -\phi}\sum_{j\neq i}^N w_{ij}\ln(n_j + g + \delta) \\
                  & + \frac{\gamma(1 - \alpha)}{1 - \alpha -\phi}\sum_{j\neq i}^N w_{ij}\log y_{jt}^*
  \end{aligned}
\end{equation*}

This steady-state equation is operationalized econometrically as the following SDM (for $t=0$):
\begin{equation*}
    \log \left(\frac{Y_i}{L_i}\right) =  \beta_0 + \beta_1\log s_i + \beta_2 \log (n_i + g + \delta) 
                   + \theta_1 \sum_{j\neq i}^N w_{ij}\ln s_j + \theta_2\sum_{j\neq i}^N w_{ij}\ln(n_j + g + \delta) 
                   + \rho \sum_{j\neq i}^N w_{ij}\log \left(\frac{Y_j}{L_j}\right) + \epsilon_i,
\end{equation*}
%
where:
\begin{itemize}
  \item $\beta_0 + \epsilon_i = \frac{1}{1 - \alpha -\phi}\log \Omega_0$, 
  \item $\beta_1 = \frac{\alpha + \phi}{1 - \alpha -\phi}$,
  \item $\beta_2 = - \frac{\alpha + \phi}{1 - \alpha -\phi}$,
  \item $\theta_1 = - \frac{\alpha\gamma}{1 - \alpha -\phi}$,
  \item $\theta_2 = \frac{\alpha\gamma}{1 - \alpha -\phi}$,
   \item $\rho = \frac{\gamma(1 - \alpha)}{1 - \alpha -\phi}$
\end{itemize}
%
with the following theoretical constraints: 
\begin{itemize}
  \item $\beta_1 = - \beta_2$,
  \item $\theta_2 = -\theta_1$
\end{itemize}

\cite{ertur2007growth} argue that the textbook Solow model is misspecified because it omits spatially correlated variables related to technological interdependence and physical capital externalities. In particular, the error term from the Solow model can be written as:
\begin{equation*}
  \vepsi_{\mbox{Solow}} = \frac{\phi}{1 - \alpha}\left(\mI - \gamma\mW\right)^{-1}k^{*} + \left(\mI - \gamma\mW\right)^{-1}\vepsi.
\end{equation*}

Hence, even if $\phi = 0$ (i.e., no domestic capital externalities), omitting spatially correlated variables induces spatial autocorrelation in the residuals, leading to model misspecification.

To test this hypothesis empirically, we can estimate the following standard Solow model using OLS:
\begin{equation*}
  \log\left(\frac{Y_i}{L_i}\right) = \beta_0 + \beta_1 \ln(s_i) + \beta_2 \ln(n_i + g  + \delta) + \epsilon_i,
\end{equation*}
%
and then use the Moran's I test on the residuals. 





%======================
\section{Exercises}
%======================

\begin{exercises}

    \exercise Assume three regions with row-normalized spatial weight matrix given in Equation \eqref{eq:w_mat_ex_spill}. Derive the total, direct an indirect effects for the following models:
        \begin{enumerate}
            \item Spatial Durbin Model given by:
		\begin{equation}
			\vy = \rho \mW \vy + \alpha \vones_n + \mX \vbeta + \mW \mX \vtheta + \vepsi
		\end{equation}
            \item Spatial Lag Model given by:
		\begin{equation}
			\vy = \rho \mW \vy + \alpha \vones_n + \mX\vbeta + \vepsi
		\end{equation}
		   \item Spatial Durbin Error Model given by:
		\begin{eqnarray}
			\vy &=& \alpha \vones_n + \mX\vbeta + \mW\mX\vtheta + \vu \\
			\vu &=& \lambda \mW\vu + \vepsi
		\end{eqnarray}
		\item OLS given by:
		\begin{equation}
			\vy = \alpha \vones_n + \mX\vbeta + \vepsi
		\end{equation}
		\item Spatial Error model given by:
		
		\begin{equation}
			\begin{aligned}
				\vy &= \alpha \vones_n + \mX\vbeta + \vu \\
				\vu &= \lambda \mW\vu + \vepsi
			\end{aligned}
		\end{equation} 
        \end{enumerate}

    \exercise Consider your results for the SLM and SDM models from Exercise  \ref{lab:2.1}. Show that for the SLM model the ratio between the indirect and the direct effect of a particular explanatory variable is independent of $\beta_k$. Show that this is not the case for the SDM model. What do you conclude?
    
    \exercise  Recall that if the row-sums of $\mW$ is less than or equal to one and $\rho$ is in the proper parameter space, i.e., $\rho <1$, the total average effect for variable $r$ can be computed as $\beta_r / (1 - \rho)$. What is the sign of the parameter that matters the most when calculating the sign of the total effect? Does the $\rho$ or $\beta_r$?

\end{exercises}
