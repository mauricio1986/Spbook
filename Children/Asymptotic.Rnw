\chapter{Review of Asymptotic Theory}

This chapter provides some basic definitions and concepts for asymptotic theory. 

%***************************************************
\section{Convergence of Deterministic Sequences}\label{sec:nonstochastic_con}\index{Convergence!deterministic sequences}
%***************************************************

In order to understand the asymptotic behavior of stochastic sequences we need first to refresh some concepts about deterministic (non-random) sequences. Recall that a sequence of nonstochastic real numbers $\left\lbrace a_n\right\rbrace$ converges to $a$ if for any $\epsilon > 0$, there exists $n^* = n^*(\epsilon)$ such that for all $n > n^*$,

\begin{equation*}
	\left| a_n - a\right|<\epsilon, 
\end{equation*}
%
e.g., if $a_n = 2 + 3/n$, then the limit is 2 since $\left| a_n - a\right| =\left| 2 + 3/n - 2\right| = \left|3/n\right|< \epsilon$ for all $n > n^* = 3/\epsilon$. 


\begin{figure}[ht]
  \caption{Convergence of sequence $2 + 3/n$}
    \label{fig:SeqConv}
    \centering
    \begin{minipage}{.9\linewidth}
        <<SeqConv, echo = FALSE, message = FALSE, fig.align='center', out.width = '10cm', out.height = '10cm'>>=
a_n <- function(n) 2 + 3 / n
par(mfrow = c(1, 1), mar = c(4, 4, 1, 1) + 0.1)
plot(a_n(seq(1, 40, 1)), 
     ylab = expression("a"[n]), 
     ylim = c(1.5, 4), 
     xlab = "n",
     col = "black",
     lwd = 2)
abline(h = 2, col = "red")
abline(h = 1.9, col = "blue", lty = 2)
abline(h = 2.1, col = "blue", lty = 2)
abline(v = 30, lty = 3)
text(5, 2.2, expression(2 + epsilon),
     cex = .8)
text(5, 1.8, expression(2 - epsilon),
     cex = .8)
text(5, 2.2, expression(2 + epsilon),
     cex = .8)
text(30, 3.5, expression(n ^ "*"),
     cex = 1)
@
\footnotesize
		\emph{Notes: This graphs shows the convergence of the sequence $2 + 3/n$ where $\epsilon = 0.1$ and $a = 2$.}
	\end{minipage}
\end{figure}


Definition \ref{definition:convergence_deterministic} give us a formal statement regarding nonstochastic sequence of numbers.

\begin{definition}[Deterministic convergence]\label{definition:convergence_deterministic}
	The sequence $\left\lbrace b_n: n = 1,2,... \right\rbrace$ of real numbers  converges to the limit $b$ if for every $\epsilon > 0$ there exists and $n^*(\epsilon)$ such that if $n>n^*(\epsilon)$ then $|b_n - b|<\epsilon$. This is also indicated as follows:
\begin{equation*}
\lim_{n\to \infty} b_n = b
\end{equation*}	
\end{definition}

In Definition \ref{definition:convergence_deterministic} by choosing a very small $\epsilon$, we ensure that $b_n$ gets arbitrarily close to its limit $b$ for all $n$ that is sufficiently large. If fact, the smaller $\epsilon$ is, the larger $n(\epsilon)$ will be. So, $\epsilon$ can be interpreted as a prespecified tolerance level for the discrepancy between $b_n$ and $b$. When a limit exists, we say that the sequence $\left\lbrace b_n \right\rbrace$ \textbf{converges} to $b$ as $n$ tends to infinity, written $b_n\to b$ as $n\to \infty$.

Figure~\ref{fig:SeqConv} shows that the sequence $2 + 3/n$ converges to 2. Note that if $\epsilon = 0.1$ then it is always true that $a_n$ will be always between $2 + \epsilon$ and $2 - \epsilon$ if and only if $n \geq n^*= 30$.

In econometric (and specially in spatial econometrics) we talk a lot about sequences of matrices. Probably you are asking yourself, what is a sequence of matrices? Hopefully, the following example will give you some intuition. 

\begin{example}[A sequence of Matrices]\label{example:sequence_matrix}
Let $\mX_n$ be an $n\times 2$ matrix whose $i$th row is defined by the $1\times 2$ vector $\left[1, i\right]$ so that

\begin{equation*}
\mX_n = \begin{pmatrix}
  1 & 1 \\
  1 & 2 \\
  \vdots & \vdots\\
  1 & n
\end{pmatrix}
\end{equation*}

Then

\begin{equation*}
\left\lbrace \begin{pmatrix}
1 & 1\\
1 & 1
\end{pmatrix},
\begin{pmatrix}
  1 & 3/2 \\
  3/2 & 5/2
\end{pmatrix},
\begin{pmatrix}
1 & 2 \\
2 & 14/3
\end{pmatrix}, ...
\right\rbrace
\end{equation*}
%
is a sequence of matrices $\mY_1, \mY_2, \mY_3,...$ defined by the function $\mY_n = \frac{1}{n}\mX_n^\top\mX_n$, where the $n$th element of the sequence is defined as

\begin{equation*}
\mY_n = \begin{pmatrix}
1 & \frac{\sum_{i = 1}^n i}{n} \\
\frac{\sum_{i = 1}^n i}{n} & \frac{\sum_{i = 1}^n i^2}{n}
\end{pmatrix} = 
\begin{pmatrix}
1 & \frac{(n + 1)}{2} \\
\frac{(n + 1)}{2}& \frac{(n +1)(2n +1)}{6}
\end{pmatrix}
\end{equation*}
\end{example}

Now, we formally state the concept of convergence for matrices. 

\begin{definition}[Limit of a Real-Valued Matrix Sequence]
Let $\left\lbrace\mX_n \right\rbrace$ be a sequence whose elements are $q\times k$ real-valued matrices. Suppose there exists a $q\times k$ matrix of real numbers $\mX$ such that $\mX_n\left[i, j\right]\to \mX\left[i, j\right]$ for $i = 1,...,q$ and $j = 1,...,k$. Then the matrix $\mX$ is the limit of the matrix sequence $\left\lbrace\mX_n \right\rbrace$ as $n\to \infty$. If the limit does not exists, the sequence is said to be divergent
\end{definition}

The definition of the limit implies that for a sufficiently large choice of $n$, the matrix $\mX_n$ becomes arbitrarily close to the matrix $\mX$, \textbf{element by element}.

Often we wish to consider the limit of a continuous function of a sequence. 

\begin{definition}[Limit of a continuous function of a sequence]
Given $\vg:\SR^k\to \SR^l(k,l, \in \SN)$ and $\vb\in \SR^k$, 

\begin{enumerate}
\item the function $\vg$ is continuous at $\vb$ if for any sequence $\lbrace \vb_n \rbrace$ such that $\vb\to \vb$, $\vg(\vb_n)\to \vg(\vb)$;
\item or equivalently, the function $\vg$ is continuous at $\vb$ if for every $\epsilon > 0$ there exists $\delta(\epsilon)>0$ such that if $\va \in \SR^k$ and $\left|a_i - b_i\right|< \delta(\epsilon)$, $i = 1, ..., k$, then $\left|g_j(\va)- g_j(\vb)\right|<\epsilon$, $j = 1, ..., l$. Further, if $B\subset \SR^k$, then $\vg$ is continuous on $B$ if it is continuous at every point of $B$. 
\end{enumerate}
\end{definition}

\begin{example}
If $\va_n \to \va$ and $\vb_n\to \vb$, then $\va_n + \vb_n\to \va + \vb$ and $\va_n\vb_n^\top\to\va\vb^\top$. 
\end{example}

\begin{example}
The matrix inverse function is continuous at every point that represents a nonsingular matrix, sot that if $\mX^\top\mX/n\to \mM$, a finite nonsingular matrix, then $\left(\mX^\top\mX/n\right)^{-1}\to \mM^{-1}$.
\end{example}


Sometimes, some sequences does not have a limit, but we can say whether they are \textbf{bounded}: 

\begin{definition}[Bounded sequence]\label{definition:bounded_sequence}\index{Convergence!bounded sequences}
 A  sequence $\left\lbrace b_n: n = 1,2,... \right\rbrace$ is \emph{bounded} if and only if there is some $a < \infty$ such that $\left|b_n\right|\leq a$ for all $n = 1,2,...$ Otherwise, we say that $\left\lbrace b_n \right\rbrace$ is \emph{unbounded}.
\end{definition}

Thus, for a sequence of real numbers to be bounded, there must exist a positive number that is larger than the absolute value of each and every number in the sequence. For a sequence that has no limit and is also unbounded, we write $b_n \to \infty$, denoting that the sequence diverges to infinity. 


\begin{example}[Bounded Sequences]
Consider $a_n = (-1)^n$, then $a_n$ does not have a limit, but it is bounded since $-1 \leq a_n \leq 1$. The sequence $a_n = 1/n$ is bounded, since $0 \leq a_n\leq 1$ for all $n = 1,2,...$
\end{example}


\begin{example}[Boundedness and Limit of Matrices]
Consider the following examples:
\begin{enumerate}
 \item Recall the sequence of matrices in Example \ref{example:sequence_matrix}. In this case, only the sequence $\left\lbrace \mY_n\left[1, 1\right]\right\rbrace$ is bounded. All other sequences of matrix elements are unbounded and, in fact, diverge to infinity. Since all the sequences of matrix elements must be bounded for the matrix sequence to converge, the matrix does not have a limit.
 \item Let $\left\lbrace \mX_n\right\rbrace$ be a sequence of matrices such that
 
\begin{equation*}
\mX_n = \begin{pmatrix}
  3n^{-1} & n^{-1} \\
  3       & 1 + n^{-1}
\end{pmatrix}.
\end{equation*}

All four sequences of the matrix elements are bounded, since $\left|3n^{-1}\right|\leq 3$, $\left|n^{-1}\right|\leq 1$, $\left|3\right|\leq 3$, and $\left|1 + n^{-1}\right|\leq 2$, for all $n$. Furthermore, limits exists for all four sequences of matrix elements, since $3n^{-1}\to 0, n^{-1}\to 0, 3\to 3$, and $1 + n^{-1}\to 1$. Thus 

\begin{equation*}
\mX_n\to \mX= \begin{pmatrix}
0 & 0 \\
3 & 1
\end{pmatrix}
\end{equation*}
 \end{enumerate}
\end{example}

Often it is useful to have a measure of the \emph{order of magnitude} of a particular sequence without particularly worrying about its convergence. 
\begin{definition}[Big and little O]\label{definition:big_little_oh}\index{big O}
Consider the following definitions:
	\begin{enumerate}
		\item A sequence $\left\lbrace x_n \right\rbrace $ is $O(n^{\lambda})$ (at most of order $n^{\lambda}$) if $n^{-\lambda}x_n$ is bounded. When $\lambda=0$, $\left\lbrace x_n \right\rbrace $ is bounded, an we also write $x_n=O(1)$.
		\item $\left\lbrace x_n \right\rbrace $ is $o(n^{\lambda})$ if $n^{-\lambda}x_n\to 0$. When $\lambda=0$, $x_n$ converges to zero, and we also write $a_n=o(1)$.
		\item If $\left\lbrace X_n\left[i,j\right]\right\rbrace$ is $O(n^\lambda)$ or $o(n^\lambda)$ for all $i$ and $j$, then the matrix sequence $\left\lbrace \mX_n\right\rbrace$ is said to be $O(n^\lambda)$ or $o(n^\lambda)$
	\end{enumerate}
\end{definition}

The big $O$ notation describes the asymptotic behavior of functions. Basically, it tells you how fast a function grows or declines. 

\begin{remark}
From the definitions we can say that if $X_n = o(n^\lambda)$, then $X_n = O(n^\lambda)$. In other words, \textbf{any convergent sequence is bounded}. The opposite is not true. Recall the example $a_n = (-1)^n$.
\end{remark}

\begin{example}[Order of Magnitude of a Sequence]
Consider the following examples:

\begin{enumerate}
  \item Let $\left\lbrace x_n\right\rbrace$ be defined by $x_n = 3n^3 - n^2 + 2$. Then $\left\lbrace x_n\right\rbrace$ is $O(n^3)$, since $n^{-3}x_n = 3 - n^{-1} + 2n^{-3}$ is bounded. Also $\left\lbrace x_n\right\rbrace$ is $o(n^{3 + \epsilon})$ for any $\epsilon > 0$ since $n^{ - 3 - \epsilon}x_n = 3n^{-\epsilon} - n^{-1 - \epsilon} + 2n^{-3 - \epsilon}\to 0$. For example, Figure \ref{fig:SeqConvBond} plots $n^{-3}x_n$, which is bounded between 4 ($n = 1$) and 2.75 $(n = 2)$. Note also that if we choose $\epsilon = 0.1$, then $n^{3.1}x_n$ clearly converges to 0.
  \item Let $\left\lbrace x_n\right\rbrace$ by defined by $x_n = 3 + n^{-1}$. Then $\left\lbrace x_n\right\rbrace$ is $O(1)$, since $x_n$ is bounded, and $\left\lbrace x_n\right\rbrace$ is $o(n^\epsilon);\forall \epsilon > 0$, since $n^{-3}x_n = 3n^{-\epsilon} + n^{-1 - \epsilon}\to 0$. 
  \item Let the vector sequence $\left\lbrace \vx_n\right\rbrace$ by defined by
  
  \begin{equation*}
    \begin{pmatrix}
    \vx_n\left[1\right] \\
    \vx_n\left[2\right]
    \end{pmatrix}= 
    \begin{pmatrix}
      3n^{-1} \\
      n^{-1}
    \end{pmatrix}.
  \end{equation*}
  Then the vector sequence $\left\lbrace \vx_n\right\rbrace$ is $o(1)$ and $O(1)$, since
  
  \begin{equation*}
    \vx_n \to \begin{pmatrix} 0 \\ 0 \end{pmatrix}
  \end{equation*}
\end{enumerate}
\end{example}

\begin{figure}[ht]
  \caption{Bounded sequence}
    \label{fig:SeqConvBond}
    \centering
    \begin{minipage}{.9\linewidth}
        <<SeqConvBoun, echo = FALSE, message = FALSE, fig.align='center', out.width = '10cm', out.height = '10cm'>>=
a_n3 <- function(n) (3 * n ^ 3 - n ^ 2  + 2) / n ^ 3
a_n4 <- function(n) (3 * n ^ 3 - n ^ 2  + 2) / n ^ (3.1)
n <- seq(1, 200, 1)

par(mfrow = c(1, 1), mar = c(4, 4, 1, 4) + 0.1)
plot(n, a_n3(n), 
     type = "b" , 
     col = "black",
     pch = 16,
     xlab = "", 
     ylab = "") 
axis(2, col = "black", las = 1, ylim = c(4, 1))
mtext(expression("x"[n] / 3), side = 2, line = 2.5)
box()
par(new = TRUE)
plot(n, a_n4(n),
     pch = 15, 
     xlab = "", 
     ylab = "", 
     axes = FALSE, 
     type = "b",
     col = "red")
mtext(expression("x"[n] / 3.1), side = 4, col = "red", line = 2.5)
axis(4, ylim = c(0, 3), col = "red", col.axis = "red", las = 1)

# Add n axis
axis(1, pretty(range(n), 10))
mtext("n", side = 1, col = "black", line = 2.5)  

## Add Legend
legend("topleft", 
       legend = c(expression("x"[n] / 3), expression("x"[n] / 3.1)),
       text.col = c("black", "red"), 
       pch = c(16, 15), 
       col = c("black","red"))
@
\footnotesize
		\emph{Notes: This graphs shows that the sequence $\left\lbrace x_n\right\rbrace$ defined by $x_n = 3n^3 - n^2 + 2$ is $O(n^3)$ and $o(n^{3 + \epsilon})$. For plotting $\epsilon = 0.1$ was selected.}
	\end{minipage}
\end{figure}

The following proposition gives some elementary facts about the orders of magnitude of sums and products of sequences.

\begin{proposition}[Properties of big and little O]\label{prop:blO-nest}
Let $a_n$ and $b_n$ be scalars. 
\begin{enumerate}
  \item If $a_n=O(n^\lambda)$ and $b_n=O(n^\mu)$, then $a_nb_n=O(n^{\lambda + \mu})$ and $a_n + b_n = O(n^\kappa)$, where $k = \textrm{max}\left[\lambda, \mu\right]$.
  \item If $a_n = o(n^\lambda)$ and $b_n = o(n^\mu)$, then $a_nb_n=o(n^{\lambda + \mu})$ and $a_n + b_n = o(n^\kappa)$, where $k = \textrm{max}\left[\lambda, \mu\right]$.
  \item If $a_n=O(n^\lambda)$ and  $b_n = o(n^\mu)$, then $a_nb_n=o(n^{\lambda + \mu})$ and $a_n + b_n = O(n^\kappa)$, where $k = \textrm{max}\left[\lambda, \mu\right]$.
\end{enumerate}
\end{proposition}
%*****************************************
\section{Convergence in Probability}
%****************************************** 

In the previous section we reviewed how a sequence of real number converges to a real number.  What about the sequence of random variables such as econometric estimators? When considering a sequence of \emph{random variables} we cannot be certain that $\left| a_n - a\right|<\epsilon$, even for large $n$, due to the \textbf{randomness}. Instead, we require that \textbf{the probability of being within $\epsilon$ is arbitrarily close to one} as $n\to \infty$. The next definition is more appropriate for convergence in random variables. 

\begin{definition}[Convergence in Probability]\label{definition:convergence_in_probability}\index{convergence in probability}
	A sequence of random variables $\left\{X_n\right\}$ \textbf{convergence in probability} to a constant (non-random) $\alpha$ if, for any $\epsilon>0$,
	
	\begin{equation*}
	\lim_{n\to \infty} \Pr(\left|X_n-\alpha\right|>\epsilon)=0
	\end{equation*}
	
	The constant $\alpha$ is called the \textbf{probability limit} of $X_n$ and is written as $\plim X_n=\alpha$ or $X_n\pto \alpha$. Evidently,
	
	\begin{equation*}
	X_n\pto \alpha\qquad\mbox{is the same as}\qquad X_n-\alpha\pto 0
	\end{equation*}	
\end{definition}

Thus, roughly, convergence in probability states that for large $n$, the probability is high that $X_n$ will be close $\alpha$.

This definition can be understood if we look at Figure \ref{fig:convinprob}. Note that the expression $\left|X_n-\alpha\right|>\epsilon$ can be true or false. The probability that it is true is given by the distribution $F_n(\cdot)$ of $X_n$. Figure \ref{fig:convinprob} shows that the probability that $\left|X_n-\alpha\right|>\epsilon$,  denoted by the red-dashed area outside the interval $\alpha \pm \epsilon$, becomes smaller as $n$ increases. Conversely, the probability of $\left|X_n-\alpha\right|<\epsilon$, given by the blue-dashed area, will become higher and higher as $n\to \infty$. In the limit, this probability should be equal to 1. That is:

\begin{equation*}
	\lim_{n\to \infty} \Pr(\left|X_n-\alpha\right|<\epsilon)=1
\end{equation*}

\begin{figure}[ht]
  \caption{Illustration of convergnce in probability to a constant}
    \label{fig:convinprob}
    \centering
    \begin{minipage}{.9\linewidth}
        <<convinprob, echo = FALSE, message = FALSE, fig.align='center', out.width = '10cm', out.height = '10cm', warning =FALSE>>=
library("DescTools")
par(mfrow = c(1, 1), mar = c(4, 4, 1, 4) + 0.1)
curve(dnorm(x, mean = 0 , sd = 4), xlim = c(-8,8), ylim = c(0, 0.4), axes =  FALSE, 
      main = "",
      type = "n", las = 1, ylab = "probability", xlab = "")
Shade(dnorm(x, mean = 0 , sd = 4), 
      breaks = c(-9, -2, 2, 9),
      col = c(hred, hblue), 
      density = c(20, 7))
curve(dnorm(x, mean = 0 , sd = 2),
      main = "",
      type = "n", las = 1, ylab = "probability", xlab = "", 
      add =  TRUE)
Shade(dnorm(x, mean = 0 , sd = 2), 
      breaks = c(-9, -2, 2, 9),
      col = c(hred, hblue), 
      density = c(20, 7))
curve(dnorm(x, mean = 0 , sd = 1),
      main = "",
      type = "n", las = 1, ylab = "probability", xlab = "", 
      add =  TRUE)
Shade(dnorm(x, mean = 0 , sd = 1), 
      breaks = c(-9, -2, 2, 9),
      col = c(hred, hblue), 
      density = c(20, 7))
abline(v = 0, col = "black")
abline(v = 2, col = "black")
abline(v = -2, col = "black")
axis(1, at = c(-2, 0, 2), labels = c(expression(alpha - epsilon), expression(alpha), expression(alpha + epsilon)))
text(6.5, 0.05, "n = 10",
     cex = .8)
text(4, 0.09, "n = 100",
     cex = .8)
text(0, 0.4, "n = 300",
     cex = .8)
@
\footnotesize
		\emph{Notes: This graphs shows that the probability of $\left|X_n-\alpha\right|>\epsilon$, which is denoted by the red-dashed areas, becomes smaller as $n$ increases. }
	\end{minipage}
\end{figure}

Definition (\ref{definition:convergence_in_probability}) can be easily extended to a sequence of random vectors or random matrices (by viewing a matrix as a vector whose elements have been rearranged) by requiring element-by-element convergence in probability. That is, a sequence of $k$-dimensional random vectors $\left\{\vx_n\right\}$ convergences in probability to a $k$-dimensional vector of constants $\valpha$ if, for any $\epsilon>0$,

\begin{equation*}
\lim_{n\to \infty} \Pr(\left\lVert\vx_{n}-\valpha\right\rVert>\epsilon)
\end{equation*}

Note that $\left\| \vx_n - \valpha\right\| $ is the Euclidean distance 

\begin{equation*}
\left[\left(\vx_n - \valpha\right)'\left(\vx_n - \valpha\right)\right]^{1/2} = \sqrt{(x_{1n}-\alpha_1)^2 + ... + (x_{Kn} - \alpha_K)^2} = \left\| \vx_n - \valpha\right\| 
\end{equation*}

Therefore, 

\begin{equation*}
\vx_n \pto \valpha\quad \mbox{iff}\quad \Pr\left[\sqrt{\sum_{j = 1}^k\left(x_{j,n}- x_j\right)^2}> \epsilon\right]\pto 0
\end{equation*}
%
as $n\to \infty$ for $\epsilon >0$ and $\forall j = 1,...,k$, where:

\begin{equation*}
\vx_n = \begin{pmatrix}
          x_{1n} \\
          \vdots \\
          x_{kn}
        \end{pmatrix}
        \quad
        \mbox{and}
        \quad
\valpha = \begin{pmatrix}
            \alpha_1 \\
            \vdots \\
            \alpha_{k}
          \end{pmatrix}
\end{equation*}

\begin{remark}
  $\vx_n \pto \valpha$ if and only if $x_{jn}\pto \alpha_j$ for $j =1, ...,k$. That is, vector convergence in probability is equivalent to component convergence in probability for each component. See our previous discussion of vector sequence.
\end{remark}

\begin{definition}[Probability Limits of Matrices (and Vectors for $k = 1$)]
Let $\left\lbrace \mY_n \right\rbrace$ be a sequence of $m\times k$ random matrices. Then

\begin{equation*}
  \plim \begin{pmatrix}
          Y_n\left[1, 1\right] & \hdots & Y_n\left[1, k\right] \\
          \vdots & \ddots & \vdots \\
          Y_n\left[m, 1\right] & \hdots & Y_n\left[m, k\right]
        \end{pmatrix} = \begin{pmatrix}
          \plim Y_n\left[1, 1\right] & \hdots & \plim Y_n\left[1, k\right] \\
          \vdots & \ddots & \vdots \\
          \plim Y_n\left[m, 1\right] & \hdots & \plim Y_n\left[m, k\right]
        \end{pmatrix}
\end{equation*}
\end{definition}

The expectation $\E(\cdot)$ is a linear operator, that is, \textbf{we cannot} state that $\E\left[\exp(\widehat{\theta})\right] = \exp\left[\E\left(\widehat{\theta}\right)\right]$. Thus, we would like to know if the $\plim$ has the same property. Fortunately, the continuous mapping theorem tell us that we can interchange them. 

\begin{theorem}[Continuous Mapping Theorem]\label{teo:continuous_prob}
	Given a continuous function $g(X)$, if $X_n\pto X$ then $g(X_n)\pto g(X)$ as $n\to \infty$, or equivalently, $\plim \left[g(X_n)\right] = g\left[\plim (X_n)\right]$.
\end{theorem}

The Continuous Mapping Theorem is a very useful theorem. Unlike the expectation operator, it shows that the $\plim$ operator passes through nonlinear functions, provided they are continuous. The lack of this property for the $\E$ operator makes finite sample analysis difficult for many estimators. 


It is useful to know the vector form of this Theorem. Let $\vg:\SR^K\to\SR^J$ be a function continuous at some point $\valpha \in \SR^K$. Then, 

\begin{equation*}
  \vx_n \pto \valpha \implies \vg(\vx_n)\pto \vg(\valpha),
\end{equation*}
%
if $\vg(\cdot)$ is continuous at $\plim \vx_n$.


Now that we have presented the meaning of convergence in probability, it is time to define what we understand for ``consistency'' in econometrics.

\begin{definition}[Consistent Estimator]\label{def:consistent_est}\index{consistent estimator}
  An estimator $\widehat{\vtheta}_n$ of a parameter $\vtheta$ is a consistent estimator $\vtheta$ if and only if
  
  \begin{equation*}
    \plim \widehat{\vtheta}_n = \vtheta,
  \end{equation*}
%
which can also be written as:

  \begin{equation*}
    \widehat{\vtheta}_n \pto \vtheta.
  \end{equation*}
  
\end{definition}

In words, a \textbf{consistent estimator} is an estimator---a rule for computing estimates of a parameter $\vtheta$---having the property that as the number of data used increases without bound, the resulting sequence of estimates converges in probability to $\vtheta$. This means that the distributions of the estimates become more and more concentrated near the true value of the parameters being estimated, so that the probability of the estimator being arbitrary close to $\vtheta$ converges to one. 

\begin{remark}
Convergence in probability is also referred to as weak consistency, and since this has been the most familiar stochastic convergence concept in econometric, the word ``weak'' if often simply dropped.
\end{remark}

\begin{example}\label{example:ols1}
In this example, we will show that the OLS estimator is consistent under the following assumptions:
\begin{enumerate}
\item $y_i = \vx_i^\top\vbeta_0 + \epsilon_i,\; i = 1, ..., n; \vbeta_0\in \SR^k$;
\item $\mX^\top\vepsi/n \pto \vzeros$;
\item $\mX^\top\mX/n\pto \mM$, finite and positive definite. 
\end{enumerate}

The sampling error is:

\begin{equation}\label{eq:sampling_error_ols}
\widehat{\vbeta}_n = \vbeta_0 + \left(\frac{\mX^\top\mX}{n}\right)^{-1}\frac{\mX^\top\vepsi}{n}.
\end{equation}

 Since $\mX^\top\mX/n\pto \mM$, if follows from Theorem \ref{teo:continuous_prob} that 
 \begin{equation}
 \det\left(\frac{\mX^\top\mX}{n}\right)\pto \det(\mM).
 \end{equation}

Because $\mM$ is positive definite, $\det\det\left(\mM\right) > 0$. It follows that for all $n$ sufficiently large $\det\det\left(\frac{\mX^\top\mX}{n}\right) > 0$, so $\left(\frac{\mX^\top\mX}{n}\right)^{-1}$ exists for all $n$ sufficiently large. Hence $\widehat{\vbeta}_n$ in Equation \eqref{eq:sampling_error_ols} exists for all $n$ sufficiently large. It follows from Theorem \ref{teo:continuous_prob} that

\begin{equation}
\widehat{\vbeta}_n\pto \vbeta_0 + \mM^{-1}\vzeros = \vbeta_0, 
\end{equation}
%
given (b) and (c)
\end{example}
\begin{definition}[Strong Convergence in Probability]
	A sequence of random variables $\left\{X_n\right\}$ \textbf{convergence in probability strongly, or, almost surely} to a constant (non-random) $\alpha$ if, for any $\epsilon>0$,
	
	\begin{equation*}
	 \Pr\left(\lim_{n\to \infty}X_n = \alpha\right)=1
	\end{equation*}
	
	This is written $X_n\asto \alpha$, as $n\to \infty$. An equivalent condition for almost sure convergence is
	
	\begin{equation*}
	  \lim_{n\to \infty}\Pr\left(\left|X_m \to X\right| < \epsilon, \forall m\geq n\right) = 1
	\end{equation*}
\end{definition}

The extension to random vector is analogous to that for convergence in probability. Note also that this concept is stronger than convergence in probability; that is, if a sequence converges almost surely, the it converges in probability. 

\begin{remark}
  $\asto \implies \pto$
\end{remark}


%In the previous definitions of convergence, the limit is a constant (i.e., a real number). The limit can be a random variable. We say that a sequence of $K$-dimensional random variables $\left \lbrace \vx_n\right\rbrace$ converges to a $k$-dimensional random variable $\vx$ and write $\vx_n\pto \vx$ if $\left\lbrace \vx_n - \vx\right\rbrace$ converges to $\vzeros$. The same applies for almost sure convergence.

\subsection{Convergence in Quadratic Mean}

We will make frequent use of a special case of convergence in probability, \textbf{convergence in mean square} or \textbf{convergence in quadratic mean}

\begin{theorem}[Convergence in Quadratic Mean]\label{teo:conv_qm}
If $X_n$ has mean $\mu_n$ and variance $\sigma^2_n$ such that the ordinary limits of $\mu_n$ and $\sigma^2_n$ are $c$ and 0, respectively, then $X_n$ converges in mean square to c,

\begin{equation*}
  X_n\qmto c
\end{equation*}
%
and 

\begin{equation*}
  \plim X_n = c.
\end{equation*}
\end{theorem}

This theorem implies that $X_n\qmto c \implies X_n\pto c$. The conditions for convergence in mean square are usually easier to verify than those for the more general form.

The vector form of this type of convergence is the following. We say that the sequence of random vectors $\vx_1, \vx_2,...,\vx_n$ converges in quadratic mean to the random vector $\vz$ if $\E(\vx_n\vx_n')$ and $\E(\vx\vx')$ exists for all $n$ if

\begin{equation*}
  \lim_{n\to \infty} \E\left[\left(\vx_n  - \vx\right)'\left(\vx_n  - \vx\right)\right] = \vzeros
\end{equation*}

A special case of convergence in quadratic mean occurs when $\vx$, instead of being a random vector, is a vector of unknown parameters, say $\vtheta$, and $\vx_n$ is an estimator for $\vtheta$. Under these circumstances we can write:

\begin{equation}\label{eq:biase_var}
  \begin{aligned}
    \E\left[\left(\vx_n  - \vx\right)'\left(\vx_n  - \vx\right)\right] & = \left(\E\left[\vx_n\right]  - \vtheta\right)'\left(\E\left[\vx_n\right]  - \vtheta\right) + \E\left[\left(\vx_n - \E\left[\vx_n\right]\right)'\left(\vx_n - \E\left[\vx_n\right]\right)\right] \\
    & = \sum_{k = 1}^K\mbox{bias}^2(x_{kn}) + \sum_{k = 1}^K \var(x_{kn})
  \end{aligned}
\end{equation}
%
where $x_{kn}$ is the $k$th element of $\vx_n$ that is assumed to be $K$ dimensional. Thus from (\ref{eq:biase_var}) $\vx_n$ converges to $\vzeros$ in quadratic mean if and only if the bias and variance of $\vx_n$ approach zero as $n\to\infty$. This result, and the fact that Chebyshev's inequality can be used to prove that convergence in quadratic mean implies convergence in probability. See below.


An useful theorem is the following:

\begin{theorem}[Consistency of the sample mean]\label{teo:consistency_sample_mean}
The mean of a random sample from any population with finite mean $\mu$ and finite variance $\sigma^2$ is a consistent estimator of $\mu$.
\end{theorem}

\begin{proof}[Proof of consistency of the sample mean]
Since $\E(\overline{X}_n) = \mu$ and $\var(\overline{X}_n) = \sigma^2/n$. Therefore, using Theorem \ref{teo:conv_qm} (Convergence in quadratic mean)

\begin{equation*}
\overline{X}_n \qmto \mu \implies \overline{X}_n \pto \mu
\end{equation*}
\end{proof}

\begin{theorem}[Sufficient Conditions for Consistency]\label{teo:chebyshev}
Chebyshev's inequality implies that a sufficient conditions for an estimator based on a sample of size $n$, say $\widehat{\theta}_n$, say to be consistent for $\theta$ are:

\begin{equation*}
  \begin{aligned}
    \lim_{n \to \infty} \E\left(\widehat{\theta}_n\right) & = \theta_0 \\
    \lim_{n \to \infty} \var\left(\widehat{\theta}_n\right)& = 0
  \end{aligned}
\end{equation*}

If these two requirements are met, then:

\begin{equation*}
\widehat{\vtheta}_n\pto \vtheta
\end{equation*}
\end{theorem}

\begin{proof}[Proof of consistency of unbiased estimator]
  Since $\widehat{\theta}_n$ is unbiased, using Chebyshev's inequality \ref{definition:chebyshev_ineq} we obtain:
  
  \begin{equation*}
    \Pr\left[\left| \widehat{\theta}_n - \theta\right|\geq \delta \right]\leq \frac{\var(\widehat{\theta}_n)}{\delta ^2}
  \end{equation*}
  
  If $\lim_{n\to\infty}\var(\widehat{\theta}(X_1,...,X_n)) = 0$, then $\Pr\left[\left| \widehat{\theta}_n - \theta\right|\geq \delta \right]\to 0$, so $\widehat{\theta}_n\pto \theta$
\end{proof}

\begin{example}
For the normal case, we have that $\E(s^2) = \sigma^2$ and $\var(s^2) = 2\sigma^4 / (n - 1)\to 0$ as $n\to \infty$, hence $s^2\pto \sigma^2$
\end{example}


\begin{example}
For the Bernoulli case, we know that $\E(\overline{X}) = \theta$ and $\var(\overline{X}) = \theta (1- \theta)/ n \to 0$ as $n\to \infty$, hence $\overline{X}\pto \theta$
\end{example}

Therefore, another alternative method for proving that some estimator $\widehat{\vtheta}$ is consistent is to demonstrate that its unbiased and its covariance matrix approaches zero as $n\to\infty$.

\begin{remark}
Theorem \ref{teo:chebyshev} (Consistency of Unbiased Estimator) is only a sufficient condition for consistency. Failing to satisfy this condition does not necessarily imply that the estimator is inconsistent. 
\end{remark}

%It is worth emphasizing that although convergence in quadratic mean implies convergence in probability, the converse is not true. Therefore if $\lim (\E(\vx_n) - \vtheta) = \vzeros$ and $\lim \E\left[\left(\vx_n - \E\left[\vx_n\right]\right)'\left(\vx_n - \E\left[\vx_n\right]\right)\right] = 0$, then these conditions are sufficient to prove that $\vx_n$ is a consistent estimator for $\vtheta$, that is, $\plim \vx_n = \vtheta$, but they are not necessary. 

\begin{theorem}[Rules for probability limits]\label{teo:rules_pto}
  If $X_n$ and $Y_n$ are random variables with $X_n \pto c$ and $Y_n\pto d$, then:
  
  \begin{enumerate}
    \item Sum rule:
        \begin{equation}
            X_n + Y_n \pto c + d
        \end{equation}
    \item Product rule:
        \begin{equation}
            X_nY_n \pto cd
        \end{equation}
    \item Ratio rule:
        \begin{equation}
            X_n/Y_n \pto c/d \quad \mbox{if $d\neq 0$}
        \end{equation}
    \item Matrix inverse rule: If $\mW_n$ is a matrix whose elements are random variables and if $\mW_n\pto \mOmega$, then
        \begin{equation}
          \mW_n^{-1}\pto \mOmega^{-1}
        \end{equation}
    \item Matrix product rule: If $\mX_n$ and $\mY_n$ are random matrices with $\mX_n\pto \mA$ and $\mY_n\pto \mB$, then
        \begin{equation}
          \mX_n\mY_n\pto \mA\mB
        \end{equation}
  \end{enumerate}
\end{theorem}

\begin{example}[Plims of Scalar Additive and Multiplicative Functions]
Let $\mA = \begin{pmatrix} 2 & 1 \\ 1 & 1 \end{pmatrix}$, and $\left\lbrace \mX_n \right\rbrace$ be such that $\plim \mX_n = \begin{pmatrix} 2 \\ 5 \end{pmatrix}$. Then, 

\begin{equation*}
  \plim\left(\mA\mX_n\right) = \mA\plim\left(\mX_n\right) = \begin{pmatrix} 9 \\ 7 \end{pmatrix}
\end{equation*}
\end{example}

\begin{example}[Plims of Matrix Functions to Constant Matrices]
Let $\left\lbrace \mY_n \right\rbrace$ be such that $\plim \mY_n = \begin{pmatrix} 1 & 2 \\ 2 & 1 \end{pmatrix}$ and $\left\lbrace \mX_n \right\rbrace$ be such that $\plim \mX_n = \begin{pmatrix} 3 & 1 \\ 2 & 1 \end{pmatrix}$. Then:

\begin{equation*}
\plim \left(\mX_n\mY_n\right) = \plim\left(\mX_n\right)\plim \left(\mY_n\right) = \begin{pmatrix} 3 & 1 \\ 2 & 1 \end{pmatrix}\begin{pmatrix} 1 & 2 \\ 2 & 1 \end{pmatrix} =  \begin{pmatrix} 5 & 7 \\ 4 & 5 \end{pmatrix}
\end{equation*}

and

\begin{equation*}
\plim \left(\mX_n^{-1}\mY_n\right) = \plim\left(\mX_n\right)^{-1}\plim \left(\mY_n\right) = \begin{pmatrix} 1 & -1 \\ -2 & 3 \end{pmatrix}\begin{pmatrix} 1 & 2 \\ 2 & 1 \end{pmatrix} =  \begin{pmatrix} -1 & 1 \\ 4 & -1 \end{pmatrix}
\end{equation*}
\end{example}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Law of Large Numbers}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Much of the work of an econometrician, and also of a student of econometrics, is to determine whether an estimator is consistent. Fortunately, the `law of large numbers' will greatly simplify this work. Roughly speaking, law of large numbers (LLN) are theorems for convergence in probability in the special case where the sequence $\left\lbrace X_n \right\rbrace$ is a sample average, i.e., $X_n = \bar{X}_n$ where:

\begin{equation*}
\bar{X}_n = \frac{1}{n}\sum_{i = 1}^n X_i.
\end{equation*}

Thus, a LLN provides a much easier way to establish the probability limit of a sequence than the alternatives of the $(\delta, \epsilon)$ definition of the probability given previously. 

%We are particularly interested in the asymptotic behavior of sample means because the OLS estimate can be written as sample averages. Recall that 

%\begin{equation*}
%\widehat{\vbeta}_n = \left(\frac{1}{n}\sum_{ i = 1}^n\vx_i\vx_i'\right)
%\end{equation*}

%For example, we are interested in the behavior of $\sum_{i = 1}^n x_i\epsilon_i/n = \overline{x\epsilon}_n$ as $n\to \infty$. Different assumptions about the stochastic properties of $x_i$ and $\epsilon_i$ lead to different %properties of $x_i^2$ and $x_i\epsilon_i$ and hence different LLN (and CLT).

%For the data different \textbf{sampling schemes} assumptions include:

%\begin{enumerate}
%	\item Simple Random Sampling: This scheme is when we randomly draw $(y_i, x_i)$ from the population. Then $x_i$ are i.i.d. So $x_i^2$ are i.i.d, and $x_i\epsilon_i$ are i.i.d if the errors $\epsilon_i$ are i.i.d.
%	\item Fixed regressors: This occurs in an experiment where we fix the $x_i$ and observe the resulting random $y_i$. Given $x_i$ fixed and $\epsilon_i$ i.i.d it follows that $x_i\epsilon_i$ are inid (even if $\epsilon_i$ are i.i.d), while $x_i^2$ are nonstochastic.
%	\item Exogenous Stratified Sampling: This occurs when oversample some values of $x$ and undersample others. The $x_i$ are inid, so $x_i\epsilon_i$ are inid (even if $\epsilon_i$ are i.i.d) and $x_i^2$ are inid.
%\end{enumerate}

Let us start with the simplest LLN's definition. 

\begin{theorem}[Khinchine's Weak Law of Large Numbers]\label{theorem:KhinchineWLLN}
	Let $\left\lbrace X_n\right\rbrace $ be an \textbf{i.i.d random sample} with $\E(X_i)=\mu$, and let $\bar{X}_n=\frac{1}{n}\sum_{i=1}^nX_i$. Then:
	
	\begin{equation*}
	\lim_{n\to \infty}\Pr\left[\left|\bar{X}_n-\mu\right|>\epsilon \right]=0
	\end{equation*} 
	
	or equivalently,
	
	\begin{equation*}
	\lim_{n\to \infty}\Pr\left[\left|\bar{X}_n-\mu\right|\leq \epsilon \right]=1.
	\end{equation*} 
	
	In other words,
	
	\begin{equation*}
	\frac{1}{n}\sum_{i=1}^nX_i\pto \E(X_i)
	\end{equation*}
%
	or  $\plim \bar{X}_n=\mu$
\end{theorem}

The WLLN shows that the estimator $\hat{\mu}=\bar{X}_n$ converges in probability to the true population mean $\mu$. Another important feature of this theorem is that it does not require the existence of moments higher order than the mean. This is a powerful result that is very convenient when we have an i.i.d sample. Moreover, this theorem will simply our proofs when we encounter sample moments such as $n^{-1}\sum_{i = 1}^n\vx_i\vx_i'$.

\begin{example}
Consider a random sampling from a population with mean $\mu_n$ and variance $\sigma_n^2$. What is the probability limit of $\widehat{\theta}_n = \bar{x}^2_n / s^2_n$? By the ratio rule in Theorem \ref{teo:rules_pto}

\begin{equation*}
  \plim \frac{\bar{x}^2_n}{s^2_n} = \frac{\plim \bar{x}^2_n}{\plim s^2_n}
\end{equation*}

Note that

\begin{equation*}
  \begin{aligned}
  \plim \bar{x}^2_n & = (\plim \bar{x}_n)^2 \quad \mbox{by Theorem \ref{teo:continuous_prob}}\\
                    & = \mu^2 \quad \mbox{by LLN \ref{theorem:KhinchineWLLN}}
  \end{aligned}
\end{equation*}

  Since $s^2_n$ is consistent $s^2_n\pto\sigma^2$, then
  
\begin{equation*}
  \plim \frac{\bar{x}^2_n}{s^2_n} = \frac{\mu^2}{\sigma^2}
\end{equation*}
\end{example}

\begin{remark}
  Theorem \ref{theorem:KhinchineWLLN} (Khinchine's Weak Law of Large Numbers) is widely used in econometric because the estimators involve averages. Note also that LLN is much easier way to get the $\plim$ than use of Definition \ref{definition:convergence_in_probability} (Convergence in Probability) or Theorem \ref{teo:conv_qm} (Convergence in Quadratic Mean).
\end{remark}

\begin{example}[Example of mean from normal]
  Consider we have $n$ different samples with pdf $\rN(1, 0.5^2)$. For example $X_1$ is the fist sample with just one observation that comes from a $\rN(1, 0.5^2)$,  $X_2$ is the second sample with two observations $(X_1, X_2)$ which also comes from a $\rN(1, 0.5^2)$; $X_3$ with three observations $(X_1, X_2, X_3)$ and so on. Note that each sample (or sequence) is a i.i.d. random sample with $\E(X_i) = \mu$. The mean for each sequence is also a sequence:
  
\begin{equation*}
  \begin{aligned}
    \bar{X}_1 & = g(X_1) = X_1 \\
    \bar{X}_2 & = g(X_1, X_2) = \frac{1}{2}\sum_{i = 1}^2X_i \\
    \bar{X}_3 & = g(X_1, X_2, X_2) = \frac{1}{3}\sum_{i = 1}^3X_i \\
    \vdots \\
    \bar{X}_n & = g(X_1, ..., X_n) = \frac{1}{n}\sum_{i = 1}^nX_i
  \end{aligned}
\end{equation*}

Now in R, we will show that the mean will converge to the true population mean $\mu = 1$, as $n\to \infty$.

<<CLT-normal, eval = FALSE>>=
# Setup
set.seed(123)  # set the seed
N <- 10000     # total number of observations
n <- 1:N       # vector: n = 1, 2, ..., N

n_dat <- rnorm(n = n, mean = 1, sd = 0.5) # Sample from N(1, 0.5^2)
xbar <- cumsum(n_dat) /  n                # Cumulated mean
plot(n, xbar, type = "l", ylab = expression(bar(x)[n]))
abline(h = 1.01, col = "blue", lty = 2)
abline(h = 0.99, col = "blue", lty = 2)
@


\begin{figure}[H]
  \caption{Convergence of mean from normal distribution}
    \label{fig:CLTnormal}
        \centering
    \begin{minipage}{.9\linewidth}
<<CLT-normalplot, echo =  FALSE, fig.align='center', out.width = '10cm', out.height = '10cm', cache = TRUE>>=
set.seed(123)
N <- 10000
n <- 1:N
n_dat <- rnorm(n = n, mean = 1, sd = 0.5)
xbar <- cumsum(n_dat) /  n
par(mfrow = c(1, 1), mar = c(4, 4, 1, 1) + 0.1)
plot(n, xbar, type = "l", ylab = expression(bar(x)[n]))
abline(h = 1.01, col = "blue", lty = 2)
abline(h = 0.99, col = "blue", lty = 2)
@
\footnotesize
		\emph{Notes: This graphs shows the convergence of $\bar{X}$ as $n\to\infty$ for a normal distribution.}
	\end{minipage}
\end{figure}

From Figure \ref{fig:CLTnormal} we can see that $\overline{X}_n$ gets arbitrarily close to $\mu$ as $n$ increases indefinitely. In words, as the sample size $n$ increases, the sample mean converges to the theoretical mean. 
\end{example}

\begin{example}[Tossing a fair coin]
Now we simulate $n = 1000$ coin tosses. After each simulated toss, we plot the proportion $X_n$ of heads obtained so far against the number $n$ of tosses so far. The LLN says we should see a trace that gets very close to $1/2$ as $n$ increases.

In R, the code is:

<<CLT-binomial, eval = FALSE>>=
# Set up
set.seed(123)                            # set seed
N <- 10000                               # total number of tosses
n <- 1:N                                 # vector: n = 1, 2, ...., N; Toss number

# Simulate and plot
h <- rbinom(n = n, size = 1, prob = 1/2) # vector: H = 0 or 1 each with p = 1./2
x <- cumsum(h) / n                       # vector: proportion of heads
plot(n, x, type = "l", ylim = c(0, 1), ylab = expression(bar(x)[n]))
abline(h = 0.52, col = "blue", lty = 2)
abline(h = 0.48, col = "blue", lty = 2)
@

\begin{figure}[H]
  \caption{Convergence of mean from binomial distribution}
    \label{fig:CLTbinomial}
     \centering
    \begin{minipage}{.9\linewidth}
<<CLT-binomialPlot, echo =  FALSE, fig.align='center', out.width = '10cm', out.height = '10cm', cache = TRUE>>=
set.seed(123)
N <- 10000                               # total number of tosses
n <- 1:N                                 # vector: n = 1, 2, ...., N; Toss number

# Simulate and plot
h <- rbinom(n = n, size = 1, prob = 1/2) # vector: H = 0 or 1 each with p = 1./2
x <- cumsum(h) / n                       # vector: proportion of heads
par(mfrow = c(1, 1), mar = c(4, 4, 1, 1) + 0.1)
plot(n, x, type = "l", ylim = c(0, 1), ylab = expression(bar(x)[n]))
abline(h = 0.52, col = "blue", lty = 2)
abline(h = 0.48, col = "blue", lty = 2)
@
\footnotesize
		\emph{Notes:} This graphs shows the convergence of $\bar{X}$ as $n\to\infty$ for a binomial distribution.
	\end{minipage}
\end{figure}

Note that the $n$th element of the vector \texttt{x} is the mean of the first $n$ elements of \texttt{h}. Figure \ref{fig:CLTbinomial} shows that the mean from a binomial distribution converges to the population mean $\mu = p = 1/2$, as $n\to \infty$. Note that the dashed lines at 0.48 and 0.52 illustrate the LLN with $\epsilon = 0.02$
\end{example}

To apply LLN for several variables we have to know that summands of iid different random variables are also i.i.d.

\begin{proposition}
	Let $\vg:\SR^K\to \SR^l$ be a continuous function. (i) Let $\mX_i$ and $\mX_t$ be identically distributed. Then $\vg(\mX_i)$ and $\vg(\mX_t)$ are identically distributed. (ii) Let $\mX_i$ and $\mX_t$ be independent. Then $\vg(\mX_i)$ and $\vg(\mX_t)$ are independent.
\end{proposition}

Therefore, using this proposition we can state the following proposition:

\begin{proposition}\label{proposition:iid_transf}
	If $\left\{(\mZ_i^\top,\mX_i,\vepsi_i)\right\}$ is an i.i.d random sequence, then $\left\{\mX_i\mX_i^\top\right\}$, $\left\{\mX_i\vepsi\right\}$, $\left\{\mZ_i\mX_i^\top\right\}$, $\left\{\mZ_i\vepsi_i\right\}$, and $\left\{\mZ_i\mZ_i^\top\right\}$ are also i.i.d sequences. 
\end{proposition}

This result is useful in situations in which we have observations from a random sample, as in a simple cross section. The result does not apply to stratified cross sections since there the observations are not identically distributed across strata, and generally will not apply to time-series data since there the observations $(\mX_i, \vepsi_i)$ generally are not independent. For these situations, we need laws of large numbers that do not  impose the i.i.d assumption.


\begin{example}
In this example, we will show that the OLS estimator $\widehat{\vbeta}_n \asto \vbeta_0$. Assume the following:

\begin{enumerate}
\item $y_i = \vx_i^\top\vbeta_0 + \epsilon_i,\; i = 1, ..., n; \vbeta_0\in \SR^k$;
\item the sample $\lbrace y_i, \vx_i^\top\rbrace$ is an i.i.d sequence;
\item $\E(\vx_i\epsilon_i) = \vzeros$;
\item $\E(\vx_i\vx_i^\top)= \mM$ is positive definite;
\end{enumerate}

Since $\left\lbrace \vx_i\right\rbrace$ is a i.i.d random sample by Assumption (b) (Random Sample), then $\left\lbrace \vx_i\vx_i^\top\right\rbrace$ is also i.i.d sequence by Proposition \ref{proposition:iid_transf}.

Note that each $(g,j)$ element of the $k\times k$ matrix $\vx_i\vx_i^\top$ is given by 

\begin{equation*}
\sum_{h = 1}^p x_{ihg}x_{ihj}.
\end{equation*}

By triangle inequality \ref{def:triangle-inequality}:

\begin{equation*}
  \left|\sum_{h = 1}^p x_{ihg}x_{ihj}\right| \leq \sum_{h = 1}^p \left|x_{ihg}x_{ihj}\right|. 
\end{equation*}

Then, by Cauchy-Schwarz inequality \ref{theorem:Cauchy-Inequality}:

\begin{equation*}
  \begin{aligned}
    \E\left|\sum_{h = 1}^p x_{ihg}x_{ihj}\right| & \leq \sum_{h = 1}^p  \E\left|x_{ihg}x_{ihj}\right|\\
                                               & \leq   \sum_{h = 1}^p\left\lbrace \left(\E\left|x_{ihg}\right|^2\right)^{1/2} \left(\E\left|x_{ihj}\right|^2\right)^{1/2}\right\rbrace
  \end{aligned}
\end{equation*}

It follows that the elements of the $\vx_i\vx_i^\top$ will have $\E\left|\sum_{h = 1}^p x_{ihg}x_{ihj}\right| < \infty$ provided simply that $\E\left|x_{ihg}\right|^2 < \infty$ for all $h = 1, ..., p$ and $g = 1, ..., k$.  Thus, by Theorem \ref{theorem:KhinchineWLLN} and assuming that $\E\left|x_{ihg}\right|^2 < \infty$, then 

\begin{equation}
  n^{-1}\sum_{i = 1}^n \vx_i\vx_i^\top\pto \mM
\end{equation}

Similarly, $\left\lbrace \vx_i\epsilon_i\right\rbrace$ is also i.i.d sequence by Proposition \ref{proposition:iid_transf}. Using our previous reasoning:

\begin{equation}
  n^{-1}\mX^\top\vepsi = n^{-1}\sum_{i = 1}^n \vx_i\epsilon_i \pto \E(\vx_i\epsilon_i) = \vzeros
\end{equation}
%
if  $\E\left|x_{ihg}\epsilon_{ih}\right| < \infty$ for all $h = 1, ..., p$ and $g = 1, ..., k$. From here, we proceed as Example \ref{example:ols1}.

\end{example}


Another important feature is that Khinchine's WLLN  is broader than Theorem \ref{teo:chebyshev} (Consistency of Unbiased Estimator), as \textbf{it does not require that the variance of the distribution be finite}. On the other hand, it is not broad enough, because most of the situations we encounter where we will need a result such as this will not involve i.i.d. random sampling. A broader LLN Theorem is the following:

\begin{theorem}[Chebychev's Weak Law of Large Numbers]\label{theorem:ChebychevWLLN}
  If $X_i, i = 1,...,n$ is a sample of observations such that $\E(X_i) = \mu_i <\infty$ and $\var(X_i)=\sigma^2_i < \infty$ such that
  
\begin{equation*}
  \frac{\bar{\sigma}^2_n}{n} = \frac{\sum_{i = 1}^n\sigma_i^2}{n^2}\to 0\quad \mbox{as $n\to\infty$},
\end{equation*}
%
then

\begin{equation*}
  \overline{X}_n - \bar{\mu}_n \pto 0.
\end{equation*}
\end{theorem}

The Chebychev's theorem does not state that $\overline{X}_n$ converges to $\bar{\mu}_n$, or even that it converges to a constant at all. The theorem states that as $n$ increases without bound, these two quantities will be arbitrarily close to each other. In other words, the difference between them converges to a constant, zero. The more important difference between the Khinchine and Chebyshev theorems is that the second allows for heterogeneity in the distributions of the random variables that enter the mean. This will be very useful in cases where the independence assumption may hold but the identical distribution assumption does not (such as random sampling with cross-sectional data). For example, the $X_i$'s may have different means and/or variances for each $i$. If we retain the independent assumption but relax the identical distribution assumption, then we can still get convergence of the sample mean.

It is important to stress that the behavior of the variance of $\overline{X}_n$ is the key element in this \textbf{LLN}. Independence implies that all covariances among the $X_i$ are zero, so that the variance of $\overline{X}_n$ simplifies to the sum of the variances of the $X_i$ divided by $n^2$. Then the key mechanism is that the variance of $\overline{X}_n$ converges to zero:

\begin{equation*}
 \lim_{n\to\infty}\var\left[ \overline{X}_n\right]=\lim_{n\to\infty}\frac{1}{n^2}\sum_{n=1}^n\var\left(X_i\right)=\lim_{n\to\infty}\frac{\sum_{i = 1}^n\sigma^2_i}{n}=0 
\end{equation*}

To illustrate Chebychev's WLLN we have created artificial data sets from independent normal distributions with different mean and standard deviations. Figure \ref{fig:chebychev} displays the sequence $\bar{X}_n - \bar{\mu}_n$ in black line and $\bar{\sigma}^2 / n^2$ as $n\to \infty$ in a red line. It can be observed that both sequences change with sample size, but as the number of observations increases both settle down to zero.  However, note that the sequence $\bar{X}_n - \bar{\mu}_n$ converges in probability, whereas $\bar{\sigma}^2 / n^2\to 0$ in a deterministic way. 


\begin{figure}[H]
  \caption{Chebychev's Convergence}
    \label{fig:chebychev}
         \centering
    \begin{minipage}{.9\linewidth}
<<cheby-convergence, echo =  FALSE, fig.align='center', out.width = '10cm', out.height = '10cm', cache = TRUE>>=
set.seed(123)
N <- 50000
n <- 1:N
mu <- runif(n) * 1
sd <- runif(n) * 1
  
n_dat <- rnorm(n = n, mean = mu, sd = sd)
xbar <- cumsum(n_dat) /  n - cumsum(mu) / n
par(mfrow = c(1, 1), mar = c(4, 4, 1, 1) + 0.1)
plot(n, xbar, type = "l", 
     ylab = expression(list(bar(x)[n] - bar(mu)[n], bar(sigma)[n]^2/n^2)),
     ylim = c(-0.01, 0.1))
lines(n, cumsum(sd^2) / n^2, col = "red")
abline(h = 0.01, col = "blue", lty = 2)
abline(h = -0.01, col = "blue", lty = 2)
@
\footnotesize
		\emph{Notes: This graphs shows the convergence of $\bar{X}_n - \bar{\mu}_n$ and $\bar{\sigma}^2 / n^2$ as $n\to\infty$.}
	\end{minipage}
\end{figure}

\begin{remark}
  When the iid assumption is relaxed, stronger restrictions need to be place on the variances of each of the random variables. If some assumption are weakened then other assumptions must be strengthened. 
\end{remark}





%We can use this theorem in order to prove that the OLS variance estimator is consistent. Consider the i.i.d random variables $\epsilon_1^2, \epsilon_2^2, ..., \epsilon_N^2$, which have mean $\E\left[\epsilon_n^2\right] = \sigma^2$. From Khinchine's theorem it follows that:

%\begin{equation}
%\frac{1}{N}\sum_{i = 1}^N \epsilon_{i}^2 \pto \sigma^2
%\end{equation}

%which is the same as to say

%\begin{equation}
%\plim N^{-1}\vepsi'\vepsi = \sigma^2
%\end{equation} 

%To prove $\widehat{\sigma}^2 = \sigma^2$ consider the following estimator:

%\begin{eqnarray}
%\widehat{\sigma}^2 &=& \frac{\widehat{\vepsi}'\widehat{\vepsi}}{N - K}\nonumber \\
% & = & \frac{1}{N - K}\vepsi'\left(\mI - \mX\left(\mX'\mX\right)^{-1}\mX'\right)\vepsi \nonumber\\
% & = & \frac{N}{N - K}\left[\frac{\vepsi'\vepsi}{N} - \frac{\vepsi'\mX}{N}\left(\frac{\mX'\mX}{N}\right)^{-1}\frac{\mX'\vepsi}{N}\right]
%\end{eqnarray}

%Then, using Slutksy's theorem and the fact that $\plim \left[N/\left(N - K\right)\right] = 1$,

%\begin{eqnarray}
%\plim \widehat{\sigma}^2 & = & \plim \frac{\vepsi'\vepsi}{N} - \plim \frac{\vepsi'\mX}{N}\plim \left(\frac{\mX'\mX}{N}\right)^{-1}\plim \frac{\mX'\vepsi}{N} \\
%& = & \sigma^2 - 0 \cdot Q^{-1} \cdot 0 \\
%& = & \sigma^2
%\end{eqnarray}

%Before, we need the following theorems:

%\begin{theorem}[Chebychev's LLN]\label{theorem:Chebychev}
%Let $\left\lbrace X_n \right\rbrace$ be a sequence of i.i.d. random variables such that $\E\left[ X_n\right]$ and $\var\left[ X_n\right] $ exists $(n=1,2,3,...)$. Denote:

%\begin{equation*}
%E_N\left[X\right]\equiv \frac{1}{N}\sum_{n=1}^NX_n 
%\end{equation*}
%then $\E_N\left[X \right]\pto \E\left[ X\right]$ as $N\to\infty$
%\end{theorem}

%The behavior of the variance of $E_N\left[X \right]$ is the key element in this \textbf{LLN}. Independence implies that all covariances among the $X_n$ are zero, so that the variance of $\E_N\left[ X\right]$ simplifies to the sum of the variances of the $X_n$ divided by $N^2$. Then the key mechanism is that the variance of $\E_N\left[ X\right]$ converges to zero because the variance of the $X_n$ $(n-1,...,N)$ are all equal:

%\begin{equation*}
% \lim_{N\to\infty}\var\left[ \E_N\left[ X\right]\right]=\lim_{N\to\infty}\frac{1}{N^2}\sum_{n=1}^N\var[X]=\lim_{N\to\infty}\frac{\var[X]}{N}=0 
%\end{equation*}

%As a result, $\E_N[X]$ converges in distribution to the constant equal to its mean.


%==============================================
\section{Convergence in Distribution}
%==============================================


\begin{definition}[Convergence in Distribution]\label{definition:convergece_in_distribution}
	If the cdfs $F_{X_n}$ of the sequence of random variables $\left\lbrace X_n \right\rbrace $ converge to the cdf $F_{X}$ as $n\to \infty$ at all points $z$ where $F_{X}(z)$ is continuous, then $\left\lbrace X_n\right\rbrace $ converges in distribution to $X$. This will be denoted
	
	\begin{equation*}
	X_n\dto X
	\end{equation*}
%	
or 
	\begin{equation*}
	\lim_{n\to \infty}|F_{X_n}-F_{X}| = 0
	\end{equation*}
\end{definition}

This theorem states that the distribution of $X_n$ gets closer and closer to that of the random variable $X$, so that the distribution of $X$, the cdf $F_{X}$, can be used as an \textbf{approximation} to the distribution of $F_{X_n}$.  We can also say that $X$ es the \textbf{limiting distribution} of $X_n$.

%This statement is about the probability distribution associated with $X_n$; it does not imply that $X_n$ converges at all.

Convergence in distribution can be extended to random vectors and matrices although not in the element by element manner that we extended the earlier convergence forms. The reason is that convergence in distribution is a property of the CDF of the random variable, not the variable itself. Thus, $\vx_n\dto \vx$ if $\lim_{n\to \infty}|F_{\vx_n}-F_{\vx}| = 0$ and likewise for a random matrix. 


\begin{remark}
  One important case in which the limiting cdf $F$ is discontinuous is when $X$ is generate, meaning that it is identically equal to a constant $c$, so that $\Pr(X = c) = 1$. 
\end{remark}

\begin{remark}
In most applications, $X$ is either a normal or chi-square distributed random variable. 
\end{remark}


As an example, it is well know that 

\begin{equation*}
t_{n-1}\dto N(0, 1)
\end{equation*}

as $n\to \infty$.

\begin{theorem}[Convergence in probability implies convergence in distribution]\label{teo:pto_implies_dto}
  If the sequence of random variables $\left\lbrace X_n \right\rbrace$ convergences in probability to a random variable $X$, the sequence also converges in distribution to $X$. In other words:
  
  \begin{equation*}
    X_n\pto X\implies X_n\dto X
  \end{equation*}
\end{theorem}

Convergence in distribution is a weaker form of convergence than convergence in probability, in the sense that $\pto \implies \dto$. Intuitively, when $X_n$ converges to $X$ in probability as $n\to \infty$, the random variable $X_n$ will be arbitrarily close to random variable $X$ for $n$ sufficiently large. Therefore, the probability law of $X_n$ will be arbitrarily close to the probability law of $X$ for $n$ sufficiently large. That is, $X_n$ will converge in distribution to $X$ as $n\to \infty$.

However, $\dto$ does not imply $\pto$. When $\vx = \vtheta$ is a vector of constants the converse does hold. That is, it is also true that

\begin{equation*}
  \vx_n\dto \vtheta \implies \vx_n\pto \vtheta
\end{equation*}

In this case the limiting distribution of $\vx_n$ is degenerate since it collapses to the single point $\vtheta$.

\begin{example}[Defining Limiting Distribution Through Convergence in Probability]
Let $\left\lbrace Y_n\right\rbrace$ be defined by $Y_n = \left(2 + n^{-1}\right)X + 3$, where $X\sim \rN(1, 2)$. Using properties of $\plim$ operator if follows that

\begin{equation*}
  \plim \left(Y_n\right) = \plim \left[\left(2 + n^{-1}\right)X\right] + \plim(3) = 2X + 2 \sim \rN(5, 8). 
\end{equation*}
Then, Theorem \ref{teo:pto_implies_dto} implies that $Y_n\dto \rN(5, 8)$.
\end{example}

Another important result is that the moments of the asymptotic distribution of a random variable are not necessarily equal to the limits of the moments of the random variable's finite sample distribution. That is, in terms of the first two moments, $\vx_n\dto \vx$ does not necessarily imply that $\lim \E(\vx_n) = \E(\vx)$ and $\lim \E(\vx_n\vx_n') = \E(\vx\vx')$. For example, in simultaneous equation estimation, we frequently encounter estimator that do not possess finite moments of any order, but that, nevertheless, possess asymptotic distributions with well-defined moments.

\begin{example}
Consider a random sample $(y_1, y_2,...,y_n)$ from a normal distribution with mean $\mu \neq 0$ and variance $\sigma^2$. As an estimator for $\mu^{-1}$, the inverse of the sample mean $\bar{y}_n^{-1}$ is a natural choice. To establish its statistical properties we note that, from Khinchine's theorem, $\plim \bar{y}_n = \mu$, and then, from the continuous mapping theorem, $\plim \bar{y}_n^{-1} = \mu^{-1}$. Also because $\sqrt{n}(\bar{y}_n - \mu)\sim \rN(0, \sigma^2)$ for all $n$, it follows that

\begin{equation*}
  \sqrt{n}(\bar{y}_n - \mu)\dto \rN(0, \sigma^2)
\end{equation*}

Then, 

\begin{equation*}
  \sqrt{n}\left(\bar{y}_n^{-1} - \mu^{-1}\right)\dto \rN(0, \sigma^2\mu^{-4})
\end{equation*}

Thus the mean of the asymptotic distribution of $\bar{y}_n^{-1}$ is $\mu^{-1}$, but $\lim \E\left(\bar{y}_n^{-1}\right)\neq \mu^{-1}$ because it can be shown that $\E\left(\bar{y}_n^{-1}\right)$ does not exists. Note that this example also demonstrate that an estimator can be consistent, that is $\plim \bar{y}_n^{-1} = \mu^{-1}$, without its bias and variance going to zero as $n\to\infty$ ($\E\left(\bar{y}_n^{-1}\right)$ and $\var\left(\bar{y}_n^{-1}\right)$ do not exist.)
\end{example}

Some useful results that combine both probability and limiting distribution are as follows.

\begin{theorem}[Rules for limiting distribution]\label{teo:rules_dto}

Consider the following rules
\begin{enumerate}
  \item If $X_n\dto X$ and $Y_n\pto c$, then
      \begin{equation}\label{eq:rule_dtop}
         X_nY_n \dto cX
      \end{equation}
%
      which means that the limiting distribution of $X_nY_n$ is $cX$. Also, 
        \begin{equation}\label{eq:rule_dtop_2}
            X_n + Y_n \dto X + c
        \end{equation}
        
        \begin{equation}\label{eq:rule_dtop_3}
            X_n / Y_n \dto X / c, \quad \mbox{if $c\neq 0$}
        \end{equation}
        
    \item If $X_n\dto X$ and $g(X_n)$ is a continuous function, then
          \begin{equation}\label{eq:rule_dtop_4}
            g(X_n)\dto g(X)
          \end{equation}
    \item $\vx_n \dto \vx$, $\mA_n\pto \mA \implies \mA_n\vx_n \dto \mA\vx$, provided that $\mA_n$ and $\vx_n$ are conformable. In particular, if $\vx \sim \rN(\vzeros, \mSigma)$, then $\mA_n\vx_n\dto \rN(\vzeros, \mA\mSigma\mA')$.
    \item $\vx_n \dto \vx$, $\mA_n\pto \mA \implies \vx_n'\mA_n^{-1}\vx_n\dto \vx'\mA^{-1}\vx$, provided that $\mA_n$ and $\vx_n$ are conformable and $\mA$ is nonsingular.
\end{enumerate}
\end{theorem}

\begin{example}[Plims of Matrix Functions to Vector Random Variables]
Let $\left\lbrace \mX_n\right\rbrace$ and $\left\lbrace \mY_n\right\rbrace$ be such that $\plim\left(\mX_n\right) = \begin{pmatrix} 3 & 2 \\ 2 & 4 \end{pmatrix}$ and $\underset{(2\times 1)}{\mY_n}\dto\underset{(2\times 1)}{\mY}\sim \rN(\vzeros, \mI)$. Then, 

\begin{equation*}
  \mX_n\mY_n\dto \left[\plim\left(\mX_n\right)\right]\mY\sim \rN\left(\vzeros, \begin{pmatrix} 13 & 14 \\ 14 & 20\end{pmatrix}\right)
\end{equation*}

and

\begin{equation*}
  \mX_n^{-1}\mY_n\dto \left[\plim\left(\mX_n\right)\right]^{-1}\mY\sim \rN\left(\vzeros, \begin{pmatrix} .3125 & -.2188 \\ -.2188 & .2031\end{pmatrix}\right)
\end{equation*}
\end{example}

\begin{remark}
An useful example of Equation (\ref{eq:rule_dtop_4}) of Theorem \ref{teo:rules_dto} is the following. The exact distribution of $t_n^2$ is $F(1, n)$. But as $n\to \infty$, $t_n$ convergences to a standard normal variable. According to this result, the limiting distribution of $t_n^2$ will be that of the square of a standard normal, which is $\chi^2(1)$. Therefore, we conclude that:

\begin{equation*}
  F(1, n)\dto \chi^2(1)
\end{equation*}
\end{remark}


\begin{lemma}[Asymptotic Equivalence]\label{lemma:asymptotic_equiv}
  If $Y_n - X_n\pto 0$ and $X_n\dto X$ as $n\to \infty$, then $Y_n\dto X$
\end{lemma}

Intuitively, if two random variables $Y_n$ and $X_n$ are very close with probability approaching one as $n\to \infty$, they will follow the same large sample probability distribution. This lemma is very useful when one is interested in deriving the asymptotic distribution of $Y_n$. We can establish the asymptotic the asymptotic equivalence (in probability) between $Y_n$ and $X_n$ in the sense that $Y_n - X_n\pto 0$ as $n\to \infty$, then the asymptotic distributions of $Y_n$ and $X_n$ will be identical.\footnote{For example, this lemma is useful when deriving the distribution of spatial GLS is the same as the spatial FGSL. } 

\begin{theorem}[Cramer-Wold device]
If $\vx_n \dto \vx$, then $\vc'\vx_n \dto \vc'\vx$ for all conformable vectors $\vc$ with real valued elements.
\end{theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Central Limit Theorems}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Recall that we are interested in a way to describe the statistical properties of estimators when their exact distribution are unknown. However the previous tools do not allow us to find the limiting distribution. From Theorem \ref{teo:pto_implies_dto} (Convergence in probability implies convergence in distribution), we know that:

\begin{equation*}
  \widehat{\vtheta}_n\pto \vtheta \implies \widehat{\vtheta}\dto \vtheta.
\end{equation*}

That is, the limiting distribution of $\widehat{\vtheta}_n$ is a spike (the asymptotic distribution of $\widehat{\theta}_j$ collapses to a single point) and not very informative. The `trick' is to apply some normalization. For example, whereas $\widehat{\vtheta}_n\pto \vtheta$, we often find that

\begin{equation}
z_n = \sqrt{n}(\widehat{\vtheta}_n - \vtheta) \dto f(z),
\end{equation}
%
where $f(z)$ is a well-defined distribution with mean and positive variance. An estimator which has this properly is said to be \textbf{root-n consistent}.

For example, consider the sequence of sample means $\bar{X}_n = n^{-1}\sum_{i = 1}^nX_i$, such that $\bar{X}_1, \bar{X}_2, \bar{X}_3, ...$ We would like that the probability that $\bar{X}_n$ will deviate from $\mu$ by any amount that decreases to zero as $n\to \infty$. We now that $\E(\bar{X}_n ) = \mu$ and $\var(\bar{X}_n) = \sigma^2/n$. Thus, $\bar{X}_n$ will eventually converge to a constant $\mu$ since its variance will go to zero eventually for a large enough $n$. In other words, because $\var(\bar{X}_n)\to 0$ the distribution shrinks as $n\to\infty$. 


Now consider the sequence of variables:

\begin{equation}
  Z_n = \sqrt{n}\left(\bar{X}_n - \mu\right), \; n = 1, 2, ...
\end{equation}

For this variable we have $\E(Z_n) = 0$ and $\var(Z_n) = \sigma^2$ so that, as $n\to \infty$, the mean of $Z_n$ remains at zero, but its variance does not converges to zero. 


Central limit theorems, establish that, under some conditions, the arithmetic mean of a sufficiently large number of independent random variables, each with a finite expected value and finite variance, will be approximately normally distributed, regardless of the underlying distribution. 

The following Theorem gives the most classical (Central Limit Theorem) CLT. 


\begin{theorem}[Lindberg-Levy CLT (Univariate)]\label{theorem:LL_CLT_U}
Let $\left\lbrace X_n \right\rbrace$ be a sequence of i.i.d. random variables such that $\E(X_n) = \mu$ and the variance is strictly positive and finite, $0 < \sigma^2 < \infty$. Define $\overline{X}_n = n^{-1}\sum_{i = 1}^nX_i$. Then the distribution of

\begin{equation*}
  \begin{aligned}
      Z_n & = \frac{\overline{X}_n  - \E(\overline{X}_n)}{\sqrt{\var(\overline{X}_n)}} \\
          & = \frac{\overline{X}_n - \mu}{\sigma / \sqrt{n}} \\
      & = \frac{\sqrt{n}(\overline{X}_n - \mu)}{\sigma} \dto \rN(0, 1)
  \end{aligned}
\end{equation*}
%
as $n$ approaches infinity. This is the same as:
 
 
 \begin{equation*}
  \sqrt{n}(\overline{X}_n - \mu) = \frac{1}{\sqrt{n}}\sum_{i = 1}^n(X_i - \mu)\dto \rN(0, \sigma^2)
 \end{equation*}
\end{theorem}

This theorem tell us that if a large random sample is taken from any population distribution with finite variance, regardless of whether this population distribution is discrete or continuous, then the distribution of the standardized sample mean

\begin{equation*}
Z_n = \frac{\sqrt{n}(\overline{X}_n - \mu)}{\sigma}
\end{equation*}
%
will approximately follow a \rN(0, 1). Therefore, for each finite $n$, the distribution of $\overline{X}_n$ will be approximately a $\rN(\mu, \sigma^2/n)$.\\

It is important to stress that CLT does not say that a large population is approximately normally distributed. It says nothing about the distribution of the population; it is only a statement about the approximate distribution of a standardized sample mean $Z_n$. 

\begin{remark}
  Sometimes CLT is interpreted incorrectly as implying that the distribution of $\overline{X}_n$ approaches a normal distribution as $n\to \infty$. This is incorrect because $\var(\overline{X}_n)\to 0$ and $\overline{X}_n$ converges to a degenerate distribution $F(\cdot)$ such that $F(x) = 0$ if $x < \mu$ and $F(x) = 1$ if $x\geq \mu$.
\end{remark}

Multivariate versions of the CLTs can be obtained where each individual $\vx_i$ is a random vector in $\SR^K$, 


\begin{equation*}
\vx_i =\begin{pmatrix}
        x_{i1}\\
        x_{i2} \\
        \vdots\\
        x_{iK}
      \end{pmatrix}  
\end{equation*}
%
with mean vector:

\begin{equation*}
\vmu = \E(\vx_i) = \begin{pmatrix}
          \mu_1 \\
          \mu_2 \\
          \vdots \\
          \mu_K
       \end{pmatrix},
\end{equation*}
%
and covariance matrix $\mQ$. Then the sum of the random vectors will be componentwise, that is:

\begin{equation*}
\begin{pmatrix}
        x_{11}\\
        x_{12} \\
        \vdots\\
        x_{1K}
      \end{pmatrix} +
\begin{pmatrix}
        x_{21}\\
        x_{22} \\
        \vdots\\
        x_{2K}
      \end{pmatrix}+
\hdots +
\begin{pmatrix}
        x_{n1}\\
        x_{n2} \\
        \vdots\\
        x_{nK}
      \end{pmatrix} =
\begin{pmatrix}
\sum_{i = 1}^n x_{i1} \\
\sum_{i = 1}^n x_{i2} \\
\vdots \\
\sum_{i = 1}^n x_{iK}
\end{pmatrix}=
\sum_{i = 1}^n\vx_i.
\end{equation*}

The multivariate version of Theorem \ref{theorem:LL_CLT_U} is the following:


\begin{theorem}[Multivariate Lindberg-Levy CLT]\label{theorem:LL_CLT_M}
Let $\left\lbrace \vx_n \right\rbrace$ be a sequence of i.i.d. random variables from a multivariate distribution. If $\E(\vx_n) = \vmu$ and finite and positive covariance matrix $\mQ$. Then the distribution of

\begin{equation*}
  \begin{aligned}
      Z_n & = \sqrt{n}(\overline{\vx}_n - \vmu) \dto \rN(\vzeros, \mQ), 
  \end{aligned}
\end{equation*}
%
as $n$ approaches infinity, where $\overline{\vx}_n = (1 / n)\sum_{i = 1}^n\vx_i$.
\end{theorem}

The Linderbeg-Levy CLT is one of the several forms of this extremely powerful result. An important extension allow us to relax the assumption of equal variances. The Linderberg-Feller CLT allows for this extension:

\begin{theorem}[Univariate Lindberg-Feller CLT]\label{theorem:LF_CLT_U}
Let $\left\lbrace X_n\right\rbrace, i = 1, 2,...,n$ be a sequence of i.i.d. random variables. If $\E(X_i) = \mu_i$ and the variance is strictly positive and finite, $0 < \sigma^2_i < \infty$. Define 

\begin{equation*}
  \bar{\mu}_n = \frac{1}{n}(\mu_1 + \mu_2 + ... +\mu_n), \quad \mbox{and} \quad \bar{\sigma}^2_n =\frac{1}{n}(\sigma^2_1 + \sigma^2_2 + ... +\sigma^2_n)
\end{equation*}

If no single term dominates this average variance, which we could state as

\begin{equation*}
\lim_{n\to \infty} \frac{\max (\sigma_i)}{n \bar{\sigma}_n} = 0,
\end{equation*}
%
and if the average variance converges to a finite constant,

\begin{equation*}
\lim_{n\to \infty} \bar{\sigma}^2_n = \bar{\sigma}^2
\end{equation*}
%
then

\begin{equation*}
  \begin{aligned}
      Z_n &  = \frac{\sqrt{n}(\overline{X}_n - \bar{\mu})}{\bar{\sigma}} \dto \rN(0, 1)
  \end{aligned}
\end{equation*}
%
 as $n$ approaches infinity.
\end{theorem}

In practical terms, the theorem states that sums of random variables, regardless of their form, will tend to be normally distributed. 


\begin{theorem}[Multivariate Lindberg-Feller CLT]\label{theorem:LF_CLT_M}
Suppose that $\vx_1, \vx_2,...,\vx_n$ are a sample of random vectors such that $\E(\vx_i) = \vmu_i$, $\var(\vx_i) = \mQ_i$, and all mixed third moments of the multivariate distribution are finite. Let

\begin{equation*}
  \bar{\vmu}_n = \frac{1}{n}\sum_{i = 1}^n\vmu_i,
\end{equation*}

\begin{equation*}
  \overline{\mQ}_n = \frac{1}{n}\sum_{i = 1}^n\mQ_i.
\end{equation*}

We assume that

\begin{equation*}
  \lim_{n \to \infty}\overline{\mQ}_n  = \mQ,
\end{equation*}
%
where $\mQ$ is a finite, positive definite matrix, and that for every $i$,

\begin{equation*}
  \lim_{n \to \infty}(n\overline{\mQ}_n)^{-1}\mQ_i = \lim_{n \to \infty}\left(\sum_{i = 1}^n \mQ_i\right)^{-1} \mQ_i = \vzeros
\end{equation*}

We allow the means of the random vectors to differ, although in the cases that will analyze, they will generally be identical. The second assumption states that individual components of the sum must be finite and diminish in significance. There is also an implicit assumption that the sum of matrices is nonsingular. Because the limiting matrix is nonsingular, the assumption must hold for large enough $n$, which is all that concerns us here. With these in place, the result is

\begin{equation*}
  \sqrt{n}(\bar{\vx}_n - \bar{\vmu}_n)\dto \rN(\vzeros, \mQ)
\end{equation*}
\end{theorem}

\begin{theorem}[Liapounov Central Limit Theorem]
Suppose that $\left\lbrace X_n\right\rbrace, i = 1, 2,...,n$ is a sequence of independent random variables with finite mean $\mu_i$ and finite positive variances $\sigma^2_i$ such that $\E\left[\left|X_i - \mu_i\right|^{2 + \delta}\right]$ if finite for some $\delta > 0$. If $\bar{\sigma}_n$ is positive and finite for all $n$ sufficiently large, then

\begin{equation*}
  \frac{\sqrt{n}(\overline{X}_n - \bar{\mu}_n)}{\bar{\sigma}_n}\dto\rN(0, 1)
\end{equation*}
\end{theorem}

This version of the central limit theorem requires only that moments slightly larger than two be finite and it is generally used when the variables are fixed. 

We end this section by defining the concept of \textbf{asymptotic variance}.


\begin{definition}[Asymptotic Variance]\label{def:asymptotic_variance}
Let $\left\lbrace\vx_n\right\rbrace$ be a sequence of random vectors. If there exists a sequence of matrices $\left\lbrace \mV_n\right\rbrace$ such that $\mV_n$ is nonsingular for all $n$ sufficiently large and $\mV^
{-1/2}\vx_n \adistr \rN(\vzeros, \mI)$, then $\mV_n$ is called the asymptotic covariance matrix of $\vx_n$, denoted $\avar(\vx_n)$.
\end{definition}

%-----------------------------------
\section{Orders in Probability}
%-----------------------------------

Similarly to the nonsthocastic sequences, we can make similar statement about $o$ and $O$ when we have random variables. The following theorem state the definition of unboudedness and convergence for random variables:

\begin{definition}[Order in Probability]\label{definition:order_prob}
Consider the following two definition:

\begin{enumerate}
		\item \textbf{Stochastically Bounded (Big O)}: The sequence of random variables  $\left\{X_n\right\}$ is at most of order in probability $n^\lambda$, and we write

		\begin{equation}
			X_n = O_p(n^\lambda)
		\end{equation}

		if, for every $\epsilon > 0$, there exists a real number $n_0$ such that:

		\begin{equation}
			\Pr\left[n^{-\lambda} \left|X_n \right| \geq n_0 \right]\leq \epsilon
		\end{equation}

		for all $n$.

		\item \textbf{Stochastic Convergence}: Also, we say that $\left\{X_n\right\}$ is of smaller order in probability than $n^\lambda$ and we write

		\begin{equation}
		X_n = o_p\left(n^\lambda\right)
		\end{equation}

		if

		\begin{equation}
			\plim n^{-\lambda}X_n = 0
		\end{equation}

		When $\lambda = 0$, $X_n$ converges to zero, and we also write $X_n=o_p(1)$.

	\end{enumerate}
\end{definition}

Intuitively, for $X_n = O_p(n^\lambda)$ with $\lambda > 0$, the order $n^\lambda$ is the fastest growth rate at which $X_n$ goes to infinity with probability approaching 1. When $\lambda < 0$, the order $n^\lambda$ is the fastest convergence rate at which $X_n$ vanishes to 0 with probability approaching 1. Thus, $X_n= O_p(1) = O_p(n^0)$ implies that for $n$ sufficiently large, $\left|X_n\right|$ takes value larger than a very large constant has a tiny probability. In other words, $\left|X_n\right|$ is bounded by a constant with a very high probability for all $n$ sufficiently large.

\begin{definition}[Stochastically Negligible]
	If $X_n\pto 0$, then $X_{n} = o_p(1)$. If $X_n = n^\lambda o_p(1)$, then $X_n = o_p(n^{-\lambda})$
\end{definition}

To give some intuition about these definitions, consider $\vx_n \dto \vx$ and $\vy_n\pto \vzeros$. Then:

\begin{equation*}
  \vx_n + \vy_n \dto \vx\quad \mbox{by \ref{eq:rule_dtop} in Theorem \ref{teo:rules_dto}}
\end{equation*}

That is, if  $\vz_n = \vx_n + \vy_n$ and $\vy_n\pto \vzeros$, implying that $\vz_n - \vx_n \pto \vzeros$, then the asymptotic distribution of $\vz_n$ is the same as that of $\vx_n$. Note that this is the same as Lemma \ref{lemma:asymptotic_equiv} (asymptotic equivalence). So we can write:

\begin{equation*}
  \vz_n \adistr \vx_n\quad \mbox{or}\quad \vz_n = \vx_n + o_p(1)
\end{equation*}
%
where $o_p(1)$ is some variable ($\vy_n$ in this case) that is sthochastically negligible, that is, it converges to zero in probability.

This is more intuitive if we think in the consistency of OLS estimator. Given the OLS consistency, it is the same to write:

\begin{equation*}
\widehat{\vbeta}_n \pto \vbeta_0\quad \mbox{as $n\to\infty$}
\end{equation*}
%
as

\begin{equation*}
\widehat{\vbeta}_n = \vbeta_0  + o_p(1) \quad \mbox{as $n\to\infty$}
\end{equation*}

In other words, a consistent estimator is equal to the true estimator plus something that converges to 0 in probability.

%------------------------------------------------------------------
\begin{lemma}[Convergence in distribution implies boundedness]
  Let $X_n$ be a random variable with CDF $F_n(\cdot)$, and let $X$ be a random variable with continuous CDF $F(\cdot)$. If $X_n\dto X$ as $n\to \infty$, then $X_n = O_p(1)$
\end{lemma}
%------------------------------------------------------------------

Intuitively, if the probability distribution of $X_n$ converges to a well-defined continuous probability distribution as $n\to \infty$, then $X_n$ is bounded in probability. This result is very useful for establishing that a sequence of random variables is bounded in probability. Often it is easier to verify that a sequence of random variables converges in distribution.

When do we use the $O_p$?  If a random vector converges in distribution $\vx_n \dto \vx$ (for example $\vx\sim \rN(\vzeros, \mV)$) then $\vx_n = O_p(1)$.

\begin{example}
  If $X_n\sim N(0,1)$ for all $n\geq 1$. Then $X_n = O_p(1)$ because for any given $\delta > 0$, there exists a finite constant $M = \Phi^{-1}(1 - \delta/2)<\infty$, where $\Phi$ is the $\rN(0, 1)$ CDF, such that

  \begin{equation*}
    \Pr(\left|X_n\right| > M) = 2\left[1 - \Phi(M)\right] = \delta < 2\delta
  \end{equation*}
  %
  for all $n \geq 1$
\end{example}

$O_p(1)$ is weaker than $o_p(1)$ in the sense that $X_n = o_p(1)$ implies $X_n = O_p(1)$ but not the reverse.

There are many simple rules for manipulating $o_p(1)$ and $O_p(1)$ sequences which can be deduced from the continuous mapping theorem or Slutsky's Theorem. 

\begin{proposition}[Properties of stochastic big and little O]\label{prop:blO-est}
Let $a_n$ and $b_n$ random scalars. 
\begin{enumerate}
  \item If $a_n=O_p(n^\lambda)$ and $b_n=O_p(n^\mu)$, then $a_nb_n=O_p(n^{\lambda + \mu})$ and $a_n + b_n = O_p(n^\kappa)$, where $k = \textrm{max}\left[\lambda, \mu\right]$.
  \item If $a_n = o_p(n^\lambda)$ and $b_n = o_p(n^\mu)$, then $a_nb_n=o_p(n^{\lambda + \mu})$ and $a_n + b_n = o_p(n^\kappa)$, where $k = \textrm{max}\left[\lambda, \mu\right]$.
  \item If $a_n=O_p(n^\lambda)$ and  $b_n = o_p(n^\mu)$, then $a_nb_n=o_p(n^{\lambda + \mu})$ and $a_n + b_n = O_p(n^\kappa)$, where $k = \textrm{max}\left[\lambda, \mu\right]$.
\end{enumerate}
\end{proposition}

One of the most common uses of this concept of stochastic order is ``root-$n$'' $(\sqrt{n})$ consistency:

\begin{definition}[$\sqrt{N}$-Consistent]
		if $\sqrt{n}\left(\vtheta_n - \vtheta_0\right)= O_p(1)$, then $\vtheta_n$ is $\sqrt{n}$ consistent for $\vtheta_0$
\end{definition}

\begin{example}[OLS and $O_p$ and $o_p$]
Recall that:

\begin{equation*}
  \widehat{\vbeta}_n = \vbeta_0 + (\mX'\mX)^{-1}\mX'\vepsi.
\end{equation*}

Under appropriate assumption, we know that:

\begin{equation*}
  \frac{1}{n}\mX'\mX\pto \E(\mX'\mX),
\end{equation*}
%
which is finite and positive definite. The fact that the elements of $n^{-1}\mX'\mX$ converge to finite limits in probability implies that $N^{-1}\mX'\mX$ is \textbf{bounded} in the sense that the sequences of the elements within $n^{-1}\mX'\mX$ are bounded, and under these circumstances we say that $\mX'\mX$ is at most of order $n$, that is, $\mX'\mX = O_p(n)$, or we can say:

\begin{equation*}
n^{-1}\mX'\mX = O_p(1).
\end{equation*}

We also assume that $n^{-1/2} \mX'\vepsi$ has probability limit which a normally distributed random variable with expectation zero and finite variance. So, we can write:

\begin{equation*}
\mX'\vepsi = O_p(n^{1/2}).
\end{equation*}

Thus:


\begin{equation*}
  \begin{aligned}
\widehat{\vbeta}_n - \vbeta_0 & =  (\mX'\mX)^{-1}\mX'\vepsi  \\
\widehat{\vbeta}_n - \vbeta_0 & =  (n^{-1}\mX'\mX)^{-1}n^{-1}\mX'\vepsi  \\
\widehat{\vbeta}_n - \vbeta_0 & = O_p(1) \cdot o_p(1) \\
\widehat{\vbeta}_n - \vbeta_0 & = o_p(1)
\end{aligned}
\end{equation*}

Also:

\begin{equation*}
  \begin{aligned}
\widehat{\vbeta}_n - \vbeta_0 & =  (\mX'\mX)^{-1}\mX'\vepsi  \\
\widehat{\vbeta}_n - \vbeta_0 & =  \left[O_p(n)\right]^{-1}O_p(n^{1/2}) \\
\widehat{\vbeta}_n - \vbeta_0 & =  O_p(n^{-1})O_p(n^{1/2}) \quad \because \left[O_p(n)\right]^{-1}= O_p(n^{-1})\\
\widehat{\vbeta}_n - \vbeta_0 & =  O_p(n^{-1/2})\\
\end{aligned}
\end{equation*}
%

So, we might then say that $\widehat{\vbeta}_n - \vbeta_0$ is converging to zero at the rate $1/\sqrt{n}$; and the rate tell us what multiplier of the variable $\widehat{\vbeta}_n - \vbeta_0$ stabilizes it so that it convergences to a well-defined random variables rather than to 0 or $\infty$.

Note also that:

\begin{equation*}
  \begin{aligned}
    \sqrt{n}(\widehat{\vbeta}_n - \vbeta_0) & =  n^{1/2}O_p(n^{-1})O_p(n^{1/2})\\
    & = n^{1/2} O_p(n^{-1/2}) \\
    & = O_p(1) \quad \because X_n = O_p(1/\sqrt{n}) \implies X_n / (1/\sqrt{n})= \sqrt{n} X_n = O_p(1)
  \end{aligned}
\end{equation*}
\end{example}

\begin{example}[Rate of convergence for sample mean]
Consider and iid random sample $X_i$ with mean $\mu = 0$ and variance $\sigma^2 = 0,25$. Then, by the CLT the sample mean $\bar{X}$ we know that $\sqrt{n}(\bar{X}-\mu) /\sigma \dto \rN(0 , 1)$. That is, $\sqrt{n}(\bar{X}-\mu)/\sigma = O_p(1)$. This implies that $\bar{X} - \mu= O_p(\sqrt{n})$. Figure \ref{fig:Op_example} shows how $\bar{X}$ converges towards $\mu$ to the speed of $1\sqrt{n}$.
\end{example}


\begin{figure}[H]
  \caption{Convergence of the sample mean and speed of convergence}
    \label{fig:Op_example}
        \centering
    \begin{minipage}{.9\linewidth}
<<CLT-normalplot-bigO, echo =  FALSE, fig.align='center', out.width = '10cm', out.height = '10cm', cache = TRUE>>=
set.seed(123)
N <- 10000
n <- 1:N
n_dat <- rnorm(n = n, mean = 0, sd = 0.5)
xbar <- cumsum(n_dat) /  n
rate <- 1/sqrt(n)
par(mfrow = c(1, 1), mar = c(4, 4, 1, 1) + 0.1)
plot(n, xbar, type = "l", ylab = expression(bar(x)[n]), ylim = c(-0.1, 0.1))
lines(n, rate, col = "blue", lty = 2)
abline(h = 0, col = "red")
legend("topright",
       legend = c(expression(bar(x)[n]), expression(1/sqrt(n)), expression(mu)),
       text.col = c("black", "blue", "red"),
       lty = c(1, 2, 1),
       col = c("black","blue", "red"))
@
\footnotesize
		\emph{Notes: This graphs shows the convergence of $\bar{X}$ to $\mu$ as $n\to\infty$ for a normal distribution as fast as $1/\sqrt{n}$.}
	\end{minipage}
\end{figure}

%-----------------------------------------------------
\section{Triangular Arrays}\label{sec:triangular-array}
%------------------------------------------------------

An important question in the context of asymptotic theory is the following: \emph{What does $n \to \infty$ mean in a spatial context?} Does it imply an increase in the geographical area, or does it refer to an increase in the number of spatial units within a given geographical area?

For spatial data, two distinct asymptotic frameworks have been studied: \textbf{increasing domain} and \textbf{infill asymptotic}. Increasing domain refers to a sampling structure where new observations (spatial units) are added at the edges (boundary points), similar to the underlying asymptotics in time series analysis. In other words, increasing domain asymptotics involve more and more observations being sampled over an expanding domain. The key issue here is defining what constitutes the boundary.

In contrast, infill asymptotics are appropriate when the spatial domain is bounded, and new observations (points) are added between existing ones, resulting in a denser surface. In most applications of spatial econometrics, the underlying structure aligns more with the increasing domain framework.

The increasing domain framework necessitates an understanding of \textbf{triangular arrays}. The following section provides a straightforward definition of triangular arrays.

%--------------------------------------------------------------------------------------
\begin{definition}[Triangular Array of Random Variables]\label{def:triangular-array} 
The ordered collection of random variables 
\begin{equation*}
\left\lbrace X_{11}, X_{21}, X_{22}, X_{31}, X_{32}, X_{33}, \ldots , X_{nn}, \ldots \right\rbrace,
\end{equation*}
%
or
\begin{equation*}
\begin{pmatrix}
X_{11} &        &        &        &        &        & \\
X_{21} & X_{22} &        &        &        &        & \\
X_{31} & X_{32} & X_{33} &        &        &        & \\
\vdots & \vdots & \vdots & \ddots &        &        & \\
X_{n1} & X_{n2} & X_{n3} & X_{n4} & \hdots & X_{nn} & \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots
\end{pmatrix}
\end{equation*}
%
is called a triangular array of random variables, and will be denoted by $\left\lbrace X_{nn}\right\rbrace$.
\end{definition}
%-------------------------------------------------------------------------


Central limit theorems (CLTs) applied to triangular arrays of random variables focus on the limiting distributions of appropriately defined functions of row averages. Consider the row average $S_n = n^{-1}\sum_{i=1}^n X_{ni}$ for the $n$-th row of the triangular array. For instance, when $n=3$ (the third row), we have $S_3 = \frac{1}{3}(X_{31} + X_{32} + X_{33})$.

Traditional CLTs address averages of the form $n^{-1}\sum_{i=1}^n X_i$, where $X_i$ are elements of a sequence $\left\lbrace X_n \right\rbrace$. However, triangular arrays, denoted by $\left\lbrace X_{ni} \right\rbrace$, are more general structures. The random variables in a row need not be identical to those in other rows, making triangular arrays more flexible for modeling.

This flexibility introduces certain statistical challenges, particularly regarding the appropriate CLT to apply. In triangular arrays, the random variables across rows may exhibit different distributions or dependencies, which complicates the derivation of limiting distributions. As a result, both the Law of Large Numbers (LLN) and the CLT for triangular arrays require slightly stronger conditions than those for independent and identically distributed (i.i.d.) sequences of random variables.

In order for $S_n$ to converge to a normal distribution as $n \to \infty$, the following conditions must be met:
\begin{itemize}
  \item Independence: assume all random variables in the array are independent.
  \item Centering: assume $\E(X_{j,i}) = 0$ for all $j, i$.
  \item Variances converge: assume $\sum_{i = 1}^n\E(X_{n, i}^2)\to \sigma^2> 0$ as $n\to\infty$
  \item No single variance is too large.
\end{itemize}

Then $S_n \dto N(0, \sigma^2)$ as $n\to\infty$.

You may be wondering: Why are triangular arrays important in the spatial context? When adopting the increasing domain approach, it becomes evident that as $n$ increases, the spatial weight matrix  $\mW$ changes as observations are added. To illustrate, consider the true parameter vector  $\vtheta_0 = (\vbeta_0^\top, \rho_0, \sigma_0^2)^\top$, where variables and estimates depend on the sample size $n$. This dependency allows us to study their behavior as $n \to \infty$.

Let $\mA_n(\rho) = \mI_n - \rho \mW_n$ for any value of $\rho$. The ``equilibrium'' vector is given by
\begin{equation}\label{eq:equilibrium_slm}
\vy_n = \mA_n^{-1}(\mX_n\vbeta_0 + \vepsi_n), 
\end{equation}
%
where $\mA_n = \mA_n(\rho_0)$ is nonsingular. Let $\vepsi_n(\vdelta) = \vy_n - \mX_n\vbeta - \rho \mW_n\vy_n$, where $\vdelta = (\vbeta^\top, \rho)^\top$. Thus, $\vepsi_n = \vepsi_n(\vdelta_0)$. Since the matrices $(\mI_n - \rho\mW_n)^{-1}$ generally depend on the sample size $n$, the vectors $\vy_n$ and $\vepsi_n$ will also depend upon $n$, forming a \textbf{triangular arrays}. This arises because new spatial units---or data points---alter the structure of the existing spatial units within the weight matrix $\mW_n$ \citep[see for example][]{kelejian1999generalized, kelejian2001asymptotic, anselinSpatialEcon}. For example, the outcome for the first spatial unit, $y_{1,n}$, will differ depending on whether the total number of observations is $n=10$ or $n=15$, due to the evolving structure of $\mathbf{W}_n$ as $n$ changes.

This dependence implies that the elements of $\vy$ should be indexed by $n$:
\begin{equation*}
  \vy_n = (y_{11}, y_{21}, y_{22}, \ldots, y_{nn}). 
\end{equation*}

For example, for $n = 1, 2, 3$, we have:
\begin{equation*}
\begin{aligned}
  n = 1 & \implies y_{11} \;         \\
  n = 2 & \implies y_{12} \; y_{22}  \\
  n = 3 & \implies y_{13} \; y_{23} \; y_{33} \\
  \vdots & \\
  n = n & \implies y_{13} \; y_{23} \; y_{33} \hdots y_{3n}
\end{aligned}
\end{equation*}
%
where $y_{11} \neq y_{12} \neq y_{13}$ and $y_{22}\neq y_{23}$. Note that the dependent variables within the same row are mutually independent (spatial units are independent) and have the same distribution. However, the distribution of the dependent variables across different rows can vary. 

The triangular array structure of $\vy_n$ partially arises from allowing the disturbances in the model to have a triangular array structure. More fundamentally, this structure also applies to the regressors in  $\mX$.  By allowing the elements of $\mX_n$ to depend on $n$, we explicitly account for spatial lags among some of the regressors.

In regularly observed time series settings, indices correspond to equidistant points on the real line, making the notion of increasing $n$ straightforward. In contrast, spatial settings involve ambiguity. For example, consider $n$ points on a two-dimensional regularly spaced lattice, where both the number of rows ($n_1$) and columns ($n_2$) increase with $n=n_1\cdot n_2$. Listing these points in lexicographic order (e.g., left to right, top to bottom) requires re-labeling as $n$ increases, which the triangular array structure accommodates \citep{anselin2021spatial}.

Another consequence of this listing is that dependence between spatial locations $i$ and $j$ is not necessarily a function of the difference $i-j$, particularly when the dependence is isotropic.

%----------------------------------
\section{Bounded Matrices and Useful Lemmas for Spatial Eonometrics}\label{sec:bounded-matrices}
%----------------------------------

In asymptotic it is important to establish whether the matrices are bounded as $n \to \infty$. As explained before, this implies that the impact of spatial interactions does not grow unbounded as the sample size increases. For example, if $\mW_n$ is not bounded, the effect of spatial dependence could explode, making the process unstable. Similalry, for asymptotic results such as the LLN and CLTs to hold in spatial context, regularity conditions on the spatial weight matrix.

In theoretical literature, we often encounter the following definition of bounded matrices. 

%-------------------------------------------------------------------
\begin{definition}[Bounded Matrices]\label{def:Bounded_Matrices}
Let $\left\lbrace \mA_n \right\rbrace$ be a sequence of $n$-dimensional square matrices, where $\mA_n = \left[a_{n,ij}\right]$,
  \begin{enumerate}
    \item The column sums of $\left\lbrace \mA_n \right\rbrace$ are uniformly bounded (in absolute value) if there exists a finite constant $c_a$ that does not depend on $n$ such that 
    \begin{equation*}
      \left\|\mA_n\right\|_{1} = \max_{1 \leq j\leq n}\sum_{i = 1}^n\left|a_{n, ij}\right| \leq c_a
    \end{equation*}
     \item The row sums of $\left\lbrace \mA_n \right\rbrace$ are uniformly bounded (in absolute value) if there exists a finite constant $c_a$ that does not depend on $n$ such that 
    \begin{equation*}
      \left\|\mA_n\right\|_{\infty} = \max_{1 \leq i\leq n}\sum_{j = 1}^n\left|a_{n, ij}\right| \leq c_a
    \end{equation*}
  \end{enumerate}
\end{definition}
%-------------------------------------------------------------------

Then $\left\lbrace \mA_n \right\rbrace$ is said to be \textbf{uniformly bounded} in row sums if $\left\lbrace\left\|\mA_n\right\|_{\infty}\right\rbrace$ is a bounded sequence. Similarly, $\left\lbrace \mA_n \right\rbrace$ is said to be \textbf{uniformly bounded} in column sums if $\left\lbrace\left\|\mA_n\right\|_{1}\right\rbrace$ is a bounded sequence. 

Note that for the typical row-standardized $\mW$, row-boundedness is guaranteed by construction since the row sum is 1. The column boundedness is in most cases also guaranteed as long as the number of neighbors is constrained. We also need $(\mI_n - \rho\mW_n)^{-1}$ to be uniformly bounded in row and column sum and consider also the parameter space of $\rho$. 

An important characteristic of uniformly bounded matrices is that this property is preserved under matrix multiplication. 

%---------------------------------------------
\begin{lemma}\label{lemma:bounded_lemma}
If $\left\lbrace \mA_n \right\rbrace$ an $\left\lbrace \mB_n \right\rbrace$ are uniformly bounded in row sums (column sums), then $\left\lbrace \mA_n \mB_n\right\rbrace$ is also uniformly bounded in row sums (column sums).
\end{lemma}
%------------------------------------------------

%---------------------------------------------------
\begin{proof}
 Suppressing the index $n$, suppose that $\sum_{i = 1}^n|a_{ij}|\leq c_a$, $\sum_{j = 1}^n|a_{ij}|\leq c_a$, $\sum_{i = 1}^n|b_{ij}|\leq c_b$, and $\sum_{j = 1}^n|b_{ij}|\leq c_b$. Let $\mD = \mA\mB$, then
 \begin{equation*}
  d_{ij} = \sum_{r = 1}^n a_{ir}b_{rj}.
 \end{equation*}
 Let $r_i$ be the $i$th row sum, then
 \begin{equation*}
 \begin{aligned}
  r_i   = & \sum_{j = 1}^n \left|d_{ij}\right|, \\
        = & \sum_{j = 1}^n \left|\sum_{r = 1}^n a_{ir}b_{rj}\right|, \\
     \leq & \sum_{j = 1}^n \sum_{r = 1}^n \left|a_{ir}b_{rj}\right|\;\mbox{by triangle inequality \ref{def:triangle-inequality}}, \\
      = & \sum_{j = 1}^n \sum_{r = 1}^n \left|a_{ir}\right|\left|b_{rj}\right|\;\mbox{by multiplicativity \ref{def:abs-multiplicativity}}, \\
      = & \sum_{r = 1}^n \sum_{j = 1}^n \left|a_{ir}\right|\left|b_{rj}\right|\;\mbox{by property of summation}, \\
      = & \sum_{r = 1}^n \left|a_{ir}\right| \sum_{j = 1}^n \left|b_{rj}\right| ,\\
        & \leq c_{a} c_{b},\; \mbox{for all $i = 1,\ldots, n$ and $n\geq 1$ by Def. \ref{def:Bounded_Matrices}}.
 \end{aligned}
 \end{equation*}
 
 Similarly, we can show that 
 \begin{equation*}
 \sum_{i = 1}^n \left|d_{ij}\right|\leq c_{a} c_{b},\; \mbox{for all $j = 1,\ldots, n$ and $n\geq 1$.}
 \end{equation*}
\end{proof}
%---------------------------------------------------

%---------------------------------------------------
\begin{lemma}\label{lemma:bounde_ON}
If $\left\lbrace \mA_n \right\rbrace$ is absolutely summable (uniformly bounded in either row or column sums), and $\mZ_n$ has bounded elements, then the elements of $\mZ_n^\top \mA_n\mZ_n = O(n)$.
\end{lemma}
%---------------------------------------------------

%---------------------------------------------------
\begin{proof}
  Suppressing the index $n$, let $Z_{ij}$ be the $(i,j)th$ element of $\mZ$, and let $\left|Z_{ij}\right|\leq c_z$ for all $i,j$ and $n\geq 1$. Let $\delta_{ij}$ be the $(i,j)$th element of $\mZ^\top \mA\mZ$, then
  \begin{equation*}
    \begin{aligned}
      \delta_{ij} & = \sum_{r = 1}^n\sum_{s = 1}^n Z_{si}a_{sr}Z_{rj},  \\
      \left|\delta_{ij}\right| & = \left|\sum_{r = 1}^n\sum_{s = 1}^n Z_{si}a_{sr}Z_{rj}\right|, \\
      \left|\delta_{ij}\right| & \leq \sum_{r = 1}^n\sum_{s = 1}^n \left|Z_{si}\right| \left|a_{sr}\right|\left|Z_{rj}\right| \; \mbox{by \ref{def:triangle-inequality}}, \\
       & \leq c_z^2\sum_{r = 1}^n\sum_{s = 1}^n  \left|a_{sr}\right| \\
        & \leq c_z^2\sum_{r = 1}^n c_a \\
        & \leq c_z^2c_an \\
        & = O(n)
    \end{aligned}
  \end{equation*}
\end{proof}

%------------------------------------------------
\begin{lemma}\label{lemma:bounde_trace}
If $\left\lbrace \mA_n \right\rbrace$ is absolutely summable (uniformly bounded in either row or column sums), then
\begin{enumerate}
 \item elements $a_{n,ij}$ of $\mA_n$ are uniformly bounded in $i$ and $j$,
 \item $\tr(\mA^m) = O(n)$ for $m\geq 1$, and 
 \item the elements of $\tr(\mA_n\mA_n^\top)= O(n)$.
\end{enumerate}
\end{lemma}
%------------------------------------------------

\begin{proof}
Proof of (c). Suppressing the index $n$
\begin{equation*}
  \begin{aligned}
    \tr(\mA_n\mA_n^\top)               & = \sum_{i = 1}^n\sum_{j=1}^n a_{ij}^2 \\
    \left|\tr(\mA_n\mA_n^\top)\right|  & = \left|\sum_{i = 1}^n\sum_{j=1}^n a_{ij}^2\right| \\
     & \leq \sum_{i = 1}^n\sum_{j=1}^n\left|a_{ij}^2\right| \\
     & \leq \sum_{i = 1}^n\left(\sum_{j=1}^n\left|a_{ij}\right|\right) ^2 \\
     & \leq  n c_1^2\quad \mbox{if $\mA_n$ is uniformly bounded in row sums}
  \end{aligned}
\end{equation*}


\begin{equation*}
  \begin{aligned}
    \left|\tr(\mA_n\mA_n^\top)\right|  & \leq \sum_{j = 1}^n\left(\sum_{i=1}^n\left|a_{ij}\right|\right) ^2 \\
     & \leq  n c_2^2\quad \mbox{if $\mA_n$ is uniformly bounded in column sums}
  \end{aligned}
\end{equation*}
\end{proof}

%----------------------------------------------------------------------
\section{Linear and Quadratic Forms}
%----------------------------------------------------------------------

%-----------------------
\subsection{Moments}
%-----------------------

Linear and quadratic forms of the error terms will often appear in the analysis of spatial models. Thus, it is important to review and analyze some of its characteristics.

First, we define a quadratic form:
%----------------------------------------------------------
\begin{definition}[Quadratic form]\label{def:quad-form}
For a $n\times n$ symmetric matrix $\mA_n=\left[a_{n, ij}\right]$ the quadratic function of $n$ variables $\vepsi$ defined by:
\begin{equation*}
\vepsi^\top \mA_n \vepsi = \sum_{i = 1}^n\sum_{j = 1}^na_{ij}\epsilon_i\epsilon_j
\end{equation*}
%
is called the quadratic form with matrix $\mA_n$.  If $\mA_n$ is not symmetric, we can replace $\mA_n$ by $\mA^s = \left(\mA_n + \mA_n^\top\right)/2$.
\end{definition}

The following Lemma is based on \citep{lee2004asymptotic}.

%--------------------------------------------------------------
\begin{lemma}[First and Second Moments]\label{lemma:second-mom-lee}
Let $\mA_n=\left[a_{ij}\right]$ be an $n$-dimensional square matrix. Then, it can be shown that:
\begin{equation*}
\E\left[\vepsi^\top_n\mA_n\vepsi_n\right]=\tr\left(\mA_n\mSigma\right)+\vmu^\top\mA_n\vmu
\end{equation*}
%
where $\vmu$ and $\mSigma$ are the expected value and variance-covariance matrix of $\vepsi_n$, respectively. This result only depends on the existence of $\vmu$ and $\mSigma$; it does not require normality of $\vepsi$. 

Assume that $\mu = \vzeros$ and $\mSigma = \sigma_0^2\mI$, then
\begin{enumerate}
  \item $\E(\vepsi_n^\top\mA_n\vepsi_n) = \sigma^2_0\tr(\mA_n)$,
  \item $\E(\vepsi_n^\top\mA_n\vepsi_n)^2 = (\mu_4 - 3\sigma^4_0)\sum_{i = 1}^na_{ii}^2+\sigma^4_0\left[\tr^2(\mA_n) + \tr(\mA_n\mA_n^\top)+\tr(\mA^2_n)\right]$, and
  \item $\var(\vepsi_n^\top\mA_n\vepsi_n) = (\mu_4 - 3\sigma^4_0)\sum_{i = 1}^na_{ii}^2+\sigma^4_0\left[\tr(\mA_n\mA_n^\top)+\tr(\mA^2_n)\right]$.
\end{enumerate}

For the moment assume that $\mA_n$ is symmetric and $\vepsi$ is normally distributed, then:
\begin{equation*}
\var\left(\vepsi^\top\mA\vepsi\right)= 2\tr\left(\mA\mSigma\mA\mSigma\right) + 4\vmu^\top\mA\mSigma\mA\vmu,
\end{equation*}
%
and the covariance:
\begin{equation*}
\cov\left(\vepsi^\top\mA_1\vepsi, \vepsi^\top\mA_2\vepsi\right)= 2\tr\left(\mA_1\mSigma\mA_2\mSigma\right) + 4\vmu^\top\mA_1\mSigma\mA_2\vmu
\end{equation*}

If $\mA_n$ is not symmetric, then:
\begin{equation}\label{eq:var-quadratic-form}
\cov\left(\vepsi^\top\mA_1\vepsi, \vepsi^\top\mA_2\vepsi\right)= 2\tr\left[\frac{1}{2}\left(\mA_1 + \mA_1^\top\right)\mSigma\frac{1}{2}\left(\mA_2 + \mA_2^\top\right)\mSigma\right] + 4\vmu^\top\frac{1}{2}\left(\mA_1 + \mA_1^\top\right)\mSigma\frac{1}{2}\left(\mA_2 + \mA_2^\top\right)\vmu
\end{equation}

In particular, if $\vepsi$'s are normally distributed with mean 0 and variance $\sigma_0^2$, then 
\begin{itemize}
  \item $\E(\vepsi_n^\top\mA_n\vepsi_n)^2 = \sigma^4_0\left[\tr^2(\mA_n) + \tr(\mA_n\mA_n^\top)+\tr(\mA^2_n)\right] = \sigma^4_0\left[\tr^2(\mA_n) + \tr(\mA_n\mA_n^{s})\right]$, and
  \item $\var(\vepsi_n^\top\mA_n\vepsi_n) = \sigma^4_0\tr(\mA_n\mA_n^{s}) = \frac{\sigma^4}{2}\tr(\mA_n^{s2}) = \frac{\sigma^4}{2}\diag(\mA_n^s)'\diag(\mA_n^s)$.
\end{itemize}
\end{lemma}
%--------------------------------------------------------------

%--------------------------------------------------------
\begin{proof}
For (a), and using the Definition of quadratic form \ref{def:quad-form} We can write:
\begin{equation*}
\begin{aligned}
\E\left(\vepsi^\top\mA\vepsi\right) &= \sum_{i = 1}^n\sum_{j = 1}^n\E(\epsilon_i a_{ij}\epsilon_j) \\
& = \sum_{i = 1}^n\sum_{j = 1}^na_{ij}\E(\epsilon_i\epsilon_j) \\
& = \sum_{i = 1}^n\sum_{j = i}^na_{ij}\sigma^2\;\mbox{since $\E(\epsilon_i\epsilon_j) = 0$ for $i\neq j$} \\
& = \sigma^2\sum_{i = 1}^n a_{ii}\\
& = \sigma^2\tr(\mA).
\end{aligned}
\end{equation*}

We provide the proof for (b). Using the Definition of quadratic form \ref{def:quad-form} We can write:
\begin{equation*}
\begin{aligned}
\E\left(\vepsi^\top\mA_n\vepsi\right)^2 & = \E\left(\sum_{i = 1}^n\sum_{j = 1}^na_{ij}\epsilon_i\epsilon_j\right)^2,\\
& = \E\left(\sum_{i = 1}^n\sum_{j= 1}^n\sum_{k = 1}^n\sum_{l = 1}^n a_{ij}a_{kl}\epsilon_{i}\epsilon_j\epsilon_k\epsilon_l\right), \\
&  = \sum_{i = 1}^n\sum_{j= 1}^n\sum_{k = 1}^n\sum_{l = 1}^n a_{ij}a_{kl}\E(\epsilon_{i}\epsilon_j\epsilon_k\epsilon_l).
\end{aligned}
\end{equation*}

Because the error terms are i.i.d with zero mean, $\E(\epsilon_{i}\epsilon_j\epsilon_k\epsilon_l) \neq 0$ only when $i = j = k = l$, $(i = j)\neq(k = l)$, $(i = k)\neq (j\neq l)$, and $(i\neq l)\neq (j =k)$. Thus
\begin{equation*}
\begin{aligned}
\E\left(\vepsi^\top\mA_n\vepsi\right)^2 & = \underset{i = j = k = l}{\sum_{i = 1}^n a_{ii}^2\E(\epsilon_i^4)} + \underset{(i = j)\neq(k = l)}{\sum_{i = 1}^n\sum_{j\neq i}^na_{ii}a_{jj}\E(\epsilon_i^2\epsilon_j^2)} \\
& + \underset{(i = k)\neq (j\neq l)}{\sum_{i = 1}^n\sum_{j\neq i}^n a_{ij}^2\E(\epsilon_i^2\epsilon_j^2)} + \underset{(i\neq l)\neq (j =k)}{\sum_{i = 1}^n\sum_{j \neq i}^na_{ij}a_{ji}\E(\epsilon_i^2\epsilon_j^2)},\\
& = \mu_4\sum_{i = 1}^na_{ii}^2 + \sigma^4\sum_{i = 1}^n\sum_{j\neq i}^na_{ii}a_{jj} + \sigma^4\sum_{i = 1}^n\sum_{j\neq i}^n a_{ij}^2 + \sigma^4\sum_{i = 1}^n\sum_{j \neq i}^na_{ij}a_{ji}, \\
& = (\mu_4 - 3\sigma^4)\sum_{i=1}^na_{ii}^2 + \sigma^4\left[\sum_{i = 1}^n\sum_{j = 1}^n a_{ii}a_{jj} + \sum_{i = 1}^n\sum_{j=1}^na^2_{ij} + \sum_{i = 1}^n\sum_{j = 1}^na_{ij}a_{ji}\right], \\
& = (\mu_4 - 3\sigma^4)\sum_{i=1}^na_{ii}^2 + \sigma^4\left[\tr^2(\mA) + \tr(\mA\mA^\top) + \tr(\mA^2)\right], \\
& = (\mu_4 - 3\sigma^4)\sum_{i=1}^na_{ii}^2 + \sigma^4\left[\tr^2(\mA) + \tr(\mA\mA^s)\right].
\end{aligned}
\end{equation*}

For (c), note that
\begin{equation*}
\begin{aligned}
\var(\vepsi^\top\mA\vepsi) & = \E\left(\vepsi^\top\mA_n\vepsi\right)^2 - \left[\E\left(\vepsi^\top\mA\vepsi\right)\right]^2, \\
& = (\mu_4 - 3\sigma^4)\sum_{i=1}^na_{ii}^2 + \sigma^4\left[\tr^2(\mA) + \tr(\mA\mA^s)\right] - \left[\sigma^2\tr(\mA)\right]^2,\\
& = (\mu_4 - 3\sigma^4)\sum_{i=1}^na_{ii}^2 + \sigma^4\tr(\mA\mA^s), \\
& = (\mu_4 - 3\sigma^4)\sum_{i=1}^na_{ii}^2 + \frac{\sigma^2}{2}\tr(\mA^s).
\end{aligned}
\end{equation*}

When $\vepsi$'s are normally distributed, $\mu_4 = 3\sigma^2$. 
\end{proof}
%--------------------------------------------------------

The following Lemma provides the moments for the product of moment conditions.

%--------------------------------------------------
\begin{lemma}\label{lemma:O-lemma-lee-quadratic}
For any $n$-dimensional (column) vector $\vq_n$ and $n\times n$ square matrices $\mA_n = [a_{n, ij}]$ and $\mB_n = [b_{n, ij}]$. Suppose that $\epsilon_{ni}$'s in $\vepsi_n$ are i.i.d $(0, \sigma^2)$ and have finite third and fourth moments $\mu_3$ and $\mu_4$. Then
\begin{enumerate}
\item $\E(\vq_n^\top\vepsi_n \cdot \vepsi_n^\top\mA_n\vepsi_n) = \vq_n^\top\diag(\mA_n)\mu_3$, 
\item $\E(\vepsi^\top\mA_n\vepsi \cdot \vepsi_n^\top\mB_n\vepsi_n) = (\mu_4 - 3\sigma^4)\diag(\mA_n)^\top\diag(\mB_n) + \sigma^4\left[\tr(\mA_n)\tr(\mB_n) + \tr(\mA_n\mB_n^s)\right]$, 
\item and 
\begin{equation*}
\begin{aligned}
\cov(\vepsi^\top\mA_n\vepsi \cdot \vepsi_n^\top\mB_n\vepsi_n) & = (\mu_4 - 3\sigma^4)\diag(\mA_n)^\top\diag(\mB_n) + \sigma^4\tr(\mA_n\mB^s_n) \\
&= (\mu_4 - 3\sigma^4)\diag(\mA_n)^\top\diag(\mB_n) + \frac{\sigma^4}{2}\tr(\mA_n^s\mB_n^2).
\end{aligned}
\end{equation*}
\end{enumerate}
\end{lemma}
%--------------------------------------------------

\begin{proof}
As previously noted, $\E(\vepsi \epsilon_{i}\epsilon_j) = \vzeros$ whenever $i\neq j$, and $\E(\vepsi\epsilon_i^2) = \mu_3\ve_i$ where $\ve_i$ is the $n$th unit vector with 1 at its $i$th place and zero elsewhere, we have:
\begin{equation*}
\begin{aligned}
 \E(\vq^\top\vepsi \cdot \vepsi^\top\mA\vepsi) & = \vq^\top\sum_{i = 1}^n\sum_{j = 1}^n a_{ij}\E(\vepsi\epsilon_i\epsilon_j), \\
 & = \mu_3\vq^\top\begin{pmatrix}
                   a_{11} \\
                   a_{22} \\
                   \vdots \\
                   a_{nn}
                   \end{pmatrix}, \\
  & =   \mu_3\vq^\top\diag(\mA).               
\end{aligned}
\end{equation*}

Since:
\begin{equation*}
  \vepsi^\top\mA_n\vepsi \cdot \vepsi_n^\top\mB_n\vepsi_n = \sum_{i=1}^n\sum_{j = 1}^n\sum_{k = 1}^n\sum_{l = 1}^n a_{ij}b_{kl}\epsilon_{i}\epsilon_j\epsilon_k\epsilon_l,
\end{equation*}
%
the mutual independence implies that $\E(\epsilon_{i}\epsilon_j\epsilon_k\epsilon_l)\neq 0$ only if $i = j = k = l$, $(i = j)\neq(k = l)$, $(i = k)\neq (j\neq l)$, and $(i\neq l)\neq (j =k)$. It follows that
\begin{equation*}
\begin{aligned}
\E(\vepsi^\top\mA_n\vepsi \cdot \vepsi_n^\top\mB_n\vepsi_n) & = \sum_{i = 1}^na_{ii}b_{ii}\E(\epsilon_i^4) + \sum_{i = 1}^n\sum_{j \neq i}^n(a_{ii}b_{jj} + a_{ij}b_{ij} + a_{ij}b_{ji})\E(\epsilon_i^2\epsilon_j^2),\\
& = (\mu_4 - 3\sigma^4)\sum_{i = 1}^na_{ii}b_{ii} + \sigma^4\sum_{i = 1}^n\sum_{j = 1}^n (a_{ii}b_{jj} + a_{ij}b_{ij} + a_{ij}b_{ji})\E(\epsilon_i^2\epsilon_j^2) \\
& = (\mu_4 - 3\sigma^4)\diag(\mA)'\diag(\mB) + \sigma^4\left[\tr(\mA)\tr(\mB) + \tr(\mA\mB^\top) + \tr(\mA\mB)\right].
\end{aligned}
\end{equation*}
\end{proof}

%-------------------------------------
\subsection{Law of Large Numbers}
%-------------------------------------

The following Lemma provides the asymptotic behavior for quadratic forms under homoskedasticity \citep[see for example Lemma 2 in][]{kelejian1999generalized}.
%--------------------------------------------------
\begin{lemma}[Consistency of quadratic forms in spatial models]\label{lemma:O-lemma-lee}
Suppose that  $\left\lbrace \mA_n\right\rbrace$ is uniformly bounded in either row and column sums, and $\vepsi_n$ is i.i.d with $\E(\epsilon_i) = 0$ and $\var(\epsilon_i) = \sigma_0^2$ for all $i = 1, \ldots, n$. Then:
\begin{enumerate}
  \item $\E(\vepsi_n^\top\mA_n\vepsi_n) = O(n)$,
  \item $\var(\vepsi_n^\top\mA_n\vepsi_n) = O(n)$,
  \item $\vepsi_n^\top\mA_n\vepsi_n = O_p(n)$,
  \item $\frac{1}{n}\vepsi_n^\top\mA_n\vepsi_n - \frac{1}{n}\E(\vepsi_n^\top\mA_n\vepsi_n) = o_p(1)$.
\end{enumerate}
\end{lemma}
%--------------------------------------------------

This Lemma states that both $\E(\vepsi_n^\top\mA_n\vepsi_n)$ and $\var(\vepsi_n^\top\mA_n\vepsi_n)$ are both well behaved sequence of nonrandom variables. It further states that $\vepsi_n^\top\mA_n\vepsi_n$ is a well behaved (bounded) random sequence such that
\begin{equation*}
\frac{1}{n}\vepsi_n^\top\mA_n\vepsi_n - \frac{1}{n}\E(\vepsi_n^\top\mA_n\vepsi_n)\pto 0.
\end{equation*}

Thus, if the limit of $\tr(\mA_n)/n$ exists, then:
\begin{equation*}
\lim_{n\to\infty}\frac{\tr(\mA_n)}{n} = \mA^*
\end{equation*}
%
and
\begin{equation*}
  \frac{\vepsi_n^\top \mA_n \vepsi_n}{n}\pto \sigma^2_0\mA^*.
\end{equation*}

%------------------------------------------
\begin{proof}
\textbf{Proof of (a)}. By Lemma \ref{lemma:second-mom-lee}, $\E(\vepsi_n^\top\mA_n\vepsi_n) = \sigma_{0}^2\tr(\mA_n)$. By Lemma \ref{lemma:bounde_trace}, $\tr(\mA) = O(n)$. Then $\E(\vepsi_n^\top\mA_n\vepsi_n) = \sigma_{0}^2 O(n) = O(n)$ since $\sigma_{0}^2$ is a constant. 

\textbf{Proof of (b)}. From Lemma \ref{lemma:second-mom-lee}
\begin{equation*}
\begin{aligned}
  \var(\vepsi_n^\top\mA_n\vepsi_n) & = (\mu_4 - 3\sigma^4_0)\sum_{i = 1}^na_{ii}^2+\sigma^4_0\left[\tr(\mA_n\mA_n^\top)+\tr(\mA^2_n)\right].
\end{aligned}
\end{equation*}

From Lemma \ref{lemma:bounde_trace} $\tr(\mA_n\mA_n^\top) = O(n)$ and $\tr(\mA_n^2) = O(n)$. Since 
\begin{equation*}
  \sum_{i = 1}^n a_{n, ii}^2 \leq \tr(\mA_n\mA_n^\top) = O(n), 
\end{equation*}
%
then 
\begin{equation*}
\begin{aligned}
  \var(\vepsi_n^\top\mA_n\vepsi_n) & = (\mu_4 - 3\sigma^4_0)O(n)+\sigma^4_0\left[O(n)+ O(n)\right] \\
  & = O(n)
\end{aligned}
\end{equation*}

\textbf{Proof of (d) and (c)}. By Chebyshev inequality \ref{definition:chebyshev_ineq}
 \begin{equation*}
 \begin{aligned}
  \Pr\left[\frac{1}{n}\left|\vepsi_n^\top \mA_n \vepsi_n - \E(\vepsi_n^\top \mA_n \vepsi_n)\right| \geq \delta \right] &\leq \frac{\var(\frac{1}{n}\vepsi_n^\top \mA_n \vepsi_n)}{\delta^2}, \\
    \Pr\left[\left|\frac{1}{n}\vepsi_n^\top \mA_n \vepsi_n - \sigma^2_{0}\frac{\tr(\mA_n)}{n}\right|  \geq \delta \right] &\leq \frac{\var(\vepsi_n^\top \mA_n \vepsi_n)}{n^2\delta^2},  \\
    & \leq \frac{1}{\delta^2}\frac{1}{n^2}\var(\vepsi_n^\top \mA_n \vepsi_n).
 \end{aligned}
 \end{equation*}

Therefore, $n^{-2}\var(\vepsi_n^\top \mA_n \vepsi_n) = n^{-2} O(n) = O(1/n) =o(1)$.\footnote{$O(n^{-1})$ implies $o(1)$. If $a_n =O(n^{-1})$ means that there exists a constant $c> 0$ such that for all sufficiently large $n$, $|a_n|\leq c/n$. $a_n =o(1)$ means that $\lim_{n\to\infty}a_n = 0$. Since $a_n =O(n^{-1})$ implies that $|a_n|\leq c/n$ for large $n$, it follows that $\lim_{n\to\infty}|a_n|\leq \lim_{n\to\infty}c/n = 0$. Thus, $a_n\to 0$ as $n\to\infty$, which is exactly the definition of $o(1)$.} Thus $\var(n^{-1}\vepsi_n^\top \mA_n \vepsi_n)\to 0$ as $n\to \infty$ and $\Pr\left[\frac{1}{n}\left|\vepsi_n^\top \mA_n \vepsi_n - \E(\vepsi_n^\top \mA_n \vepsi_n)\right|\geq \delta \right]\to 0$, so 
\begin{equation*}
\frac{1}{n}\vepsi_n^\top\mA_n\vepsi_n - \frac{1}{n}\E(\vepsi_n^\top \mA_n\vepsi_n) = o_p(1).
\end{equation*}

Since $\E\left[(\vepsi_n^\top\mA_n\vepsi_n)^2\right]= \var(\vepsi_n^\top\mA_n\vepsi) +  \left(\E(\vepsi_n^\top\mA_n\vepsi_n)\right)^2 = O(n) + O(n)O(n)= O(n^2)$ by Property \ref{prop:blO-nest}. The Chebyshev inequality \ref{definition:chebyshev_ineq} implies that
\begin{equation*}
\Pr\left(\frac{1}{n}\left|\vepsi_n^\top\mA_n\vepsi_n\right| \geq \delta\right) \leq \frac{1}{\delta^2n^2}\E\left[(\vepsi_n^\top\mA_n\vepsi_n)^2\right] = \frac{1}{\delta^2}O(1).
\end{equation*}

Then $\frac{1}{n}\vepsi^\top_n\mA_n\vepsi_n = O_p(1)$, and $n^{-1}\vepsi_n^\top\mA_n\vepsi_n$ is bounded. Furthermore, $\vepsi^\top_n\mA_n\vepsi_n = O_p(n)$.
\end{proof}
%--------------------------------------------------------------------

%----------------------------------
\begin{lemma} Suppose that 
Suppose that  $\left\lbrace \mA_n\right\rbrace$ consists of nonstochastic $n\times n$ matrices uniformly bounded in spectral matrix norm. Let $\vc_n$ be a column vector of constants. If, $\frac{1}{n}\vc_n^\top\vc_n$, then $\frac{1}{\sqrt{n}}\vc_n^\top\mA_n\vepsi_n = o_p(1)$. On the other hand, if $\frac{1}{n}\vc_n^\top\vc_n = O(1)$, then $\frac{1}{\sqrt{n}}\vc_n^\top\mA_n\vepsi_n=O_p(1)$.
\end{lemma}
%----------------------------------

% \begin{proof}
% Note that:
% \begin{equation*}
% \begin{aligned}
%   \var\left(\frac{1}{\sqrt{n}}\vc_n^\top\mA_n\vepsi_n\right) & = \frac{1}{n}\sigma^2\vc_n^\top\mA_n\mA_n^\top\vc_n.
% \end{aligned}
% \end{equation*}
% 
% By Chebyshev's inequality if $\var\left(\frac{1}{\sqrt{n}}\vc_n^\top\mA_n\vepsi_n\right)\to 0$, then $\frac{1}{\sqrt{n}}\vc_n^\top\mA_n\vepsi_n = o_p(1)$
% \end{proof}

% %-----------------------------------------------------------------------------------------------
% \begin{theorem}[Consistency of quadratic forms in spatial models]\label{teo:quadratic-forms-ley}
% Let $\mA_n$ be an $n\times n$ nonstochastic matrix whose row and columns sums are uniformly bounded in absolute value. Let $\vepsi^{\top}_n=(\epsilon_{n1}, ..., \epsilon_{nn})$ where $\epsilon_{ni}$ are iid $(0, \sigma^2_0)$ and $\E(\epsilon_{ni}^4)<\infty$. Then
% \begin{equation*}
%   \frac{\vepsi_n^\top \mA_n \vepsi_n}{n}\pto \E\left(\vepsi_n^\top \mA_n \vepsi_n\right) = \sigma^2_0\frac{\tr(\mA_n)}{n}
% \end{equation*}
% \end{theorem}
% %-----------------------------------------------------------------------------------------------
% 
% 
% \begin{proof}
%  This result follows from Theorem \ref{teo:chebyshev}. By Chebyshev inequality \ref{definition:chebyshev_ineq}
%  \begin{equation*}
%  \begin{aligned}
%   \Pr\left[\frac{1}{n}\left|\vepsi_n^\top \mA_n \vepsi_n - \E(\vepsi_n^\top \mA_n \vepsi_n)\right| \geq \delta \right] &\leq \frac{\var(\vepsi_n^\top \mA_n \vepsi_n)}{n^2 \delta^2} \\
%     \Pr\left[\left|\frac{1}{n}\vepsi_n^\top \mA_n \vepsi_n - \sigma^2_{0}\frac{\tr(\mA_n)}{n}\right|  \geq \delta \right] &\leq \frac{\var(\vepsi_n^\top \mA_n \vepsi_n)}{n^2\delta^2}  \\
%     & \leq \frac{1}{\delta^2}n^{-2}\var(\vepsi_n^\top \mA_n \vepsi_n)
%  \end{aligned}
%  \end{equation*}
%  
%  By Lemma \ref{lemma:bounde_trace}, $\tr(\mA_n) = O(n)$. Hence $\lim_{n\to\infty} \frac{1}{n}\E(\vepsi_n^\top \mA_n \vepsi_n) = \sigma_{0}^2\mA^* = O(1)$ and the expectation exists. 
%  
%  By Lemma \ref{lemma:O-lemma-lee} $\var(\vepsi_n\mA_n\vepsi_n) = O(n)$. Therefore, $n^{-2}\var(\vepsi_n^\top \mA_n \vepsi_n) = \var(n^{-1}\vepsi_n^\top \mA_n \vepsi_n) = O(n^{1}) = o(1)$. Thus $\var(n^{-1}\vepsi_n^\top \mA_n \vepsi_n)\to 0$ as $n\to \infty$ and $\Pr\left[\frac{1}{n}\left|\vepsi_n^\top \mA_n \vepsi_n - \E(\vepsi_n^\top \mA_n \vepsi_n)\right|\geq \delta \right]\to 0$, so 
% \begin{equation*}
% \frac{1}{n}\vepsi_n^\top\mA_n\vepsi_n - \frac{1}{n}\E(\vepsi_n^\top \mA_n\vepsi_n) = o_p(1)
% \end{equation*}
% %
% which can also be written as
% \begin{equation*}
%   \frac{\vepsi_n^\top \mA_n \vepsi}{n}\pto \sigma^2_0\mA^*.
% \end{equation*}
% \end{proof}


%---------------------------------
\section{CLT for Spatial Models}
%---------------------------------

\begin{theorem}[Liapounov's CLT theorem for double arrays]
Let $\left\lbrace X_{ni}: i = 1, \ldots, n\right\rbrace$ be an array of independent random variables. Let $\E(X_{ni}) = \mu_n$ and $\E(X_{ni} - \mu_{ni})^2 = \sigma_{ni}^2\neq 0$. If
\begin{equation*}
\frac{\sum_{k = 1}^n\E\left|X_{nk}- \mu_{nk}\right|^2}{(\sum_{i= 1}^n \sigma^2_{ni})^{1 + \delta/2}}\to 0,
\end{equation*}
%
for a positive $\delta >0$, then
\begin{equation*}
\sqrt{n}\frac{\sum_{i = 1}^n(X_{ni}-\mu_{ni})}{(\sum_{i = 1}^n \sigma^2_{ni})^{1/2}}\dto \rN(0,1).
\end{equation*}
\end{theorem}

The following theorem states the limiting distribution for triangular arrays with homoskedastic errors in linear forms:

%--------------------------------------------------------------------
\begin{theorem}[CLT for triangular arrays with homoskedastic errors, \citep{kelejian1998generalized}]\label{teo:CLT_tri_arr}
Let $\left\lbrace v_{i,n}, 1 \leq i \leq n, n\geq 1\right\rbrace $ be a triangular array of identically distributed random variables. Assume that the random variables $\left\lbrace v_{i,n}, 1 \leq i \leq n\right\rbrace$ are jointly independently distributed for each $n$ with $\E(v_{i,n})= 0$ and $\E(v_{i,n}^2) = \sigma^2 < \infty$. Let $\left\lbrace a_{ij,n}, 1 \leq i \leq n, n\geq 1\right\rbrace, j = 1,\ldots,k$ be triangular arrays of real numbers that are bounded in absolute value. Further let
\begin{equation*}
  \vv_n = \begin{pmatrix}
            v_{1, n} \\
            \vdots \\
            v_{n,n}
          \end{pmatrix}, \quad
          \mA_n = \begin{pmatrix}
            a_{11,n} & \hdots & a_{1k, n} \\
            \vdots &         & \vdots \\
            a_{n1,n} & \hdots & a_{nk, n}
          \end{pmatrix}
\end{equation*}

Then:
\begin{equation*}
\frac{1}{\sqrt{n}}\mA_n^\top\vv_n = O_p(1)
\end{equation*}

Furthermore, assume that $\lim_{n\to\infty}n^{-1}\mA^\top_n\mA_n = \mQ_{AA}$ is finite and nonsingular matrix. Then
\begin{equation*}
\frac{1}{\sqrt{n}}\mA_n^\top\vv_n \dto \rN(\vzeros, \sigma^2 \mQ_{AA})
\end{equation*}
\end{theorem}
%--------------------------------------------------------------------

\cite{kelejian2001asymptotic} introduced a CLT for a single quadratic form under the assumptions useful for spatial models. The generalization to vectors of linear quadratic forms is given in \cite{kelejian2010specification}.

%------------------------------------------------------
\begin{theorem}[CLT for Vectors of Linear Quadratic Forms with Heterokedastic Innovations]\label{teo:clt_quadratic}
		Assume the following:
		\begin{enumerate}
			\item For $r = 1,\ldots, m$ let $\mA_{r,n}$ with elements $(a_{ijr})_{i,j = 1, \ldots,n}$ be an $n\times n$ non-stochastic symmetric real matrix with $\sup_{1\leq j \leq n, n\geq 1}\sum_{i = 1}^n|a_{ijr}| < \infty$,
			\item and let $\va_r = (a_{ir},\ldots ,a_{nr})^\top$ be a $n \times 1$ non-stochastic real vector with $\sup_{n}\frac{\sum_{i=1}^n \left|a_{ir}\right|^{\delta_1}}{n}<\infty$ for some $\delta_1 > 2$.
			\item Let $\vepsi = (\epsilon_1,\ldots ,\epsilon_n)^\top$ be an $n \times 1$ random vector with the $\epsilon_i$ distributed totally independent with $\E\left[\epsilon_i\right] = 0, \E\left[\epsilon_i^2\right]$, and  $\sup_{1 \leq i \leq n, n \geq 1}\E\left|\epsilon_{i}\right|^{\delta_2}<\infty$ for some $\delta_2 >4$. 
		\end{enumerate}
		Consider the $m \times 1$ vector of linear quadratic forms $\vv_n = \left[Q_{1n},\ldots, Q_{mn}\right]'$ with:
		\begin{equation*}
			Q_{rn}= \vepsi'\mA_r\vepsi + \va_r'\vepsi = \sum_{i=1}^n\sum_{j=1}^na_{ijr}\epsilon_i\epsilon_j + \sum_{i=1}^na_{ir}\epsilon_i.
		\end{equation*}
		
		Let $\mu_{\vv} = \E\left[\vv_n\right] = \left[\mu_{Q_1},\ldots,\mu_{Q_2}\right]^\top$ and $\mSigma_{\vv_n} =\left[\sigma_{Q_{rs}}\right]_{r,s =1,\ldots,m}$ denote the mean and VC matrix of $\vv_n$, respectively, then:
		\begin{equation*}
			\begin{aligned}
			\mu_{Q_r} & = \sum_{i = 1}^na_{iir}\sigma_i^2 \\
			 \sigma_{Q_{rs}} & = 2\sum_{i=1}^n\sum_{j =1}^na_{ijr}a_{ijs}\sigma^2_i\sigma_j^2 + \sum_{i = 1}^na_{ir}a_{is}\sigma_i^2 \\
			 	            & + \sum_{i = 1}^na_{iir}a_{iis}\left[\mu_i^{(4)} - 3\mu_i^{4}\right] + \sum_{i =1}^n(a_{ir}a_{iis} + a_{is}a_{iir})\mu_i^{(3)}
			\end{aligned}
		\end{equation*}
		%
		with $\mu_i^{(3)} = \E(\epsilon_i^3)$ and $\mu_i^{(4)} = \E(\epsilon_i^4)$. Furthermore, given that $n^{-1}\lambda_{min}(\mSigma_{\vv_n})\geq c$ for some $c > 0$, then
		\begin{equation*}
		\mSigma_{\vv_n} ^{-1/2}(\vv_n - \mu_{\vv_n})\dto \rN(\vzeros, \mI_m),
		\end{equation*}
		%
		and thus:
		\begin{equation*}
			n^{-1/2}(\vv_n - \mu_{\vv_n}) \adistr \rN(\vzeros, n^{-1}\mSigma_{\vv_n})
		\end{equation*}
\end{theorem}	
%--------------------------------------------------

In case that $\mA_{r,n}$ is not symmetric, it can be replaced by $(\mA_{r, n}+ \mA_{r, n}^\top)/2$. See our Definition \ref{def:quad-form}. Also note that the matrix $\mA_{r,n}$ is allows to have non-zero diagonal elements. However, for most spatial models this is not needed, since, by convention, the diagonals of $\mW$ and $\mM$ are zero. If the diagonal elements of $\mA_{r,n}$ are zero, then the mean $\mu_{Q_r}  = \sum_{i = 1}^na_{iir}\sigma_i^2 = 0$. Similarly, if the $\epsilon_i$ are homoskedastic, then $\tr(\mA_r) = \sum_{i = 1}^n a_{ii,r} = 0$ and, as a result, $\mu_{Q_r} = 0$. 

In addition, the covariance $\sigma_{Q_{rs}}$ between $Q_{rn}$ and $Q_{sn}$ can be written more compactly as
\begin{equation*}
2\tr\left(\mA_r\mSigma\mA_s\mSigma\right) + \va_r^\top\mSigma\va_s,
\end{equation*}
%
with $\mSigma = \Diag(\sigma_i^2)$. Finally, note that if $a_{iir} = a_{iis} = 0$, then the last two terms drop out from the expression for covariance. Under normality, $\mu_i^{(3)} = 0$ and $\mu_{i}^{(4)} = 3$ the last two terms are always equal to zero. 

This Theorem would be useful in, for example, Chapter \ref{chapter:gmm}. For an univariate application see Annex \ref{Annex:asymptotic-moran-I}. Consider the following quadratic moment:
\begin{equation*}
\begin{aligned}
g_n(\rho_0, \widetilde{\vdelta}_n) & = \widetilde{\vu}_n^\top\left(\mI_n - \rho_0\mM_n\right)^\top\mA_n\left(\mI_n - \rho_0\mM_n\right)\widetilde{\vu}_n, \\
                                  & = \widetilde{\vu}_n^\top\mC_0\widetilde{\vu}_n, 
\end{aligned}
\end{equation*}
%
where $\mC_0 = (1 / 2)\left(\mI_n - \rho_0\mM_n\right)^\top\left(\mA_n + \mA_n^\top\right)\left(\mI_n - \rho_0\mM_n\right)$. A Taylor expansion of $g_n(\rho_0, \widetilde{\vdelta}_n)$ around $\vdelta_0$, gives:
\begin{equation*}
  \frac{1}{n}g_n(\rho_0, \widetilde{\vdelta}_n) = \frac{1}{n}g_n(\rho_0, \vdelta_0) + \frac{1}{n}\frac{\partial g_n(\rho_0, \vdelta_0)}{\partial \vdelta}(\widetilde{\vdelta}_n - \vdelta_0).
\end{equation*}

If $\vu_n = \vy - \mZ_n\vdelta_0$, with $\mZ_n = \left[\mW_n\vy_n, \mX_n\right]$, then
\begin{equation*}
\begin{aligned}
\frac{1}{n}\frac{\partial g_n(\rho_0, \vdelta_0)}{\partial \vdelta} & = 2\frac{1}{n}\E\left(\vu_n^\top\mC_0\frac{\partial \vu_n}{\partial \vdelta_0}\right), \\
& = - 2n^{-1}\E\left(\vu_n^\top\mC_0\mZ_n\right).
\end{aligned}
\end{equation*}

If we assume that $(\widetilde{\vdelta}_n - \vdelta_0) = \mT_{n}^\top\vepsi_n + o_p(1)$, and $\vu_n = (\mI_n - \rho_0\mM_n)^{-1}\vepsi_n$, then
\begin{equation*}
\begin{aligned}
  \frac{1}{n}g_n(\rho_0, \widetilde{\vdelta}_n)  & = \frac{1}{2}n^{-1}\vepsi_n^\top\left(\mA_n + \mA_n^\top\right)\vepsi_n - 2n^{-1}\E\left(\vu_n^\top\mC_0\mZ_n\right)\mT_{n}^\top\vepsi_n + o_p(1) \\
  & = \frac{1}{2}n^{-1}\vepsi_n^\top\left(\mA_n + \mA_n^\top\right)\vepsi_n + \va_n^\top\vepsi_n + o_p(1),
  \end{aligned}
\end{equation*}
%
where
\begin{equation*}
\va_n = -2n^{-1}\mT_n\E\left(\mZ_n^\top\mC_0\vu_n\right) = -n^{-1}\mT_n\E\left[\mZ_n^\top(\mI_n - \rho_0\mM_n^\top)(\mA_n + \mA_n^\top)(\mI_n - \rho_0\mM_n)\vu_n\right]. 
\end{equation*}

Since $\mZ_n$ is stochastic, $\va_n \neq \vzeros$. 



%-----------------------
\section{Exercises}
%-------------------

\begin{exercises}
    \exercise Provide a proof of Proposition \ref{prop:blO-nest}.
    \exercise Let $\mA_n$ be a $k\times k$ matrix and let $\vb_n$ be a $k\times 1$ vector. If $\mA_n = o(1)$ and $\vb_n = O(1)$, show that $\mA_n\vb_n= o(1)$.
    \exercise Prove the following result for the 2SLS estimator. Suppose: (i) $y_i = \vx_i^\top\vbeta_0 + \epsilon_i$, $i= 1, ..., n$, $\vbeta_0\in \SR^k$; (ii) $\mZ^\top\vepsi/n\pto \vzeros$; (iii) $\mZ^\top\mX/n\pto \mQ$, finite with full column rank; (iv) $\widehat{\mP}_n\pto \mP$, finite, symmetric, and positive definite. Then $\widehat{\vbeta}_n$ exists in probability, and  $\widehat{\vbeta}\pto \vbeta_0$.
\end{exercises}   



\section*{Appendix}
\begin{subappendices}

%----------------------------------
\section{Matrix Norm}
%----------------------------------

\begin{definition}[Matrix Norm]
Given a square complex or real matrix $\mA$, a matrix norm $\lVert \mA \rVert$ is a nonnegative number associated with $\mA$ having the propertie
\begin{enumerate}
  \item $\lVert \mA \rVert> 0$ when $\mA\neq 0$ and $\lVert \mA \rVert = 0$ if and only if $\mA = 0$, 
  \item $\lVert k \mA \rVert = \left|k\right|\lVert \mA \rVert$ for any scalar $k$, 
  \item $\lVert \mA  + \mB \rVert \leq \lVert \mA \rVert + \lVert \mB \rVert$, 
  \item $\lVert \mA\mB \rVert \leq \lVert \mA \rVert \lVert \mB \rVert$
\end{enumerate}
\end{definition}

The maximum absolute column sum norm $\lVert \mA \rVert_1$ is defined as
\begin{equation*}
\lVert \mA \rVert_1 = \max_j\sum_{i = 1}^n\left|a_{ij}\right|
\end{equation*}

The spectral norm $\lVert \mA \rVert_2$ is defined as
\begin{equation*}
\lVert \mA \rVert_2 = \sigma_{\max}(\mA) = \sqrt{\lambda_{\max}\left(\mA^\top \mA\right)}
\end{equation*}

The maximum absolute row sum norm is defined by
\begin{equation*}
\lVert \mA \rVert_{\infty} = \max_i\sum_{j = 1}^n\left|a_{ij}\right|
\end{equation*}

$\lVert \mA \rVert_1$, $\lVert \mA \rVert_2$ and $\lVert \mA \rVert_{\infty}$ satisfy the inequality $\lVert \mA \rVert_2^2 \leq \lVert \mA \rVert_1\lVert \mA \rVert_{\infty}$.


%-------------------------------------------------------------------------
 \section{Inqualities}\label{appendix-inequalities}
%-------------------------------------------------------------------------

\begin{definition}[Multiplicativity of absolute value]\label{def:abs-multiplicativity}
For any two random variables $a$ and $b$
\begin{equation}
\left|ab\right| = \left|a\right|\left|b\right|
\end{equation}
\end{definition}

\begin{definition}[Triangle Inequality]\label{def:triangle-inequality}
For any real numbers $x_j$, 
\begin{equation}
  \left|\sum_{j= 1}^n x_j\right| \leq \sum_{j = 1}^n \left|x_j\right|
\end{equation}
\end{definition}

\begin{definition}[Chebyshev's inequality]\label{definition:chebyshev_ineq}
	 If $X_n$ is a random variable with mean $\mu$ and \textbf{finite variance}, then, for every $\delta > 0$,
	 
	 \begin{equation*}
	 \Pr\left[\left| X_n - \mu\right|\geq \delta \right]\leq \frac{\E\left[(X_n - \mu)^2\right]}{\delta ^2}
	 \end{equation*}
\end{definition}

To prove the Chebyshev inequality, we use the Markov's Inequality

\begin{definition}[Markov's inequality]\label{definition:chebyshev_ineq}
	 If $X_n$ is a nonnegative random variable, then for every $\delta > 0$,
	 
	 \begin{equation*}
	 \Pr\left[X_n \geq \delta \right]\leq \frac{\E\left[X_n\right]}{\delta}
	 \end{equation*}
\end{definition}

\begin{definition}[Jensen's Inequality]\label{theorem:Jansen-Inequality}
If $g(X_n)$ is a concave function of $X_n$ then

\begin{equation*}
  g\left[\E(X_n)\right] \geq \E\left[g(X_n)\right]
\end{equation*}
\end{definition}

\begin{definition}[Cauchy-Schwarz Inequality]\label{theorem:Cauchy-Inequality}
For two random variables
\begin{equation*}
  \E\left[\left|X_nY_n\right|\right]\leq \left\lbrace \E\left[X_n^2\right]\right\rbrace^{1/2} \left\lbrace \E\left[Y_n^2\right]\right\rbrace^{1/2} 
\end{equation*}
\end{definition}


\end{subappendices}