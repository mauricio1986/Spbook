\chapter{Introduction to Spatial Econometric}\label{chapater:Introduction}


%===============================================================
\section{Why do We Need Spatial Econometric?}\label{sec:why_se}
%===============================================================

An essential consideration in any study involving spatial units, such as cities, regions, or countries, is the potential relationships and interactions between them. For instance, when modeling pollution at the regional level, it becomes impractical to treat each region as an independent entity. Regions cannot be analyzed in isolation since they are spatially interrelated through ecological and economic interactions.

Consider Figure \ref{fig:example_poll}, where Region 3 (R3) is highly industrialized, while Regions 1, 2, 4, and 5 are residential areas. If Region 3 increases its economic activity, pollution will not only rise within that region but will also affect neighboring regions. It is anticipated that contamination will increase in Regions 1 and 5, albeit to a lesser extent. These spatial externalities from R3 can be generated by both spatial-economic interactions (e.g., transportation of inputs and outputs from Region 3) and spatial-ecological interactions (e.g., carbon emissions).

\begin{figure}[h]
\caption{Environmental Externalities}
\label{fig:example_poll}
\centering
\begin{tikzpicture}[scale = 1.5]
\draw[color = gray] (0,0) grid[xstep = 1cm, ystep = 1cm] (5,1);
\node (R1) at (.5, .5) {R1};
\node (R2) at (1.5,.5) {R2};
\node (R3) [color =  red]  at (2.5,.5) {R3};
\node (R4) at (3.5,.5) {R4};
\node (R5) at (4.5,.5) {R5};
\draw [->] (R3) -- (R2);
\draw [->] (R3) -- (R4);
\draw [->] (R2) -- (R1);
\draw [->] (R4) -- (R5);
\end{tikzpicture}
\end{figure}

Similarly, when studying crime at the city level, it is crucial to incorporate the possibility that crime is localized. The identification of concentrations or clusters of higher criminal activity has become a central mechanism for targeting criminal justice and crime prevention responses. These crime clusters, commonly referred to as \textbf{hotspots}, represent geographic locations with high crime concentrations relative to the overall distribution of crime across the entire region of interest.

Both examples implicitly highlight the significance of geographic location and distance. They underscore the importance of the first law of geography, as articulated by Waldo Tobler: \emph{``everything is related to everything else, but near things are more related than distant things''}. This foundational principle gives rise to fundamental concepts such as \textbf{spatial dependence} and \textbf{spatial autocorrelation}.\index{Tobler's law}

%---------------------------------------------
\subsection{Spatial Dependence}\label{sec:spatial_dependence}\index{Spatial dependence}
%---------------------------------------------

Spatial dependence occurs when the values observed at one location or region, say observation $i$, depend on the values of neighboring observations at nearby locations. Formally, we can express this as:
\begin{equation*}
  y_i = f(y_j),\quad i = 1,\ldots,n\;, j\neq i.
\end{equation*}

In simpler terms, what happens in region $i$ depends on what happens in region $j$ for all $j \neq i$. In our previous example, we would like to estimate 
\begin{equation*}
  \begin{aligned}
y_1 & = \beta_{21} y_2 + \beta_{31} y_3 + \beta_{41} y_4 + \beta_{51} y_5 + \epsilon_1, \\
y_2 & = \beta_{12} y_1 + \beta_{32} y_3 + \beta_{42} y_4 + \beta_{52} y_5 + \epsilon_2, \\
y_3 & = \beta_{13} y_1 + \beta_{23} y_2 + \beta_{43} y_4 + \beta_{53} y_5 + \epsilon_3, \\
y_4 & = \beta_{14} y_1 + \beta_{24} y_2 + \beta_{34} y_3 + \beta_{54} y_5 + \epsilon_4, \\
y_5 & = \beta_{15} y_1 + \beta_{25} y_2 + \beta_{35} y_3 + \beta_{45} y_5 + \epsilon_5, 
\end{aligned}
\end{equation*}
%
where $\beta_{ji}$ represents the effect of pollution in region $j$ on region $i$. However, this model becomes impractical as it would result in a system with many more parameters than observations. With $n = 5$ observations, we would have to estimate 20 parameters, exceeding the available degrees of freedom. Intuitively, allowing for dependence relations between a set of $n$ observations/locations introduces potentially $n^2 - n$ relations, accounting for the exclusion of dependence on oneself.

The crucial point is that, under standard econometric modeling, incorporating spatial dependency in such a manner is impractical. However, as we will explore in the next sections, we can efficiently integrate spatial relationships using the so-called spatial weight matrix. 

%===========================================
\subsection{Spatial Autocorrelation}\label{sec:Spatial_autocorrelation}
%===========================================

Another crucial concept is \textbf{spatial autocorrelation}\index{Spatial autocorrelation}. In a spatial context, autocorrelation denotes the correlation between the values of a variable at two different locations. It can also be defined as the correlation between the same attribute at two (or more) different locations or the coincidence of values' similarity with location similarity. Essentially, spatial autocorrelation investigates whether the presence of a variable in one region of a spatial system makes the presence of that variable in neighboring regions more or less likely.

The counterpart of spatial autocorrelation (and spatial dependency) is spatial randomness. Spatial randomness implies the absence of any discernible spatial pattern in the data. In other words, the value observed in one spatial unit is equally likely as in any other spatial unit. Spatial randomness is important because it will form the null hypothesis later. If rejected, there is evidence of spatial structure.

As an illustrative example, Figure \ref{fig:MR} depicts the spatial distribution of poverty in the Metropolitan Region, Chile. It reveals a discernible spatial pattern where communes with similar poverty rates are clustered.
\begin{figure}[ht]
  \caption{Spatial Distribution of Poverty in Metropolitan Region, Chile}
    \label{fig:MR}
    \centering
    	\begin{minipage}{.9\linewidth}
        <<MetroRegion, echo = FALSE, message = FALSE, fig.align='center', out.width = '8cm', out.height = '8cm', warning = FALSE>>=
        library("maptools") # Tools for reading and handling spatial objects
        mr <- readShapeSpatial("mr_chile")
        library("RColorBrewer")
        spplot(mr, "POVERTY", at = quantile(mr$POVERTY, p = c(0, .25, .5, .75, 1), na.rm = TRUE),
        col.regions = brewer.pal(5, "Blues"))
@
\footnotesize
		\emph{Notes:} This graph shows the spatial distribution of poverty in the Metropolitan Region, Chile. 
	\end{minipage}	
\end{figure}

Formally, the presence of spatial autocorrelation can be expressed through the following moment conditions:
\begin{equation*}
  \cov(y_i, y_j) = \E(y_iy_j) - \E(y_i)\E(y_j) \neq 0 \;\; \text{for} \;\; i \neq j,
\end{equation*}
%
where $y_i$ and $y_j$ are observations on a random variable at locations $i$ and $j$ in space. Here, $i$ and $j$ can represent either points or areal units. Therefore, nonzero spatial autocorrelation exists between attributes of a feature defined at locations $i$ and $j$ if the covariance between feature attribute values at those points is nonzero. If this covariance is \textbf{positive} (i.e., if data with attribute values above the mean tend to be near other data with values above the mean), then we refer to it as \textbf{positive spatial autocorrelation}; conversely, if the opposite is true, we term it \textbf{negative spatial autocorrelation}. Figure \ref{fig:Autocorrelation} illustrates examples of positive and negative spatial autocorrelation.

\begin{figure}[ht]
  \caption{Spatial Autocorrelation}
    \label{fig:Autocorrelation}
    \centering
    	\begin{minipage}{1\linewidth}
        <<Autocorrelation, echo = FALSE, message = FALSE, fig.align='center', out.width = '10cm', out.height = '10cm'>>=
library("maptools")
library("spdep")
library("spatialreg")
# Set up a grid data representing  neighborhoods
set.seed(123)
n <- 20
grid_high <- GridTopology(cellcentre.offset = c(1, 1),
                          cellsize  = c(1, 1), 
                          cells.dim = c(n, n))
grid_sp <- SpatialGrid(grid_high)
### Create attributes
rook <- cell2nb(n, n)
y <- rnorm(n^2)
Iw1 <- invIrM(rook, 0.98)
Iw2 <- invIrM(rook, -0.98)
y1 <- c(Iw1 %*% y)
y2 <- c(Iw2 %*% y)
# Create a SpatialGridDataFrame
grid_data <- SpatialGridDataFrame(grid = grid_sp, data = data.frame(y1, y2))
# Now check names of each grid cell and plot the four cells from the left-top corner.
#par(mfrow = c(1,2), mar = c(0, 0, 0, 0))
layout(matrix(1:2, 1, 2), widths = c(4,4))
plot(grid_data[,, "y1"], 
     col = grey(1:100 / 100), 
     axes = FALSE, 
     what = "image",
     asp = 1.5,
     border = grey(0.6)
     )
title(main = list("Positive Spatial Autocorrelation", cex = 1,
                  col = "black", font = 1))
plot(grid_data[,, "y2"], 
     col = grey(1:100 / 100), 
     axes = FALSE, 
     what = "image",
     asp = 1.5,
     border = grey(0.6)
     )
title(main = list("Negative Spatial Autocorrelation", cex = 1,
                  col = "black", font = 1))
@
\footnotesize
		\emph{Notes:} Spatial Autocorrelation among 400 spatial units arranged in an 20-by-20 regular square lattice grid. Different gray-tones refer to different values of the variable ranging from low values (white) to high values (black). The left plot shows positive spatial autocorrelation, whereas right plot shows negative spatial autocorrelation. 
	\end{minipage}	
\end{figure}

Positive autocorrelation is more commonly observed, but negative autocorrelation does exist. Instances of negative autocorrelation can be found in studies on welfare competition or federal grants competitions among local governments \citep{saavedra2000model, boarnet2002federal}, regional employment \citep{filiztekin2009regional, pavlyuk2011spatial}, cross-border lottery shopping \citep{garrett2002revenue}, foreign direct investment in OECD countries \citep{garretsen2009fdi}, and the locations of the Turkish manufacturing industry \citep{basdas2009spatial}. In essence, our interest lies in studying non-random spatial patterns and explaining this non-randomness. Potential causes of non-randomness, as outlined by \cite{gibbons2015spatial}, include:

\begin{enumerate}
    \item Economic agents may be randomly allocated across space, but some characteristics of locations vary across space and influence outcomes.
    \item Location may have no causal effect on outcomes, but outcomes may be correlated across space because heterogeneous individuals or firms are non-randomly allocated across space.
    \item Individuals or firms may be randomly allocated across space, but they interact in a way that decisions by one agent affect the outcomes of other agents.
    \item Individuals or firms may be non-randomly allocated across space, and the characteristics of others nearby directly influence individual outcomes.
\end{enumerate}

%********************************
\section{Spatial Weight Matrix}\index{Weight matrix}
%********************************

One of the crucial issues in spatial econometric is the problem of formally incorporating spatial dependence into the model. As we reviewed in Section~\ref{sec:spatial_dependence}, the main problem is that we have more parameter than observations. So, the question is: What would be a good criteria to define closeness in space? Or, in other words, how to determine which other units in the system influence the one under consideration?

The device typically used in spatial analysis to define the concept of closeness in space is the so-called ``spatial weight matrix'', or more simply, $\mW$ matrix. Assuming there are $n$ spatial objects (regions, cities, countries), the $\mW$ matrix is a square matrix of dimension $n \times n$. This matrix imposes a structure in terms of identifying neighbors for each location, assigning weights that measure the intensity of the relationship among pairs of spatial units. Each element $(i,j)$ of $\mW$, denoted as $w_{ij}$, expresses the degree of spatial proximity between the pair. The matrix can be represented as follows:
\begin{equation*}
\mW = \begin{pmatrix}
        w_{11} & w_{12} & \hdots & w_{1n} \\ 
        w_{21} & w_{22} & \hdots & w_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        w_{n1} & w_{n2} & \hdots & w_{nn} 
      \end{pmatrix}.
\end{equation*}

Generally, we assume that the diagonal elements of this ``spatial neighbors'' matrix are set to zero, indicating that ``regions are not neighbors to themselves''.

A more formal definition of a spatial weight matrix is as follows:

%-----------------------------------------------------------------------------------
\begin{definition}[Spatial Weight Matrix]\index{Weight matrix!Definition}\label{def:W}
  Let $n$ be the number of spatial units. The spatial weight matrix, $\mW$, is a $n\times n$ \textbf{positive} and \textbf{non-stochastic} matrix with element $w_{ij}$ at location $i,j$. The values of $w_{ij}$ or the weights for each pair of locations are assigned by some preset rules which define the spatial relations among locations. By convention, $w_{ij} = 0$ for the diagonal elements.
\end{definition}
%-----------------------------------------------------------------------------------

Positive means that $w_{ij}\geq 0$ for all $i\neq j$. Thus, the interactions between spatial units cannot be negative. Non-stochastic means that the researcher takes $\mW$ as known \emph{a priori}, and therefore, all results are conditional upon the specification of $\mW$.

The definition of $\mW$ also requires a rule for $w_{ij}$. In other words, we need to figure out how to assign a real number to $w_{ij}$, for $i\neq j$, representing the strength of the spatial relationship between $i$ and $j$. There are several ways of doing that. But, in general, there are two basic criteria. The first type establishes a relationship based on shared borders or vertices of lattice or irregular polygon data (contiguity). The second type establishes a relationship based on the distance between locations. Generally speaking, contiguity is most appropriate for geographic data expressed as polygons (so-called areal units), whereas distance is suited for point data, although in practice, the distinction is not that absolute. 

%==========================================
\subsection{Weights Based on Boundaries}
%==========================================

Polygon or lattice data allow for the construction of contiguity-based spatial weight matrices, which represent spatial relationships among regions. A typical specification of the contiguity relationship in such matrices is given by:
\begin{equation*}
  w_{ij}= 
   \begin{cases}
      1 & \mbox{if $i$ and $j$ are contiguous,} \\ 
      0 & \mbox{if $i$ and $j$ are not contiguous.} 
   \end{cases}
\end{equation*}

In regular grids, the definition of contiguity can vary. Drawing an analogy to chess, three common contiguity criteria are rook contiguity, bishop contiguity, and queen contiguity. Each criterion defines neighborhood relationships differently, as detailed below.

\subsubsection{Rook Contiguity}\index{Weight matrix!Rook contiguity}

Rook contiguity considers two regions as neighbors if they share a common border or side. For example, in the regular grid shown in Figure~\ref{fig:Rook_cont_grid}, which contains 9 regions (each represented as a square), the neighbors of region 5 under the rook criterion are regions 2, 4, 6, and 8 (highlighted in red).

\begin{figure}[h]
\caption{Rook Contiguity}
\label{fig:Rook_cont_grid}
\centering
\begin{tikzpicture}[scale = 1.5]
\path [fill=red!5] (1,2) -- (2,2) -- (2,3) -- (1,3);
\path [fill=red!5] (1,0) -- (2,0) -- (2,1) -- (1,1);
\path [fill=red!5] (0,1) -- (1,1) -- (1,2) -- (0,2);
\path [fill=red!5] (2,1) -- (3,1) -- (3,2) -- (2,2);
\draw[color = gray] (0,0) grid[xstep = 1cm, ystep = 1cm] (3,3); % grid of 3 times 3
\node at (.5, 2.5) {1};
\node at (1.5, 2.5) {2};
\node at (2.5, 2.5) {3};
\node at (.5, 1.5) {4};
\node [color =  red] at (1.5,1.5) {5} ;
\node at (2.5,1.5) {6};
\node at (.5, .5) {7};
\node at (1.5,.5) {8};
\node at (2.5,.5) {9};
\end{tikzpicture}
\end{figure}

Using this criterion, the $9 \times 9$ spatial weight matrix $\mW$ is:
\begin{equation*}
  \mW = 
  \begin{pmatrix}
     0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
     1 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 \\
     0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
     1 & 0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 \\
     0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 \\
     0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 1 \\
     0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 0 \\
     0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 & 1 \\
     0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 \\
  \end{pmatrix}
\end{equation*}

\subsubsection{Bishop Contiguity}\index{Weight matrix!Bishop contiguity}

Under bishop contiguity, two regions are neighbors if they share a common corner. This criterion is less frequently used in practice. In Figure~\ref{fig:Bishop_cont_grid}, the neighbors of region 5 are regions 1, 3, 7, and 9 (highlighted in red). Note that regions located at the grid's interior have more neighbors than those along the periphery.

\begin{figure}[h]
\caption{Bishop Contiguity}
\label{fig:Bishop_cont_grid}
\centering
\begin{tikzpicture}[scale = 1.5]
\path [fill=red!5] (0,2) -- (1,2) -- (1,3) -- (0,3);
\path [fill=red!5] (0,0) -- (1,0) -- (1,1) -- (0,1);
\path [fill=red!5] (2,2) -- (3,2) -- (3,3) -- (2,3);
\path [fill=red!5] (2,0) -- (3,0) -- (3,1) -- (2,1);
\draw[color = gray] (0,0) grid[xstep = 1cm, ystep = 1cm] (3,3); % grid of 3 times 3
\node at (.5, 2.5) {1};
\node at (1.5, 2.5) {2};
\node at (2.5, 2.5) {3};
\node at (.5, 1.5) {4};
\node [color =  red] at (1.5,1.5) {5} ;
\node at (2.5,1.5) {6};
\node at (.5, .5) {7};
\node at (1.5,.5) {8};
\node at (2.5,.5) {9};
\end{tikzpicture}
\end{figure}

The corresponding $\mW$ matrix is:
\begin{equation*}
\mW = 
  \begin{pmatrix}
     0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
     0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 \\
     0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
     0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
     1 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 1 \\
     0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
     0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
     0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 \\
     0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
  \end{pmatrix}
\end{equation*}

This criteria is seldom used in practice. 

\subsubsection{Queen Contiguity}\index{Weight matrix!Queen contiguity}

Queen contiguity considers two regions as neighbors if they share either a common side or a common corner. In Figure~\ref{fig:Queen_cont_grid}, the neighbors of region 5 include all surrounding regions: 1, 2, 3, 4, 6, 7, 8, and 9 (highlighted in red).

\begin{figure}[h]
\caption{Queen Contiguity}
\label{fig:Queen_cont_grid}
\centering
\begin{tikzpicture}[scale = 1.5]
\path [fill=red!5] (0,2) -- (1,2) -- (1,3) -- (0,3);
\path [fill=red!5] (0,0) -- (1,0) -- (1,1) -- (0,1);
\path [fill=red!5] (2,2) -- (3,2) -- (3,3) -- (2,3);
\path [fill=red!5] (2,0) -- (3,0) -- (3,1) -- (2,1);
\path [fill=red!5] (1,2) -- (2,2) -- (2,3) -- (1,3);
\path [fill=red!5] (1,0) -- (2,0) -- (2,1) -- (1,1);
\path [fill=red!5] (0,1) -- (1,1) -- (1,2) -- (0,2);
\path [fill=red!5] (2,1) -- (3,1) -- (3,2) -- (2,2);
\draw[color = gray] (0,0) grid[xstep = 1cm, ystep = 1cm] (3,3); % grid of 3 times 3
\node at (.5, 2.5) {1};
\node at (1.5, 2.5) {2};
\node at (2.5, 2.5) {3};
\node at (.5, 1.5) {4};
\node [color =  red] at (1.5,1.5) {5} ;
\node at (2.5,1.5) {6};
\node at (.5, .5) {7};
\node at (1.5,.5) {8};
\node at (2.5,.5) {9};
\end{tikzpicture}
\end{figure}


%=========================================
\subsection{Weights Based on Distance}\index{Weight matrix!Based on distance}
%=========================================

Weights can also be defined as a function of the distance between regions $i$ and $j$, denoted as $d_{ij}$. Typically, this distance is computed as the separation between centroids, although other significant points, such as capital cities or major urban centers, could also be used. Notably, unlike contiguity-based weights, distance-based matrices only require the coordinates of the relevant points.

Several methods exist to compute the distance between two spatial units. Let $x_i$ and $x_j$ represent the longitudes, and $y_i$ and $y_j$ denote the latitudes of regions $i$ and $j$. The Minkowski metric provides a general formula for distance:
\begin{equation*}
  d_{ij}^p = \left(\left|x_i - x_j\right|^p + \left|y_i - y_j\right|^p\right),
\end{equation*}
%
where $p$ is a parameter that allows flexibility in distance computation. A commonly used variant is the Euclidean distance, which corresponds to $p = 2$:
\begin{equation*}
  d_{ij}^e = \sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}.
\end{equation*}

Another popular alternative is the Manhattan (or block) distance, which considers movement along east-west and north-south directions, i.e., along straight angles. This metric corresponds to $p = 1$:
\begin{equation*}
   d_{ij}^m = \left|x_i - x_j\right| + \left|y_i - y_j\right|.
\end{equation*}

While these metrics are suitable for treating the Earth as a plane, they may lack accuracy over larger distances due to the Earth's curvature. The Euclidean distance, for instance, represents the straight-line length on a map but might not align with the shortest path on the Earth's surface. For more accurate analyses, particularly for long-distance travel, the great circle distance is often preferred. This method considers the Earth's curvature and calculates the shortest path between two points on a sphere. The great circle distance is given by:
\begin{equation*}
d_{ij}^{cd} = r \times \arccos^{-1}\left[\cos|x_i - x_j| \cos y_i \cos y_j + \sin y_i \sin y_j \right],
\end{equation*}
%
where $r$ is the Earth's radius. The arc distance is obtained in miles with $r = 3959$ and in kilometers with $r = 6371$.

%=================================
\subsubsection{Inverse Distance}\label{sec:inverse_distance}
%=================================

Now, we must translate information about distances among spatial points into a weight scheme. The objective is to ensure that $w_{ij} \to 0$ as $d_{ij} \to \infty$. In simpler terms, as point $j$ gets farther from point $i$, the spatial weight $w_{ij}$ should decrease, aligning with Tobler's first law.

In the inverse distance weighting scheme, the weights are inversely proportional to the separation distance, as expressed by the following formula:
\begin{equation*}
  w_{ij} =
  \begin{cases}
  \frac{1}{d_{ij}^{\alpha}} & \mbox{if} \;\;i \neq j \\
  0 & \mbox{if}\;\; i = j,
  \end{cases}
\end{equation*}
%
where the exponent $\alpha$ is a parameter that is usually set by the researcher. In practice, the parameters are seldom estimated, but typically set to $\alpha = 1$ or $\alpha = 2$. Consequently, the weights become the reciprocal of the distance: the greater the distance between spatial units, the smaller the spatial weight or connection. Conventionally, diagonal elements of spatial weights are set to zero to avoid division by zero in the case of inverse distance weights. Plugging in a value of $d_{ii} = 0$ would yield division by zero for inverse distance weights. 

\subsubsection{Negative Exponential Model}

In the negative exponential model, weights decrease exponentially with separation distance:
\begin{equation*}
  w_{ij} = \exp\left(-\frac{d_{ij}}{\alpha}\right),
\end{equation*}
%
where $\alpha$ is a parameter that is commonly chosen by researcher. Since the weights are given by the exponential of the negative distance, the greater the distance between $i$ and $j$, the lower $w_{ij}$.

Both the inverse distance and negative exponential distance models rely on the parameter value, functional form, and the chosen distance metric. As the weights are inversely related to distance, larger distances yield smaller weights, and vice versa. A potential challenge arises when distances are so vast that inverse distance weights approach zero, possibly resulting in a spatial weight matrix with zero values. Additionally, issues may occur if the distance metric yields values less than one, which is typically undesirable \citep{anselin2014modern}. 

\subsubsection{$k$-nearest Neighbors}

An alternative approach to spatial weights that mitigates the issue of isolates involves selecting the $k$-nearest neighbors. Unlike the distance band, this relation is not symmetric. However, a challenge arises when ties occur—when multiple locations $j$ share the same distance from $i$. Several solutions exist to address ties, ranging from randomly selecting one of the $k$-th order neighbors to including all of them.

\subsubsection{Threshold Distance (Distance Band Weights)}

In contrast to the $k$-nearest neighbors method, the threshold distance specifies that a region $i$ is neighbor of $j$ if the distance between them is less than a specified maximum distance:
\begin{equation*}
  w_{ij}= 
   \begin{cases}
      1 & \mbox{if}\;\; 0\leq d_{ij} \leq d_{max} \\ 
      0 & \mbox{if}\;\; d_{ij} > d_{max}.
   \end{cases}
\end{equation*}


To prevent isolates resulting from an excessively stringent critical distance, the distance must be chosen so that each location has at least one neighbor. Such a distance conforms to a max-min criterion, i.e., it is the largest of the nearest neighbor distances.

Importantly, a weights matrix derived from a distance band is always symmetric, as distance is inherently a symmetric relation.

%============================================
\subsection{Row-Standardized Weights Matrix}\index{Weight matrix!Row-standardization}
%============================================

In practical applications, spatial weights are rarely used in their raw binary (or distance) form. Instead, they are often transformed or standardized to improve interpretability and comparability. One common approach is to compute weighted averages, placing greater emphasis on nearby observations than on distant ones. To achieve this, a row-standardized weight matrix $\mW^s$ is defined, with elements $w_{ij}^s$ given by:
\begin{equation*}
w_{ij}^s = \frac{w_{ij}}{\sum_j w_{ij}}.
\end{equation*}

This formulation ensures that all weights lie within the range of 0 to 1, facilitating the interpretation of matrix operations as an averaging of neighboring values. Moreover, row-standardization enhances the comparability of spatial parameters across models in various spatial stochastic processes \citep{AnselinBera1998}.

An additional feature of row-standardized matrices is that the sum of the weights in each row equals unity, and the total sum of all weights, $S_0 = \sum_i\sum_j w_{ij}$, equals the number of observations, $n$. This property simplifies the interpretation and application of the weights, as we will explore further in subsequent sections.

However, an important consequence of row-standardization is the loss of symmetry in the weights matrix. Symmetric matrices are characterized by real eigenvalues, but row-standardized matrices generally do not retain symmetry. Despite this, row-standardized matrices remain widely used due to their practical advantages.

Row-standardized matrices are also referred to as row-stochastic or Markov matrices. Formally, a row-stochastic matrix is defined as follows:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{definition}[Row-stochastic Matrix]
	A real $n\times n$ matrix $\mA$ is called \textbf{Markov} matrix or \textbf{row-stochastic matrix} if 
		\begin{enumerate}
			\item $a_{ij} \geq 0$ for $1\leq i, j \leq n$;
			\item $\sum_{j=1}^n a_{ij} = 1$ for $1\leq i \leq n$
		\end{enumerate}
\end{definition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

An important property of row-stochastic matrices concerns their eigenvalues:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{theorem}[Eigenvalues of row-stochastic Matrix]\label{teo:eigen_values}
	Every eigenvalue $\omega_i$ of a row-stochastic Matrix satisfies $\left|\vomega\right|\leq 1$
\end{theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Therefore, the eigenvalues of the row-stochastic (i.e., row-normalized, row standardized or Markov) neighborhood matrix $\mW^s=(w_{ij}^s)$ are in the range $\left[-1, +1\right]$.

Finally, the behavior of $\mW^s$ is important for asymptotic properties of estimators and test statistics \citep[][pp. 244]{AnselinBera1998}. In particular, the $\mW$ matrix should be also exogenous, unless endogeneity is considered explicitly in the model specification. 

%============================================
\subsection{Spatial Lagged Variables}\label{sec:spatial_lag_var}\index{Weight matrix!Spatial lag}
%============================================

Having explored the spatial weight matrix, we can now introduce the concept of  \textbf{spatially lagged variables} or \textbf{spatial lag operator}. The spatial lag operator takes the form $\vy_L = \mW\vy$ with dimension $n \times 1$, where each element is given by $\vy_{Li} = \sum_{j}w_{ij}y_j$, i.e., a weighted average of the $\vy$ values in the neighbor of $i$.

For example:
\begin{equation*}
  \mW\vy =    \begin{pmatrix}
     0 & 1 & 0 \\
     1 & 0 & 1 \\
     0 & 1 & 0
  \end{pmatrix}
  \begin{pmatrix}
     10 \\
     50 \\
     30
  \end{pmatrix} =
  \begin{pmatrix}
     50 \\
     10 + 30 \\
     50
  \end{pmatrix}.
\end{equation*}

Using a row-standardized weight matrix:
\begin{equation*}
  \mW^s\vy =    \begin{pmatrix}
     0 & 1 & 0 \\
     0.5 & 0 & 0.5 \\
     0 & 1 & 0
  \end{pmatrix}
  \begin{pmatrix}
     10 \\
     50 \\
     30
  \end{pmatrix} =
  \begin{pmatrix}
     50 \\
     5 + 15 \\
     50
  \end{pmatrix}.
\end{equation*}

Consequently, for spatial unit $i$, the spatial lag of $y_i$, denoted a $\vy_{Li}$ (the variable $\mW\vy$ observed for location $i$) is:
\begin{equation*}
  \vy_{Li} = w_{i, 1^s}y_i + w_{i, 2}^sy_2 + \cdots + w_{i, n}^sy_n, 
\end{equation*}
%
or equivalently,
\begin{equation*}
  \vy_{Li} = \sum_{j = 1}^nw_{i,j}^sy_j,
\end{equation*}
%
where the weights $w_{ij}$ are the elements of the $i$th row of the matrix $\mW^s$ matched with the corresponding elements of the vector $\vy$. In other words, this represents a weighted sum of values observed at neighboring locations, excluding non-neighbors.

\begin{remark}
As stated by \citet[][p. 23-24]{anselin1988spatial}, standardization must be done with caution.\footnote{See also \citet[][p. 12]{elhorst2014spatial} and references therein.} For example, when the weights are based on an inverse distance function (or similar concept of distance decay), which has a meaningful economic interpretation, scaling the rows so that the weights sum to one may result in a loss of that interpretation. Can you give an example?
\end{remark}


%============================================
\subsection{Higher Order Spatial Weights}\label{sec:HSO}\index{Weight matrix!Higher order}
%============================================

To expand our understanding of geographical space as defined by the matrix $\mW$, we turn to the concept of higher-order neighbors. These are neighbors that are not directly adjacent but are reachable through other spatial units. For instance, we might consider the neighbors of a spatial unit's neighbors or even their neighbors' neighbors. To formalize this, we introduce the notion of \textbf{higher-order spatial weight matrices}.

The higher-order spatial weight matrix of order $l$, denoted as $\mW^l$, is defined as:
\begin{equation*}
  \mW^l = \underbrace{\mW \cdot \mW \cdot \dots \cdot \mW}_{l \text{ times}}.
\end{equation*}

For example:
\begin{itemize}
\item The second-order spatial weight matrix is $\mW^2 = \mW \cdot \mW$.
\item The third-order spatial weight matrix is $\mW^3 = \mW \cdot \mW \cdot \mW$, and so on.
\end{itemize}

The element $w_{ij}$ in these higher-order matrices represents whether spatial unit $j$ is a neighbor of order $l$ to spatial unit $i$. Specifically:
\begin{itemize}
\item For $l = 2$, $w_{ij}$ equals 1 if $j$ is adjacent to a first-order neighbor of $i$, and 0 otherwise.
\item For $l = n$, $w_{ij}$ equals 1 if $j$ is adjacent to the $(n-1)$-order neighbors of $i$, and 0 otherwise.
\end{itemize}

To illustrate these points, consider the following spatial structure from example in Section \ref{sec:why_se}:
\begin{equation}\label{eq:W5x5}
\mW = \begin{pmatrix}
      0 & 1 & 0 & 0 & 0 \\
      1 & 0 & 1 & 0 & 0 \\
      0 & 1 & 0 & 1 & 0 \\
      0 & 0 & 1 & 0 & 1 \\
      0 & 0 & 0 & 1 & 0
      \end{pmatrix}.
\end{equation}

Then, $\mW^2 = \mW\mW$ based on the $5\times 5$ first-order contiguity matrix $\mW$ from \eqref{eq:W5x5} is:
\begin{equation}
\mW^2 = \begin{pmatrix}\label{eq:W25x5}
      1 & 0 & 1 & 0 & 0 \\
      0 & 2 & 0 & 1 & 0 \\
      1 & 0 & 2 & 0 & 1 \\
      0 & 1 & 0 & 2 & 0 \\
      0 & 0 & 1 & 0 & 1
      \end{pmatrix}
\end{equation}

For region $R1$, the second-order neighbors are $R1$ and $R3$. This indicates that $R1$ is its own second-order neighbor (via feedback) and also shares second-order adjacency with $R3$, which is a first-order neighbor of $R2$. 

Now consider R2. The first panel of Figure \ref{fig:example_hon} shows the first-order neighbors of $R2$ based on the spatial weight matrix in \eqref{eq:W5x5}: $R1$ and $R3$. Panel B considers the second-order neighbors of $R2$: $R2$ itself and $R4$. To understand this, note that there is a feedback effect from the first impact from $R2$ coming from $R1$ and $R3$ (first-order neighbors of $R2$). This explains why the element $w^2_{22} = 2$. Additionally, there is an indirect effect coming from $R4$ through $R3$ that finally impacts $R2$, yielding a value of 1 for the element $w^2_{24}$.

Similarly, for region $R3$, the second-order neighbors are regions $R1$ (which is a neighbor to the neighboring region $R2$), $R3$ (a second-order neighbor to itself), and $R5$ (which is a neighbor to the neighboring region $R4$). 

\begin{figure}[ht]
\caption{Higher-Order Neighbors}
\label{fig:example_hon}
\centering
\begin{tikzpicture}[scale = 1.5]
\draw[color = gray] (0,0) grid[xstep = 1cm, ystep = 1cm] (5,1);
\node (R1) at (.5, .5) {R1};
\node (R2) at (1.5,.5) {R2};
\node (R3) [color =  red]  at (2.5,.5) {R3};
\node (R4) at (3.5,.5) {R4};
\node (R5) at (4.5,.5) {R5};
\draw [->] (R2) -- (R1);
\draw [->] (R2) -- (R3);
\end{tikzpicture}
\\
\vspace{1cm}
\begin{tikzpicture}[scale = 1.5]
\draw[color = gray] (0,0) grid[xstep = 1cm, ystep = 1cm] (5,1);
\node (R1) at (.5, .5) {R1};
\node (R2) at (1.5,.5) {R2};
\node (R3) [color =  red]  at (2.5,.5) {R3};
\node (R4) at (3.5,.5) {R4};
\node (R5) at (4.5,.5) {R5};
\draw [->] (R1) -- (R2);
\draw [->] (R3) -- (R2);
\draw [->, dashed] (R4) to[out= 270,in=270] (R3) to[out=270,in=270] (R2);
\end{tikzpicture}
\end{figure}

Similarly, the third-order neighbors are: 
\begin{equation*}
\mW^3 = \begin{pmatrix}
      0 & 2 & 0 & 1 & 0 \\
      2 & 0 & 3 & 0 & 1 \\
      0 & 3 & 0 & 3 & 0 \\
      1 & 0 & 3 & 0 & 2 \\
      0 & 1 & 0 & 2 & 1
      \end{pmatrix}
\end{equation*}

Could you explain the elements of this matrix?

%============================================
\section{Examples of Weight Matrices in R}
%============================================

Creating spatial weight matrices manually is a tedious and error-prone process, especially for large datasets. Fortunately, modern statistical software provides robust tools to simplify this task. To start, we typically need a \textbf{shapefile}, a widely used format for storing geographical information.

The shapefile format is a digital vector storage format that supports geometric data types such as points, lines, and polygons, along with associated attributes. It enables diverse representations of geographic data by combining shapes with their corresponding attribute data. A complete shapefile consists of three mandatory files with the following extensions:
\begin{itemize}
  \item \texttt{.shp}: The shape file containing the feature geometry.
  \item \texttt{.shx}: The shape index file, which facilitates fast positional indexing of the feature geometry.
  \item \texttt{.dbf}: The attribute file, formatted in \texttt{dBase} IV format, storing columnar attributes for each shape.
\end{itemize}

These three components must be present for a shapefile to be functional. The \texttt{.shp} file stores the actual geometric data, while the \texttt{.shx} and \texttt{.dbf} files provide supporting information to enable efficient access and attribute storage.

In this example, we demonstrate how to create spatial weight matrices using \proglang{R}. Specifically, we focus on a map of the communes in the Metropolitan Region of Chile. To begin, we load the shapefile into \proglang{R} using the \pkg{sf} package \citep{pabesmasf}, which is designed for handling spatial data.

<<load-sf>>=
#Load package sf
library("sf")
@

If the shapefile \texttt{mr\_chile.shp} is located in the working directory, we can load it using the \texttt{read\_sf} function:
<<read-shape-mr, warning=FALSE>>=
# Read shape file
mr <- read_sf("mr_chile.shp")
class(mr)
@

The \code{read\_sf} function reads data from the shapefile into an object of class ``\code{sf}''. The \code{names} function provides the name of the variables in the \texttt{.dbf} file associated with the shape file. 

<<names>>=
# Names of the variables in .dbf
names(mr)
@

Now, let's visualize the shapefile using the \code{plot} function:

<<plot_mr_false, eval =  FALSE>>=
# Plot shapefile
plot(st_geometry(mr), main = "Metropolitan Region-Chile", axes = TRUE)
@

The metropolitan region with its 52 communes is shown in Figure \ref{fig:plot_mr}.

\begin{figure}[h]
  \caption{Plotting a Map in R}
    \label{fig:plot_mr}
        <<plot_mr, echo = FALSE, fig.align='center', out.width = '9cm', out.height = '9cm'>>=
plot(st_geometry(mr), main = "Metropolitan Region-Chile", axes = TRUE)
@
\end{figure}

%===============================================
\subsection{Creating Contiguity Neighbors}\label{sec:computing-W-in-R}
%===============================================

To construct spatial weight matrices, the \pkg{spdep} package \citep{bivand2013computing} provides an extensive suite of tools. After installation, the package can be loaded as follows:

<<message = FALSE, warning = FALSE>>=
#Load package
library("spdep")
@

In \pkg{spdep}, neighbor relationships between $n$ observations are represented by objects of class \texttt{nb}. These objects are lists of length $n$, where each element is an integer vector containing the indices of neighboring regions. If a region has no neighbors, the corresponding element is assigned an integer value of zero.

The \texttt{poly2nb} function constructs neighbor relationships based on \textbf{contiguity}, generating a neighbors list of class \texttt{nb}. This function supports both the Queen and Rook criteria for defining spatial relationships. Refer to \texttt{help(poly2nb)} for detailed documentation.


As explained in previous section, the Queen criterion defines neighbors as regions that share at least one vertex or edge. The following example demonstrates how to construct a neighbor list for the communes in the Metropolitan Region of Chile:\index{Weight matrix!poly2nb function}

<<queen, message = FALSE>>=
# Create queen W
sf_use_s2(FALSE)
queen.w <- poly2nb(as(mr, "Spatial"), queen =  TRUE, row.names = mr$NAME)
@

Once the \texttt{nb} object is created, it can be explored using standard methods such as \texttt{print}, \texttt{summary}, and \texttt{plot}. For example, the summary of the Queen-based neighbors list provides information about the spatial relationships:
<<sum-queen>>=
# Summary of W
summary(queen.w)
@

The output provides crucial information about the neighbors, including the number of regions (52 communes in this example), the number of nonzero links, the percentage of nonzero weights, the average number of links, and more.

For instance, the commune of San Bernardo stands out as the most connected region with 12 neighbors under the queen scheme. Conversely, the least connected regions are Tiltil, San Pedro, and Maria Pinto, each with only 2 neighbors. The output also shows the distribution of neighbors, revealing that 7 out of 52 regions have 4 neighbors, and only 2 communes have 8 neighbors.

To visualize the region with the largest number of neighbors (San Bernardo) and its immediate neighbors, use the following code:
<<nneight-false, eval = FALSE>>=
# Plot communes with largest number of contiguities
cards <- card(queen.w)
maxconts <- which(cards == max(cards))
fg <- rep("grey", length(cards))
fg[maxconts] <- "red"
fg[queen.w[[maxconts]]] <- "blue"
plot(st_geometry(mr), col = fg)
@

\begin{figure}[ht]
  \caption{Commune with largest number of contiguities}
    \label{fig:plot_nn}
    \centering
        	\begin{minipage}{1\linewidth}
<<neight, echo = FALSE, fig.align='center', out.width = '10cm', out.height = '10cm'>>=
# Plot communes with largest number of contiguities
cards <- card(queen.w)
maxconts <- which(cards == max(cards))
fg <- rep("grey", length(cards))
fg[maxconts] <- "red"
fg[queen.w[[maxconts]]] <- "blue"
plot(st_geometry(mr), col = fg)
@
\footnotesize
		\emph{Notes:} The commune in red is the spatial unit (San Bernardo) with the largest number of neighbors based on the queen criteria, whereas the communes in blue are its neighbors. 
	\end{minipage}	
\end{figure}

Figure \ref{fig:plot_nn} provides a visual representation where the red-colored commune, San Bernardo, stands out as the spatial unit with the largest number of neighbors according to the queen criteria. Meanwhile, the communes in blue represent its neighboring units.

To transform the \code{list} into an actual matrix $\mW$, we can use the \code{nb2listw} function\index{Weight matrix!nb2listw function}:

<<queen-list>>=
# From list to matrix
queen.wl <- nb2listw(queen.w, style = "W")
summary(queen.wl)
@

An important argument for \code{nb2listw} is \code{style}. This argument indicates what type of matrix to create. For example, \code{style = "W"} creates a row-standardize matrix so that $w^s_{ij} = w_{ij}/ \sum_j w_{ij}$. After normalization, each row of $\mW^s$ sums to 1. Other options include \code{"B"} for basic binary coding; \code{"C"} for global standarization, that is, $w^s_{ij} = w_{ij} \cdot (n/ \sum_{i}\sum_j w_{ij})$. If \code{style = "U"}, then $w^s_{ij} = w_{ij}/ \sum_i\sum_j w_{ij}$. In a \code{minmax} matrix, the $(i,j)$th element of $\mW^s$  becomes $w^s_{ij} = w_{ij} / \min\left\lbrace \max_i(\tau_i), \max_i(c_i)\right\rbrace$, with $\max_i(\tau_i)$ being the largest row sum of $\mW$ and $\max_i(c_i)$ being the largest column sum of $\mW$ \citep{kelejian2010specification}. Finally, \code{"S"} is the variance-stabilizing coding scheme where $w^s_{ij} = w_{ij}/ \sqrt{\sum_j w_{ij} ^2}$ \citep{tiefelsdorf1999variance}. 

Additionally, the \code{summary} function provides several constants essential for global spatial autocorrelation statistics, which we will discuss later. 

We can also inspect the attributes of the object using the function \code{attributes}:

<<queen-list2>>=
# Attributes of wlist
attributes(queen.w)
@

Weight matrices based on contiguity are generally symmetric. Use the following command to verify:
<<queen-list3>>=
# Symmetric W
is.symmetric.nb(queen.w)
@

The Rook criterion considers neighbors that share a common edge. Here's how to construct a Rook-based neighbors list:
<<rook, message = FALSE>>=
# Rook W
rook.w <- poly2nb(as(mr, "Spatial"), row.names = mr$NAME, queen =  FALSE)
summary(rook.w)
@

Finally, we can visualize the spatial connectivity implied by the Queen and Rook criterion using the following set of commands (see Figure \ref{fig:Queen-Rook}). 

<<plot-queen-rookF, fig.align='center', out.width = '8cm', out.height = '8cm', eval =  FALSE>>=
# Plot Queen and Rook W Matrices
plot(st_geometry(mr), border = "grey")
coords <- st_coordinates(st_centroid(st_geometry(mr)))
plot(queen.w, coords, add =  TRUE, col = "red")
plot(rook.w, coords, add =  TRUE, col = "yellow")
@

\begin{figure}[ht]
  \caption{Queen and Rook Criteria for MR}
    \label{fig:Queen-Rook}
<<plot-queen-rookT, fig.align='center', out.width = '8cm', out.height = '8cm', echo =FALSE, message = FALSE, warning= FALSE>>=
# Plot Queen and Rook W Matrices
plot(st_geometry(mr), border = "grey")
coords <- st_coordinates(st_centroid(st_geometry(mr)))
plot(queen.w, coords, add =  TRUE, col = "red")
plot(rook.w, coords, add =  TRUE, col = "yellow")
@
\end{figure}

%==================================================
\subsection{Creating Distance-based Neighbors}
%==================================================

We proceed to build spatial weight matrices using the $k$-nearest neighbors criteria\index{Weight matrix!knearneigh function}. 

<<knbs1>>=
# K-neighbors
head(coords, 5)                                       # show coordinates
k1neigh <- knearneigh(coords, k = 1, longlat = TRUE)  # 1-nearest neighbor
k2neigh <- knearneigh(coords, k = 2, longlat = TRUE)  # 2-nearest neighbor
@

Here, the \code{coords} function extracts spatial coordinates from the shapefile, while \code{knearneigh} returns a matrix containing indices of points belonging to the set of $k$-nearest neighbors for each observation. The \code{k} argument specifies the number of nearest neighbors to return. If the point coordinates are given in longitude-latitude decimal degrees, distances are measured in kilometers when \code{longlat = TRUE}. For \code{longlat = FALSE}, great-circle distances are computed. The resulting objects \code{k1neigh} and \code{k2neigh} are of class \code{knn}.

Inverse distance weight matrices can be computed as follows (see Section \ref{sec:inverse_distance}):

<<inv-dist-mat>>=
# Inverse weight matrix
dist.mat <- as.matrix(dist(coords, method = "euclidean"))
dist.mat[1:5, 1:5]
dist.mat.inv <- 1 / dist.mat # 1 / d_{ij}
diag(dist.mat.inv) <- 0      # 0 in the diagonal
dist.mat.inv[1:5, 1:5]
# Standardized inverse weight matrix
dist.mat.inve <- mat2listw(dist.mat.inv, style = "W", row.names = mr$NAME)
summary(dist.mat.inve)
@

The \code{dist} function from the \pkg{stats} package computes the distance matrix using the specified metric---Euclidean distance in this example. Other available methods include \code{maximum}, \code{manhattan}, \code{canberra}, \code{binary}, and \code{minkowski}. The \code{mat2listw} function converts a square spatial weight matrix into a list format suitable for spatial analysis. For additional details on spatial weight matrices, see \cite{stewart2010choosing}.

The following code demonstrates how to plot different weight matrices:

<<plot-all-wsF, eval= FALSE>>=
# Plot Weights
par(mfrow = c(3, 2))
plot(st_geometry(mr), border = "grey", main = "Queen")
plot(queen.w, coords, add =  TRUE, col = "red")
plot(st_geometry(mr), border = "grey", main = "1-Neigh")
plot(knn2nb(k1neigh), coords, add = TRUE, col = "red")
plot(st_geometry(mr), border = "grey", main = "2-Neigh")
plot(knn2nb(k2neigh), coords, add = TRUE, col = "red")
plot(st_geometry(mr), border = "grey", main = "Inverse Distance")
plot(dist.mat.inve, coords, add =  TRUE, col = "red")
@


\begin{figure}[h!]
  \caption{Different Spatial Weight Schemes for MR}
    \label{fig:more_ws}
<<plot-all-wsT, echo =FALSE, message = FALSE, fig.align='center', fig.height = 7, fig.width = 7, fig.show='hold'>>=
# Plot Weights
par(mfrow = c(3, 2))
plot(st_geometry(mr), border = "grey", main = "Queen")
plot(queen.w, coords, add =  TRUE, col = "red")
plot(st_geometry(mr), border = "grey", main = "1-Neigh")
plot(knn2nb(k1neigh), coords, add = TRUE, col = "red")
plot(st_geometry(mr), border = "grey", main = "2-Neigh")
plot(knn2nb(k2neigh), coords, add = TRUE, col = "red")
plot(st_geometry(mr), border = "grey", main = "Inverse Distance")
plot(dist.mat.inve, coords, add =  TRUE, col = "red")
@
\end{figure}

\subsection{Constructing a Spatially Lagged Variable}

Spatially lagged variables play a crucial role in various spatial tests and regression specifications. In the \pkg{spdep} package, these variables are crafted using the  \code{lag.listw} function.

Let's begin by combining the variables  \code{POVERTY} and \code{URB\_POP} into a matrix and check the contents with \code{head}:

<<Xmatrix>>=
# X matrix
X <- cbind(mr$POVERTY, mr$URB_POP)
head(X, 5)
@

Now, we can generate a spatially lagged version of this matrix, using the \code{queen.w} weights\index{Weight matrix!lag.listw function}:

<<WX-matrix>>=
# Create WX
WX <- lag.listw(nb2listw(queen.w), X)
head(WX)
@


%****************************************************
\section{Testing for Spatial Autocorrelation}
%*****************************************************

As discussed in Section \ref{sec:Spatial_autocorrelation}, spatial autocorrelation refers to the relationship between a variable and itself across spatial locations. Positive spatial autocorrelation occurs when high (or low) values cluster together, while negative spatial autocorrelation reflects spatial outliers, where high values align with low neighboring values or vice versa.

A critical question arises: does the observed spatial pattern genuinely reflect a spatially autocorrelated process, or is it merely due to random chance? To answer this, formal tests of spatial autocorrelation are required to evaluate whether the value of a variable at one location is independent of the values at neighboring locations.


%=============================================================
\subsection{Global Spatial Autocorrelation: Moran's I}\label{sec:moransI}
%============================================================

Global spatial autocorrelation measures the overall clustering tendency within a dataset. These indices assess the degree to which similar observations tend to occur near one another. They do so by calculating the similarity between values at distinct locations $i$ and $j$, weighted by the proximity of these locations. High similarity and proximity indicate clustering of similar values, whereas dissimilar values in close proximity suggest dispersion.

The most widely used measure of global spatial autocorrelation is Moran's I.\footnote{Other measures, such as Geary's $C$, also exist, but this discussion focuses on Moran's I.} This statistic quantifies overall clustering and tests the null hypothesis of random spatial distribution. Rejection of this null hypothesis indicates a spatial pattern or structure.


Moran's I is given by\index{Moran's I test}:
\begin{equation}\label{eq:I-moran}
I = \frac{\sum_{i = 1}^n\sum_{j=1, j\neq i}^n w_{ij}\left(x_i - \bar{x}\right)\left(x_j - \bar{x}\right)}{S_0 \sum_{i = 1}^n\left(x_i - \bar{x}\right)^2/n} = \frac{n\sum_{i = 1}^n\sum_{j=1}^n w_{ij}\left(x_i - \bar{x}\right)\left(x_j - \bar{x}\right)}{S_0 \sum_{i = 1}^n\left(x_i - \bar{x}\right)^2},
\end{equation}
%
where $S_0=\sum_{i = 1}^n\sum_{j=1}^nw_{ij}$ and $w_{ij}$ is an element of the spatial weight matrix that measures spatial distance or connectivity between regions $i$ and $j$.
In matrix form:
\begin{equation*}
	I = \frac{n}{S_0} \frac{\vz^\top\mW\vz}{\vz^\top\vz},
\end{equation*}
%
where: 
\begin{equation*}
\vz = \begin{pmatrix}
          x_1 - \bar{x}\\
          x_2 - \bar{x} \\
          \vdots\\
          x_n - \bar{x}
      \end{pmatrix},
\end{equation*}

If the $\mW$ matrix is row standardized, then:
\begin{equation*}
	I = \frac{\vz^\top\mW^s\vz}{\vz^\top\vz},
\end{equation*}
%
because $S_0=n$.  The values of Moran's I range from -1 (indicating perfect dispersion) to +1 (indicating perfect correlation), while a zero value suggests a random spatial pattern.


A very useful tool for understanding the Moran's I test is the Moran Scatterplot. The idea of the Moran scatterplot is to display the variable for each region (on the horizontal axis) against the standardized spatial weighted average (average of the neighbors' $x$, also known as spatial lag) on the vertical axis (See Figure \ref{fig:moran_scatterplot}). As pointed out by \cite{anselin1996chapter}, expressing  variables in standardized from (i.e. with mean zero and standard deviation equal to one) enables the assessment of both the global spatial association, where the slope of the line represents the Moran's I coefficient, and local spatial association (identifiable by the quadrant in the scatterplot). 

The Moran Scatterplot is divided into four distinct quadrants, each corresponding to a type of local spatial association between a region and its neighbors\index{Moran's I test!Moran scatterplot}:

\begin{itemize}
  \item Quadrant I displays the region with high $x$ (above the average) surrounded by regions with high $x$ (above the average). This quadrant is usually  denoted High-High.
  \item Quadrant II show the regions with low value surrounded by region with high values. This quadrant is usually denoted Low-High.
  \item Quadrant III display the regions with low value surrounded by regions with low values, and is denoted Low-Low.
  \item Quadrant IV shows the regions with high value surrounded by regions with low values. It is noted High-Low.
\end{itemize}

Regions located in quadrant I and III refer to positive spatial autocorrelation, the spatial clustering of similar values, whereas quadrant II and IV represent negative spatial autocorrelation, the spatial clustering of dissimilar values. 

\begin{figure}[ht]
  \caption{Moran Scatterplot}\label{fig:moran_scatterplot}
<<moran, echo =FALSE, message = FALSE, fig.align='center', fig.height = 5, fig.width = 4, fig.show='hold'>>=
set.seed(1)
library("spdep")
rook5x10 <- cell2nb(5, 10)
y <- rnorm(5 * 10)
Iw <- invIrM(rook5x10, 0.9)
yc <- c(Iw %*% y)
moran.plot(yc, nb2listw(rook5x10), pch = 19,
           xlab = expression(x),
           ylab = expression(Wx),
           col = "red")
text(c(2,-1, -1, 2), c(1.5, 1.5, 0.5, -0.5),  labels = c("Quadrant I", "Quadrant II",
                                                "Quadrant III", "Quadrant IV"),
     col = "blue", cex = 0.5)
@
\end{figure}


To grasp the essence of Moran's I, it is essential to draw parallels with the Ordinary Least Squares (OLS) coefficient. Recall the OLS coefficient formula:
\begin{equation*}
  \widehat{\beta}= \frac{\sum_{i = 1}^n\left(x_i - \bar{x}\right)(y_i - \bar{y})}{\sum_{i = 1}^n \left(x_i - \bar{x}\right)^2}
\end{equation*}

Now, examining (\ref{eq:I-moran}), Moran's I equates to the slope coefficient of a linear regression, where the spatial lag $\mW\vx$ is regressed on the observation vector $\vx$, both measured in deviation from their means. It's worth noting that Moran's I is not equivalent to the slope of $\vx$ on $\mW\vx$, which might seem more intuitive.

The hypothesis tested by the Moran's $I$ is the following:
\begin{itemize}
  \item $H_0$: $\vx$ is spatially independent; the observed $\vx$ is assigned at random among locations. In this case, $I$ is close to zero.
  \item $H_1$: $\vx$ is not spatially independent. In this case $I$ is statistically different from zero. 
\end{itemize}

Now, considering the distribution of Moran's I, our interest lies in the distribution of:
\begin{equation*}
  \frac{I - \E\left[I\right]}{\sqrt{\var(I)}}
\end{equation*}

There are two methods to compute the mean and variance of Moran's I. The first assumes a normal distribution for $x_i$, while the second involves randomization of $x_i$. Under the normal assumption, it is assumed that the random variable $x_i$ results from $n$ independent draws from a normal population. Conversely, under the randomization assumption, irrespective of the underlying distribution of populations, observed values of $x_i$ are repeatedly and randomly permuted.

\subsubsection{Moments Under Normality Assumption}

Theorem \ref{teo:Moran_normal} gives the moments of Moran's I under normality. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{theorem}[Moran's $I$ Under Normality]\label{teo:Moran_normal}\index{Moran's I test!Normality}
Assume that $\left\lbrace \vx_i\right\rbrace = \left\lbrace x_1, x_2,\ldots, x_n\right\rbrace$ are independent and distributed as $\rN(\mu, \sigma^2)$, but $\mu$ and $\sigma^2$ are unknown. Then:
\begin{equation*}
\E\left(I\right) = - \frac{1}{n - 1}, 
\end{equation*}
%
and
\begin{equation*}
\E\left(I^2\right) = \frac{n^2S_1 - nS_2 + 3S_0^2}{S_0^2(n^2 - 1)},
\end{equation*}
%
where $S_0=\sum_{i = 1}^n\sum_{j=1}^nw_{ij}$, $S_1= \sum_{i = 1}^n\sum_{j = 1}^n(w_{ij} + w_{ji})^2/2$, $S_2 = \sum_{i = 1}^n(w_{i.} + w_{.i})^2$, where $w_{i.}= \sum_{j = 1}^nw_{ij}$ and $w_{i.}=\sum_{j = 1}^nw_{ji}$
Then:
\begin{equation*}
\var\left(I\right)=\E\left(I^2\right) - \E\left(I\right)^2.
\end{equation*}
\end{theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% %----------------------------------------
% \begin{proof}
% Let $z_i=x_i - \bar{x}$. Under normality of $x_i$, the following moments are true for $z_i$:
% \begin{eqnarray*}
% \E\left[z_i\right]   & = & 0, \\
% \E\left[z_i^2\right] & = & \sigma^2 - \frac{\sigma^2}{n}, \\
% \E\left[z_iz_j\right] & = & - \frac{\sigma^2}{n} \\
% \E\left[z_i^2 z_j^2\right] & = & \frac{(n^2 - 2n + 3)\sigma^2}{m^2}, \\
% \E\left[z_i^2 z_jz_k\right] & = & - \frac{(n -3)\sigma^4}{n}, \\
% \E\left[z_iz_jz_kz_l\right] & = & \frac{3 \sigma^4}{n^2}.
% \end{eqnarray*}
% 
% Then:
% \begin{equation}
%   \begin{aligned}
%     \E\left[I\right] & = \frac{n}{S_0}\frac{\E\left[\sum_{i = 1}^n\sum_{j=1}^nw_{ij}z_iz_j\right]}{\E\left[\sum_{i=1}^nz_i^2\right]}
%     & = \frac{n}{S_0}\sum_{i = 1}^n\sum_{j=1}^nw_{ij}\frac{\E\left[z_iz_j\right]}{\sum_{i=1}^n\E\left[z_i^2\right]} \\
%     & = \frac{-nS_0\frac{\sigma^2}{n}}{S_0 n (1 - 1/n)\sigma^2 } \\
%     & = -\frac{\frac{\sigma^2}{n}}{(1 - 1/n)\sigma^2 } \\
%     & = -\frac{1}{n - 1} 
%   \end{aligned}
% \end{equation}
% %
% and
% \begin{equation}
%   \begin{aligned}
%     \E\left[I^2\right] & = \E\left[\frac{n^2}{S_0^2}\frac{\left[\sum_{i = 1}^n\sum_{j=1}^nw_{ij}z_iz_j\right]^2}{\left[\sum_{i=1}^nz_i^2\right]^2}\right] \\
%     & = \frac{n^2}{S_0^2} \E\left[\frac{1/2\sum_{(2)}(w_{ij} + w_{ji})^2 z_i^2 z_j^2 + \sum_{(3)}(w_{ij} + w_{ji})(w_{ik} + w_{ki})z_i^2z_jz_k + \sum_{(4)} w_{ij}w_{kl}z_iz_jz_kz_l}{s}\right]
%   \end{aligned}
% \end{equation}
% \end{proof}
% %----------------------------------------

\subsubsection{Moran's $I$ under Randomization}

%Recall that when testing a null hypothesis we need a test statistic that will have different values under the null hypothesis and the alternatives. We then need to compute the sampling distribution of the test statistic when the null hypothesis is true. For some test statistics and some null hypotheses this can be done analytically. Then, the $p$-value is the probability that the test statistic would be at least as extreme as we observed, if the null hypothesis is true.

%A permutation test gives a simple way to compute the sampling distribution for any test statistic. 

%To estimate the sampling distribution of the test statistic we need many samples generated under the strong null hypothesis. A permutation test builds sampling distribution by resampling the observed data. Specifically, we can ``shuffle'' or permute the observed data by 


%A randomization test (also called a permutation test) is a type of statistical significance test in which the distribution of the test statistic under the null hypothesis is obtained by calculating all possible values of the test statistics under rearrangements of the labels on the observed points. 

%No matter what the underlying distribution of the population, we consider the observed values of $x_i$ were repeatedly randomly permuted.


%Under randomization 

%The testing method under randomization assumption is called permutation test. 

Theorem \ref{teo:Moran_random} gives the moments of Moran's I under randomization. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{theorem}[Moran's $I$ Under Randomization]\label{teo:Moran_random}\index{Moran's I test!Randomization}
Under permutation, we have:
\begin{equation*}
\E\left(I\right) = - \frac{1}{n - 1}, 
\end{equation*}
%
and
\begin{equation*}
\E\left(I^2\right) = \frac{n\left[\left(n^2 - 3n + 3\right)S_1 - nS_2 + 3S_0^2\right]-b_2\left[\left(n^2 - n\right)S_1 - 2nS_2 + 6S_0^2\right]}{(n-1)(n-2)(n-3)S_0^2},
\end{equation*}
%
where $S_0=\sum_{i = 1}^n\sum_{j=1}^nw_{ij}$, $S_1= \sum_{i = 1}^n\sum_{j = 1}^n(w_{ij} + w_{ji})^2/2$, $S_2 = \sum_{i = 1}^n(w_{i.} + w_{.i})^2$, where $w_{i.}= \sum_{j = 1}^nw_{ij}$ and $w_{i.}=\sum_{j = 1}^nw_{ji}$.Then:
\begin{equation*}
\var\left(I\right)=\E\left(I^2\right) - \E\left(I\right)^2
\end{equation*}
\end{theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

It is important to note that the expected value of Moran's $I$ under normality and randomization is the same. 

\subsubsection{Monte Carlo Moran's $I$}

The normality assumption is a very strong assumption. However we can use the Moran’s I test based on Monte Carlo simulation.

The essence of any Monte Carlo test, especially for Moran's I, involves the following steps\index{Moran's I test!Monte carlo}: 

\begin{itemize}
  \item Specify a test statistic $T$ for which large values indicate evidence against the null hypothesis $H_0$ (no spatial autocorrelation).
  \item  Given an observed value $t_{obs}$ of the test statistic, compute the p-value as $\Pr(T\geq t_{obs}|H_0)$. This involves understanding the distribution of $T$ under the assumption of $H_0$.
\end{itemize}

he algorithm for the Moran's I Monte Carlo test is as follows:

\begin{algorithm}[Moran's' I Monte Carlo Test]
The procedure is the following:

\begin{enumerate}
\item Rearrange the spatial data by shuffling their location and compute the Moran's I $S$ times. This will create the distribution under $H_0$. This operationalizes spatial randomness. 
\item Let $I_1^*, I_2^*,\ldots, I_S^*$ be the Moran's I for each time. A consistent Monte Carlo p-value is then:
  \begin{equation*}
    \widehat{p} = \frac{1 + \sum_{s=1}^S 1(I^*_s \geq I_{obs})}{S + 1}
  \end{equation*}
  \item For tests at the $\alpha$ level or at $100(1- \alpha)\%$ confidence intervals, there are reasons for choosing $S$ so that $\alpha(S + 1)$ is an integer. For example, use $S=999$ for confidence intervals and hypothesis tests when $\alpha = 0.05$.
\end{enumerate}
\end{algorithm}


%%=====================================================
%\subsection{Local Indicators of Spatial Association (LISA)}
%=====================================================

%\cite{anselin1995local} suggests that a local indicator of spatial association (LISA) is any statistic that satisfies the following two requirements:

%\begin{enumerate}
%  \item the LISA for each observation gives an indication of the extent of significant spatial clustering of similar values around that observation;
%  \item the sum of LISAs for all observations is proportional to a global indicator of spatial association.
%\end{enumerate}

%=====================================================
\section{Application: Poverty in Santiago, Chile}
%=====================================================

In this section we undertake and exploratory spatial data analysis (ESDA) for poverty in Metropolitan Region, Chile.  


%================================
\subsection{Cloropeth Graphs}
%================================

If we are interested in the geographical variation in poverty, we should start by plotting the spatial distribution of poverty.  This can be useful in a variety of way.  Typically, aggregate or national-level indicators tend to obscure crucial disparities among different spatial units. Consequently, utilizing poverty mapping becomes instrumental in emphasizing these geographical variations. Another notable advantage lies in the inherent legibility of poverty maps. Maps, as powerful visual tools, adeptly represent complex information in an easily understandable format.

So, we start by plotting the geographical variation of poverty among communes by using the \code{plot} function. In particular, we use a cloropleth\footnote{The name of this technique is derived from the Greek words choros - space, and pleth - value} map using the quantile classification. In a quantile graph, the variable is sorted and grouped in categories with equal number of observations, or quantiles. 


<<cloro-graphsfalse, eval = FALSE>>=
# Cloropleth graphs ----
library("RColorBrewer")
plot(mr["POVERTY"], 
       breaks = "quantile",
       nbreaks = 5, 
       pal = brewer.pal(5, "Blues"), 
       main = "", 
       axes = TRUE)
@

Figure \ref{fig:cloro-graph} provides some useful insights. First, it clearly shows that the spatial pattern of poverty in the MR is not spatial homogeneous, but rather the intensity of poverty varies across space. Secondly, it exemplifies how disaggregated poverty indicators can unveil additional information compared to aggregate indicators. For instance, it reveals that poverty intensity is lower in peripheral communes compared to central communes.

\begin{figure}
\caption{Cloropleth map: Poverty in the Metropolitan Region}\label{fig:cloro-graph}
<<cloro-graphs, fig.align='center', fig.height = 3, fig.width = 4, fig.show='hold', echo = FALSE>>=
library("RColorBrewer")
plot(mr["POVERTY"], 
       breaks = "quantile",
       nbreaks = 5, 
       pal = brewer.pal(5, "Blues"), 
       main = "", 
       axes = TRUE)
@
\end{figure}

How to interpret quantile maps? A quantile classification scheme is an ordinal ranking of the data values, dividing the distribution into intervals that have an equal number of data values. Quantile classification ensures maps are easily comparable and can be `easy to read'.

% We can also plot the data using the equal interval classification. Equal interval divides the data into equal size classes (e.g., 0-10, 10-20, 20-30, etc) and works best on data that is generally spread across the entire range. In the following example we use a defined interval classification. 
% 
% <<cloro-graphs2F, eval =  FALSE>>=
% # Define interval classification 
% mr$pov.cat <- cut(mr$POVERTY, 
%                   breaks = c(0, 9, 12, 16, 28),
%                   labels = c("< 9", "9-12", "12-16", ">16"))
% spplot(mr, "pov.cat", col.regions = brewer.pal(5, "Reds"))
% @
% 
% \begin{figure}[h]
% \caption{Cloropleth map: Poverty in the Metropolitan Region (Equal Interval)}\label{fig:cloro-graph2}
% <<cloro-graphs2T, fig.align='center', fig.height = 3, fig.width = 4, fig.show='hold', echo = FALSE>>=
% plot(mr["POVERTY"], 
%        breaks = "quantile",
%        nbreaks = 5, 
%        pal = brewer.pal(5, "Blues"), 
%        main = "", 
%        axes = TRUE)
% @
% \end{figure}


While the visual exploration of poverty's spatial distribution in Figure \ref{fig:cloro-graph} provides initial insights, it's crucial to acknowledge the sensitivity of results to factors such as the number of defined intervals. Therefore, a more rigorous and formal analysis is imperative to discern the potential presence of spatial dependence. The objective is to ascertain whether a statistically significant spatial autocorrelation pattern exists in the distribution of poverty.

To achieve this, we now employ the Moran’s I test.


%================================
\subsection{Moran's I Test}
%================================

First, we create two spatial weight matrices (queen and rook) to assess the robustness of the test under different spatial schemes. 

<<pov-W, message = FALSE>>=
# Generate W matrices
queen.w <- poly2nb(as(mr, "Spatial"), row.names = mr$NAME, queen =  TRUE)
rook.w  <- poly2nb(as(mr, "Spatial"), row.names = mr$NAME, queen =  FALSE)
@

Moran's I test statistic for spatial autocorrelation is implemented in \pkg{spdep} \citep{spdep}. There are mainly two function for computing this test: \code{moran.test}, where the inference is based on a normal or randomization assumption, and \code{moran.mc}, for a permutation-based test\index{Moran's I test!moran.test function}. 

<<moranI-normal>>=
# Moran's I test
moran.test(mr$POVERTY, listw = nb2listw(queen.w), randomisation = FALSE, 
           alternative = 'two.sided')
moran.test(mr$POVERTY, listw = nb2listw(rook.w), randomisation = FALSE, 
           alternative = 'two.sided')
@

The \code{randomisation} option is set to \code{TRUE} by default, which implies that in order to get inference based on a normal approximation, it must be explicitly set to \code{FALSE}, as in our case. Similarly, the default is a one-sided test, so that in order to obtain the results for the more commonly used two-sided test, the option \code{alternative} must be explicitly to \code{'two.sided'}. Note also that the \code{zero.policy} option is set to \code{FALSE} by default, which means that islands result in a missing value code \code{NA}. Setting this option to \code{TRUE} will set the spatial lag for island to the customary zero value. 

The results show that the Moran's I statistic are $\approx$ 0.30 and 0.34, respectively, and highly significant. This implies that there is evidence of robust \textbf{positive spatial autocorrelation} in the poverty variable (since we are rejecting the null hypothesis of random spatial distribution).


\begin{remark}
If you compute the Moran's I test for two different variables, but using the same spatial weight matrix, the expectation and variance of the Moran's I test statistic will be the same under the normal approximation. Why? 
\end{remark}

The test under randomization gives the following results:

<<moranI-randomization>>=
# Moran test under randomization
moran.test(mr$POVERTY, listw = nb2listw(queen.w), 
           alternative = 'two.sided')
@

Note how the value of the statistic and its expectation do not change relative to the normal case, only the variance is different.

We can carry out a Moran's $I$ test based on random permutation the function \code{moran.mc}. Unlike previous test, it needs the number of permutations \code{nsim}. Since the rank of the observed statistic is computed relative to the reference distribution of statistics for the permuted data sets, it is good practice to set this number to something ending on 9 (such as 99 or 999). This will lead to rounded pseudo p-values like 0.01 or 0.001.\index{Moran's I test!moran.mc function} 


<<moranI-mc>>=
# Moran's Test
set.seed(1234)
moran.mc(mr$POVERTY, listw = nb2listw(queen.w), 
           nsim = 99)
@


Note that none of the permuted data sets yielded a Moran's $I$ greater than the observed value of 0.3065, hence a pseudo p-value of $(0 + 1) / (99 + 1) = 0.01$.

The Moran scatter plot can also be obtained using the function \code{moran.plot} of \pkg{spdep}:\index{Moran's I test!moran.plot function}

<<moran-ploteF, eval = FALSE>>=
# Moran's plot
moran.plot(mr$POVERTY, listw = nb2listw(queen.w))
@

\begin{figure}[ht]
\caption{Moran Plot for Poverty}\label{fig:mp-poverty}
<<moran-plotT, fig.align='center', fig.height = 5, fig.width = 4, fig.show='hold', echo = FALSE>>=
moran.plot(mr$POVERTY, listw = nb2listw(queen.w),
           xlab = "Poverty Rate",
           ylab = "Spatially Lagged Poverty Rate"
           )
@
\end{figure}

Figure \ref{fig:mp-poverty} displays the Moran scatterplot of poverty with the queen weight matrix. Positive spatial autocorrelation, detected by the value of the Moran's I, is reflected by the fact that most of the communes are located in quadrant I and III. However, there are some exceptions such as the communes located in quadrant II and IV. For example, San Miguel is a commune with low poverty rate, but surrounded by communes with high poverty.  

A major limitation of Moran's I is that it cannot provide information on the specific locations of spatial patterns; it only indicates the presence of spatial autocorrelation globally. A single overall indication is given of whether spatial autocorrelation exists in the dataset, but no indication is given of whether local variations exist in spatial autocorrelation (e.g., concentrations, outliers) across the spatial extent of the data. 


%-----------------------
\section{Exercises}
%-------------------

\begin{exercises}
 \exercise Another method used for creating spatial weight matrices in Monte Carlo studies is the ``$k$-ahead and $k$-behind'' criterion in a circular world. (This was introduced by \cite{kelejian1999generalized}). In this approach, each spatial unit is assumed to have $k$ neighbors which are ahead of it in the order of sample, and $k$ units which are behind it. The number $k$ is typically chosen to be small relative to the sample size. Thus, each spatial unit has $2k$ neighbors. Weighting matrices which are built on this framework are typically row normalized, and all of the nonzero elements in the matrix are $1/(2k)$. Suppose $n =10$ and $k =2$. Specify the third row of the $10\times 10$ weighting matrix.\label{exercise:k-ahead}
 \exercise For a general sample size, say $n$, which corresponds to a checkerboard of squares, what is the minimum number of neighbors a unit can have if the weighting matrix is based on a queen pattern?
 \exercise  Let $INC_r$ the income per capita in cross-sectional unit $r = 1, ...., n$. Consider the following specification for $w_{ij}$:
	
	\begin{equation*}
	w_{ij}= \alpha\left[1 - \frac{\left|INC_i - INC_j\right|}{INC_i+ INC_j}\right], 
	\end{equation*}
	%
	where $\alpha$ is some pre-selected positive constant. Show that $\alpha$ will cancel if the weight matrix is row-normalized.
	\exercise  Create in \proglang{R} your own function to plot a Moran Scatterplot. Show that your function works well using a simulated example. 
\end{exercises}