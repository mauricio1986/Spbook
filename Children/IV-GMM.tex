\chapter{Instrumental Variables and GMM}\label{chapter:gmm}

Maximum Likelihood Estimation (MLE) is a widely used method in spatial econometrics; however, its application can become computationally demanding as the number of spatial units grows. This is primarily due to the necessity of manipulating $n \times n$ matrices, which involves operations such as matrix multiplication, inversion, and eigenvalue computation. These challenges are exacerbated when working with large datasets, limiting the practicality of MLE in such contexts.

In response to these computational difficulties, alternative estimation techniques such as Instrumental Variables (IV) and the Generalized Method of Moments (GMM) have been developed \citep{kelejian1998generalized, kelejian1999generalized, lee2007gmm}.  These methods offer a more computationally efficient approach, as they circumvent the need for calculating the Jacobian determinant---a key component of the MLE framework---and do not depend on the normality assumption.

This chapter focuses on the theoretical foundations and practical implementation of IV and GMM methods in spatial econometrics, with a particular emphasis on coding in R and their practical applications. Section \ref{sec:gmm-review} provides a comprehensive review of the GMM estimator for spatial models, drawing heavily on insights from \cite{pruchaHB}. The Spatial Two-Stage Least Squares (S2SLS) estimator for the Spatial Lag Model (SLM) is discussed in Section \ref{sec:s2sls}, followed by an in-depth analysis of its corresponding GMM estimator in Section \ref{sec:gmm-slm}. Sections \ref{sec:sfgls-sem} and \ref{sec:gmm-sem} explore the Spatial Feasible Generalized Least Squares (SFGLS) estimator and the GMM estimator for the Spatial Error Model (SEM), respectively. Finally, Section \ref{sec:sfg2sls-sac} outlines the Spatial Feasible Generalized Two-Stage Least Squares (SFG2SLS) estimation procedure for the Spatial Autoregressive Combined (SAC) model.


%*************************************************
\section{A Review of GMM}\label{sec:gmm-review}\index{Generalized method of moments}
%*************************************************

Before explaining the estimation procedure for the SLM, SEM, and SAC models, we review some key aspects of the Generalized Method of Moments (GMM) in the spatial econometrics context.

%===================================
\subsection{Model Specification}
%===================================

Suppose the data are generated by the following model:
\begin{equation*}
f(y_{in}, \vx_{in}, \vtheta_0) = \epsilon_{in}, \quad i = 1, \ldots, n,
\end{equation*}
%
where $f(y_{in}, \vx_{in}, \vtheta_0)$ represents a system of spatial equations, $y_{in}$ is the dependent variable for unit $i$, $\vx_{in}$ is a vector of explanatory variables, $\epsilon_{in}$ is a disturbance term, $\vtheta_0$ is a $k \times 1$ unknown parameter vector, and $f(\cdot)$ is a known function.

Assume there exists a $1 \times s$ vector of instruments $\vh_{in}$, and let $w_{in}$ denote all observable variables, including instruments, for the $i$-th unit. For simplicity, assume $\epsilon_{in}$ is i.i.d. $(0, \sigma^2)$, and $\vh_{in}$ is non-stochastic (these assumptions can be relaxed). The explanatory variables may take the form $\vx_{in} = \left[ \vx_i, \bar{\vx}_{in}, \bar{y}_{in} \right]$, where $\vx_i$ is exogenous, $\bar{\vx}_{in} = \sum_j w_{ij} \vx_j$, and  $\bar{y}_{in} = \sum_j w_{ij} y_{jn}$ are spatial lags, with $w_{ij}$ denoting spatial weights and $w_{ii} = 0$ for all $i = 1, \ldots, n$.


Suppose that there exists a vector $s \times 1$ of sample moments
\begin{equation}\label{eq:sample_moments_gmm}
\vg_n(\vtheta) = \vg_n(w_1, \ldots ,w_n, \vtheta) 
= \begin{pmatrix}
g_{1n}(w_1,\ldots,w_n, \vtheta) \\
\vdots \\
g_{sn}(w_1,\ldots,w_n, \vtheta)
\end{pmatrix},
\end{equation}
%
with $s \geq k$ for identification. Further assume:
\begin{equation*}
\E\left[\vg_n(w_1,\ldots ,w_n, \vtheta)\right] = \vzeros \iff \vtheta = \vtheta_0,
\end{equation*}
%
that is, the model is identified. Let $\mUpsilon_n$ be some $s \times s$ symmetric positive semidefinite weighting matrix, then the corresponding GMM estimator is defined as:
\begin{equation}\label{eq:gmm_one_step}
\widehat{\vtheta}_n = \underset{\vtheta \in \mTheta}{\argmin}\;\;\underset{(1\times s)}{\vg_n(w_1,\ldots,w_n, \vtheta)}^\top\underset{(s\times s)}{\mUpsilon_n}\underset{(s\times 1)}{\vg_n(w_1,\ldots,w_n, \vtheta)}.
\end{equation}
%
where $\mTheta$ is the parameter space. If $s = k$ (i.e., the model is just identified),  the weighting matrix $\mUpsilon_n$ is irrelevant and $\widehat{\vtheta}_n$ can be found as a solution to the moment condition:
\begin{equation}
\vg_n(w_1,\ldots,w_n, \widehat{\vtheta}) = \vzeros.
\end{equation}

In classical GMM literature, \textbf{linear moment conditions} are of the form:
\begin{equation*}
\E\left[\frac{1}{n} \sum_{i=1}^n \vh_i^\top \epsilon_i \right] = \vzeros,
\end{equation*}
which holds under the maintained assumptions since $\E[\vh_i^\top \epsilon_i] = \vh_i^\top \E[\epsilon_i] = \vzeros$.

The spatial econometrics literature often considers \textbf{quadratic moment conditions}. Let $\mA_q$ be an $n \times n$ matrix with $\tr(\mA_q) = 0$. As explained in the following sections, such matrices are of class $\calP_1$. Assuming $\mA_q$ is non-stochastic, the quadratic moment conditions are:
\begin{equation}\label{eq:mom_review}
\E\left[\frac{1}{n} \sum_{i=1}^n \sum_{j=1}^n a_{ijq} \epsilon_i \epsilon_j \right] = \vzeros, 
\end{equation}
%
which clearly holds under the maintained assumptions. To see this, let $\vepsi = \left[\epsilon_1, \ldots ,\epsilon_n\right]^\top$, then the moment conditions in \eqref{eq:mom_review} can be rewritten as:
\begin{equation*}
 \E\left[\frac{\vepsi^\top\mA_q\vepsi}{n}\right] = \tr\left[\frac{\mA_q\E\left(\vepsi\vepsi^\top\right)}{n}\right] = \sigma^2\frac{\tr(\mA_q)}{n} = \vzeros,
\end{equation*}
%
since under the maintained assumptions $\E\left[\vepsi\vepsi^\top\right] = \sigma^2\mI_n$ and $\tr(\mA_q) = 0$.

Now let $\vtheta_0 = \left[\lambda_0, \vdelta_0\right]^\top$ and suppose the sample moment vector in \eqref{eq:sample_moments_gmm} can be decomposed into:
\begin{equation*}
\vg_n(w_1,\ldots ,w_n, \vtheta) = 
\begin{pmatrix}
\vg_n^{\vlambda}(w_1,\ldots,w_n, \lambda, \vdelta) \\
\vg_n^{\vdelta}(w_1,\ldots,w_n, \lambda, \vdelta) 
\end{pmatrix},
\end{equation*}
%
where $\lambda$ is, for example, the spatial autoregressive parameter and $\vdelta$ is the rest of parameters in the model, such that:
\begin{eqnarray*}
\E\left[\vg_n^{\vlambda}(w_1,\ldots,w_n, \lambda, \vdelta)\right] &=& \vzeros \iff \lambda = \lambda_0,\\
\E\left[\vg_n^{\vdelta}(w_1,\ldots,w_n, \lambda, \vdelta)\right] &=& \vzeros \iff \vdelta = \vdelta_0,
\end{eqnarray*}
%
and that some easily (and consistent) computable initial estimator, say $\widehat{\vdelta}_n$, for $\vdelta_0$ is available. In this case, we may consider the following GMM estimator for $\lambda_0$ corresponding to some weighting matrix $\mUpsilon^{\lambda\lambda}_n$:
\begin{equation}\label{eq:gmm_two_step_a}
\widehat{\lambda}_n = \underset{\lambda}{\argmin}\;\;\vg_n^{\lambda}(w_1,\ldots,w_n, \lambda, \widehat{\vdelta})^\top\mUpsilon^{\lambda\lambda}_n(w_1,\ldots,w_n, \lambda, \widehat{\vdelta}).
\end{equation}

Using $\widehat{\lambda}_n$ we may further consider the following estimator for $\vdelta_0$ corresponding to some weight matrix $\mUpsilon^{\delta\delta}_n$:
\begin{equation}\label{eq:gmm_two_step_b}
\widehat{\vdelta}_n = \underset{\vdelta}{\argmin}\;\;\vg_n^{\vdelta}(w_1,\ldots,w_n, \widehat{\lambda}, \vdelta)^\top\mUpsilon^{\delta\delta}_n(w_1,\ldots,w_n, \widehat{\lambda}, \vdelta).
\end{equation}

GMM estimator like $\widehat{\vtheta}$ in Equation \eqref{eq:gmm_one_step} are often referred to as \textbf{one-step estimators}. Estimators like $\widehat{\lambda}_n$ and $\widehat{\vdelta}_n$ in Equations \eqref{eq:gmm_two_step_a} and \eqref{eq:gmm_two_step_b} above, where the sample moments depend on some initial estimator, are often referred to as \textbf{two-step estimators}.

When the model conditions hold, the most efficient one-step estimator is expected to outperform even the most efficient two-step estimators in terms of statistical efficiency. However, practical trade-offs often influence the choice of estimator. One key trade-off involves computational complexity. For small sample sizes, maximum likelihood (ML) estimation may serve as an alternative to GMM, offering robust performance. Conversely, in large samples, computational efficiency and feasibility often outweigh gains in statistical efficiency, making two-step GMM estimators an attractive option.

Moreover, Monte Carlo studies indicate that the efficiency loss associated with two-step estimators is often modest in many practical scenarios. Another critical consideration is the potential impact of misspecifying a moment condition. Such misspecification can lead to inconsistent estimates for all model parameters, underscoring the importance of careful model specification and moment selection.

%===================================
\subsection{Asymptotic Distribution of One-Step GMM Estimator}\label{sec:gmm-one-step}
%===================================

Assuming that $\widehat{\vtheta}_n$ is an interior point, the first-order condition for maximization of the objective function is:
\begin{equation}\label{eq:foc_review}
  \underset{(k\times 1)}{\vzeros} = \underset{(k\times 1)}{\frac{\partial Q_n(\widehat{\vtheta}_n)}{\partial \vtheta}} = - \underset{(k\times s)}{\mG_n(\widehat{\vtheta}_n)^\top}\underset{(s\times s)}{\mUpsilon_n}\underset{(s\times 1)}{\vg_n(\widehat{\vtheta}_n)},
\end{equation}
%
where $Q_n = \vg_n(\vtheta)^\top\mUpsilon_n\vg_n(\vtheta)$, and $\mG_n(\vtheta)$ is the Jacobian of $\vg_n(\vtheta)$:
\begin{equation*}
  \mG_n(\vtheta) \equiv  \frac{\partial \vg_n(\vtheta)}{\partial \vtheta^\top}.
\end{equation*}

Using a Taylor expansion of $\vg_n(\vtheta)$, we obtain:
\begin{equation}\label{eq:taylor_exp_g}
  \vg_n(\widehat{\vtheta}_n) = \vg_n(\vtheta_0) + \mG_n(\overline{\vtheta})\left(\widehat{\vtheta}_n - \vtheta_0\right).
\end{equation}

Substituting \eqref{eq:taylor_exp_g} into the first-order condition \eqref{eq:foc_review}, we get:
\begin{equation*}
  \vzeros = \frac{\partial Q_n(\widehat{\vtheta}_n)}{\partial \vtheta} = - \mG_n(\widehat{\vtheta}_n)^\top\mUpsilon_n\vg_n(\widehat{\vtheta}_n) - \mG_n(\widehat{\vtheta}_n)^\top\mUpsilon_n\mG_n(\overline{\vtheta})\left(\widehat{\vtheta}_n - \vtheta_0\right).
\end{equation*}

Solving this for $\left(\widehat{\vtheta}_n - \vtheta_0\right)$, multiplying by $\sqrt{n}$, and under some regularity conditions yields:
\begin{equation*}
\sqrt{n}(\widehat{\vtheta}_n - \vtheta_0) = - \left[\mG^\top_0\mUpsilon_0\mG_0\right]^{-1}\mG^\top_0\mUpsilon_0\left[\sqrt{n}\vg_n(\vtheta_0)\right] + o_p(1),
\end{equation*}
%
where
\begin{equation}\label{eq:asymptotic_distr_moment} 
\begin{aligned}
\mG_0 & = \plim_{n\to\infty}\frac{\partial \vg_n(\vtheta_0)}{\partial \vtheta} \quad \mbox{by some LLN},  \\
\mUpsilon_0 & = \plim_{n\to\infty}\mUpsilon_n\quad \mbox{by some LLN}, \\ 
\sqrt{n}\vg_n(\vtheta_0)& \dto  \rN(\vzeros, \mPsi_0) \quad \mbox{by some CLT} 
\end{aligned}
\end{equation}
%\begin{eqnarray}
% \mG_n(\widehat{\vtheta})& \pto & \mG \quad \mbox{by some LLN}\\
% \mUpsilon_n &\pto & \mUpsilon\quad \mbox{by some LLN} \\
% \sqrt{n}\vg_n(\vtheta_0)& \dto & \rN(\vzeros, \mPsi) \quad \mbox{by some CLT} \label{eq:asymptotic_distr_moment} 
%\end{eqnarray}
%
where $\mPsi_0$ is some positive definite matrix. Then applying traditional asymptotic rules:
\begin{equation*}
\sqrt{n}(\widehat{\vtheta}_n - \vtheta_0)\dto \rN\left(\vzeros, \mPhi_0\right),
\end{equation*}
%
where:
\begin{equation*}
\mPhi_0 = \left[\mG^\top_0\mUpsilon_0\mG_0\right]^{-1}\mG^\top_0\mUpsilon_0\mPsi_0\mUpsilon_0\mG_0\left[\mG^\top_0\mUpsilon_0\mG_0\right]^{-1}. 
\end{equation*}

It can be seen that if we choose $\mUpsilon_n = \widehat{\mPsi}^{-1}_n$ (with weights given by the inverse of the variance-covariance matrix of the moment conditions), where $\widehat{\mPsi}_n\pto \mPsi_0$, the variance-covariance simplifies to
\begin{equation*}
\mPhi_0 = \left[\mG^\top_0\mPsi^{-1}_0\mG_0\right]^{-1}.
\end{equation*}

Since $\left[\mG^\top_0\mUpsilon_0\mG_0\right]^{-1}\mG^\top_0\mUpsilon_0\mPsi_0\mUpsilon_0\mG_0\left[\mG^\top_0\mUpsilon_0\mG_0\right]^{-1} - \left[\mG^\top_0\mPsi^{-1}_0\mG_0\right]^{-1}$ is positive semidefinite it follows that $\mUpsilon_n= \widehat{\mPhi}^{-1}_n$ gives the optimal GMM estimator with lower asymptotic variance. 

The most relevant literature on one-step GMM estimators in spatial econometrics includes:
\begin{itemize}
  \item GMM estimator of SLM assuming homoskedasticity \citep{lee2007gmm}.
  \item GMM estimator of SLM assuming homoskedasticity, but reducing the joint maximization to the maximization with respect to $\rho$ only. 
  \item GMM estimator of SLM assuming heteroskedasticity \citep{lin2010gmm}. 
  \item GMM estimator of SLM with additional endogenous variables \citep{liu2015gmm}.
  \item GMM estimator of SAC model assuming homoskedasticity \citep{lee2010efficient, liu2010efficient}.
\end{itemize}

%===================================
\subsection{Asypmtotic Distribution of Two-Step GMM Estimator}\label{section:2step-gmm}
%===================================

The usual approach to deriving the limiting distribution of two-step GMM estimators is to manipulate the score of the objective function by expanding the sample moment vector around the true parameter, using a Taylor expansion.\footnote{For more on two-step estimation see \citet[][section 6]{newey1994large}}

Consider the two-step GMM estimators for $\lambda_0$ defined in Equation (\ref{eq:gmm_two_step_a}). Applying this approach, and assuming typical regularity conditions, we get:
\begin{equation}\label{eq:taylor_gmm_1}
\sqrt{n}\left(\widehat{\lambda}_n - \lambda_0\right) = - \left[(\mG_0^{\lambda\lambda})^\top\mUpsilon_0^{\lambda\lambda}\mG_0^{\lambda\lambda}\right]^{-1}\left(\mG_0^{\lambda\lambda}\right)^\top\mUpsilon_0^{\lambda\lambda}\left[\sqrt{n}\vg_n^{\lambda}(\lambda_0, \vdelta_0) + \mG_0^{\lambda\delta}\sqrt{n}\left(\widetilde{\vdelta}_n - \vdelta_0\right)\right] + o_p(1),
\end{equation}
%
where
\begin{eqnarray*}
	\frac{\partial \vg_n^{\lambda}(\lambda_0, \vdelta_0)}{\partial \vlambda} &\pto&  \mG^{\lambda\lambda}_0, \\
	\frac{\partial \vg_n^{\lambda}(\lambda_0, \vdelta_0)}{\partial \vdelta} &\pto&  \mG^{\lambda\delta}_0, \\
	\mUpsilon^{\lambda\lambda}_n & \pto & \mUpsilon^{\lambda\lambda}_0.
\end{eqnarray*}

In many cases the estimator $\widetilde{\vdelta}_n$ will be asymptotically linear in the sense that
\begin{equation*}
\sqrt{n}\left(\widetilde{\vdelta}_n - \vdelta_0\right)=\frac{1}{\sqrt{n}}\mT_n^\top\vepsi_n + o_p(1),
\end{equation*}
%
where $\mT_n$ is a non-stochastic $n\times k_{\delta}$ matrix, where $k_{\delta}$ is the dimension of $\vdelta_0$, and where $\vepsi_n = (\epsilon_1,\ldots,\epsilon_n)^\top$. Now define:
\begin{equation*}
\vg_{*n}^{\lambda}(\lambda_0, \vdelta_0) = \vg_n^{\lambda}(\lambda_0, \vdelta_0) + \frac{1}{n}\mG^{\lambda\delta}_0\mT_n^\top\vepsi_n.
\end{equation*}

Then Equation \eqref{eq:taylor_gmm_1} can be rewritten as:
\begin{equation}\label{eq:taylor_gmm_2}
\sqrt{n}\left(\widehat{\lambda}_n - \lambda_0\right) = - \left[(\mG^{\lambda\lambda}_0)^\top\mUpsilon^{\lambda\lambda}_0\mG^{\lambda\lambda}_0\right]^{-1}(\mG^{\lambda\lambda}_0)^\top\mUpsilon^{\lambda\lambda}_0\left[\sqrt{n}\vg_{*n}^{\lambda}(\lambda_0, \vdelta_0) \right] + o_p(1).
\end{equation}

Now suppose that 
\begin{equation*}
\sqrt{n}\vg_{*n}^{\lambda}(\lambda_0, \vdelta_0) \dto \rN\left(0, \mPsi^{\lambda\lambda}_{*}\right), 
\end{equation*}
%
where $\mPsi^{\lambda\lambda}_{*}$ is some positive definite matrix. Then
\begin{equation*}
\sqrt{n}\left(\widehat{\lambda}_n - \lambda_0\right)\dto \rN\left(\vzeros, \mPhi_{*}^{\lambda\lambda}\right),
\end{equation*}
%
with:
\begin{equation*}
\mPhi_{*}^{\lambda\lambda} = \left[(\mG^{\lambda\lambda}_0)^\top\mUpsilon^{\lambda\lambda}_0\mG^{\lambda\lambda}_0\right]^{-1}(\mG^{\lambda\lambda}_0)^\top\mUpsilon^{\lambda\lambda}_0\mPsi_{*}^{\lambda\lambda}\mUpsilon^{\lambda\lambda}_0\mG^{\lambda\lambda}_0\left[(\mG^{\lambda\lambda}_0)^\top\mUpsilon^{\lambda\lambda}_0\mG^{\lambda\lambda}_0\right]^{-1}.
\end{equation*}

From this it is seen that if we choose $\mUpsilon_n^{\lambda\lambda} = \left(\widetilde{\mPsi}_{*n}^{\lambda\lambda}\right)^{-1}$ where $\widetilde{\mPsi}_{*n}^{\lambda\lambda}\pto \mPsi_{*}^{\lambda\lambda}$, then variance-covariance simplifies to
\begin{equation*}
\mPhi_{*}^{\lambda\lambda} = \left[(\mG^{\lambda\lambda}_0)^\top(\mPsi^{\lambda\lambda}_{*})^{-1}\mG^{\lambda\lambda}_0\right]^{-1}.
\end{equation*}

So, using the weighting matrix $\mUpsilon_n^{\lambda\lambda}$, a consistent estimator for the inverse of the limiting variance-covariance matrix $\mPsi_{*}^{\lambda\lambda}$ yields the efficient two-step GMM estimator.

Suppose that Equation \eqref{eq:asymptotic_distr_moment} holds and:
\begin{equation*}
	\mPsi = \begin{pmatrix}
	\mPsi^{\lambda\lambda} & \mPsi^{\lambda\delta} \\
	\mPsi^{\delta\lambda} & \mPsi^{\delta\delta}
	\end{pmatrix},
\end{equation*}
%
then the limiting distribution of the sample moment vector $\vg_n^{\lambda}$ evaluated at the true parameter is given by
\begin{equation*}
\sqrt{n}\vg_{n}^{\lambda}(\vlambda_0, \vdelta_0)\dto \rN\left(\vzeros, \mPsi^{\lambda\lambda}\right).
\end{equation*}

Note that in general $\mPsi_{*}^{\lambda\lambda}\neq \mPsi^{\lambda\lambda}$, unless $\mG^{\lambda\delta} = \vzeros$, and that in general $\mPsi_{*}^{\lambda\lambda}$ will depend on $\mT_n$, which in turn will depend on the employed estimator $\widehat{\vdelta}_n$. In other words, unless $\mG^{\lambda\delta}_0 = \vzeros$, for a two-step GMM estimator, we cannot simply use the variance-covariance matrix $\mPsi^{\lambda\lambda}$ of the sample moment vector $\vg^{\lambda}(\lambda_0, \vdelta_0)$, rather we need to work with the variance-covariance matrix $\mPsi_{*}^{\lambda\lambda}$.

\cite{pruchaHB} illustrate the difference between $\mPsi^{\lambda\lambda}$, with elements $\Psi_{rs}^{\lambda\lambda}$, and $\mPsi_{*}^{\lambda\lambda}$, with elements $\Psi_{*rs}^{\lambda\lambda}$, for the important special case where the moment conditions are quadratic and $u_i$ is i.i.d $\rN(0, \sigma^2)$. For simplicity assume that
\begin{equation*}
\vg_n^{\lambda}(\lambda_0, \vdelta_0) = \begin{pmatrix}
\frac{1}{n}\sum_{i = 1} ^n\sum_{j = 1} ^na_{ij1}\epsilon_i \epsilon_j \\
\frac{1}{n}\sum_{i = 1} ^n\sum_{j = 1} ^na_{ij2} \epsilon_i \epsilon_j
\end{pmatrix}.
\end{equation*}

Now, for $r = 1,2$, let $a_{ir}$ denote the $(i,r)$th element of $\mG^{\lambda\delta}_0\mT_n^\top$, then by Equation \eqref{eq:asymptotic_distr_moment}:
\begin{equation*}
\vg_{*n}^{\lambda}(\lambda_0, \vdelta_0) = \begin{pmatrix}
\frac{1}{n}\sum_{i = 1} ^n\sum_{j = 1} ^na_{ij1}\epsilon_i \epsilon_j + \frac{1}{n}\sum_{i=1}^na_{i1}\epsilon_i\\
\frac{1}{n}\sum_{i = 1} ^n\sum_{j = 1} ^na_{ij2}\epsilon_i \epsilon_j + \frac{1}{n}\sum_{i=1}^na_{i2}\epsilon_i\
\end{pmatrix}
\end{equation*}

It then follows from Limiting Distribution for linear-quadratic forms \ref{teo:clt_quadratic} that
\begin{equation*}
\Psi_{rs}^{\lambda\lambda} = 2\sigma^4\sum_{i = 1} ^n \sum_{j = 1}^n a_{ijr}a_{ijs}, 
\end{equation*}
%
but
\begin{equation*}
\Psi_{*rs}^{\lambda\lambda} = 2\sigma^4\sum_{i = 1} ^n \sum_{j = 1} ^n a_{ijr}a_{ijs} + \sigma^2\sum_{i=1}^n a_{ir}a_{is}.
\end{equation*}

Note that $a_{ir}$ and $a_{is}$ in the last sum of the RHS for the expression for $\Psi_{*rs}^{\lambda\lambda}$ depend on what estimator $\widehat{\vdelta}_n$ is employed in the sample moment vector $\vg_n^{\lambda}(\lambda_0, \widehat{\vdelta})$ used to form the objective function for the two-step GMM estimator $\widehat{\lambda}_n$ defined in Equation (\ref{eq:gmm_two_step_a}). It is for this reason that in the literature on two-step GMM estimation, users are often advised to follow a specific sequence of steps, to ensure the proper estimation of respective variance-covariance matrices. 

The relevant theoretical literature on two-step GMM estimators for spatial models are:
\begin{enumerate}
  \item Generalized Spatial Two Stage Least Squares Estimator (GS2SLS) of the SAC model assuming homoskedasticity \citep{kelejian1998generalized}.
  \item Feasible Generalized Least Squares (FGLS) of the SEM model under homoskedascitiy. \citep{kelejian1999generalized}.
  \item Feasible Generalized Two Stage Least Squares Estimator (FG2SLS) Estimator of the SAC model under heteroskedasticity \citep{kelejian2010specification}.
\end{enumerate}

%=======================================================
\section{Spatial Two Stage Estimation of SLM}\label{sec:s2sls}\index{Instrumental Variables!S2SLS}
%=======================================================

In this section, we derive the Spatial Two Stage Least Square (S2SLS) procedure for estimating the SLM. To do so, we rely on \cite{kelejian1998generalized} who first derived the asymptotic properties of the S2SLS estimator.\footnote{In particular, \cite{kelejian1998generalized} derived this model as the first step in their Generalized S2SLS.} This estimation approach has two notable advantages:
\begin{enumerate}
  \item It does not require the computation of the Jacobian term, making it computationally more efficient than ML-based methods. 
  \item It avoids the strong assumption of normality of the error terms, thereby offering greater flexibility.
\end{enumerate}

To understand the essence of this procedure, let us revisit the formulation of the SLM, expressed as:
\begin{equation*}
  \vy_n =   \mX_n\vbeta_0 + \rho_0\mW_n\vy_n + \vepsi_n.
\end{equation*}

Alternatively, we can write the model more concisely as:
\begin{equation*}
  \vy_n = \mZ_n \vdelta_0 + \vepsi_n,
\end{equation*}
%
where $\mZ_n = \left[\mX_n, \mW_n\vy_n\right]$ is an $n \times (k + 1)$ matrix that combines the \textbf{exogenous regressors} and the spatially lagged dependent variable, and the $(k + 1)\times 1$ coefficient column vector is rearranged as  $\vdelta_0 = (\vbeta^\top, \rho)^\top$. As discussed in Section \ref{sec:consequences_slm}, the inclusion of the spatially lagged dependent variable, $\mW_n\vy_n$, on the right-hand side of the equation introduces endogeneity or simultaneous equation bias. Therefore, the OLS estimates are inconsistent.

To address this issue, we may use an instrumental variables (IV) approach rather than resorting to QML or ML methods. The IV principle relies on the existence of a matrix of instruments, $\mH_n$, that is strongly correlated with $\mZ_n$ but asymptotically uncorrelated with the error term $\vepsi_n$.

At this point is important to stress that the only endogenous variable in this model is the spatially lagged dependent variable. Thus, the instrument  matrix $\mH_n$ should include all the predetermined variables, that is, $\mX_n$ and the instrument(s) for $\mW_n\vy_n$. 

%==================================
\subsection{Instruments in the Spatial Context}\index{Instrumental Variables!instruments}
%==================================

What constitutes the best instruments for $\mW_n\vy_n$? To construct the instrument matrix $\mH_n$, it is essential to refer to the literature on \textbf{optimal instrumental variables}. Broadly, this literature suggest that the `best instruments' for the r.h.s variables are their conditional means. Consequently, the ideal instruments are expressed as:
\begin{equation*}
  \begin{aligned}
\E\left(\mZ_n|\mX_n\right) & = \left[\E\left(\mX_n|\mX_n\right), \E\left(\mW_n\vy_n|\mX_n\right)\right], \\
                         & = \left[\mX_n, \mW_n \E\left(\vy_n|\mX_n\right)\right]\quad \mbox{since $\mW_n$ is non-stochastic}.
\end{aligned}
\end{equation*}

Given that $\mX_n$ is exogenous, it serves as its own best instrument, whereas the best instruments for $\mW_n\vy_n$ are given by $\mW_n\E\left(\left.\vy_n\right|\mX_n\right)$. Noting that the reduced-form equation is $\vy_n = (\mI_n - \rho\mW_n)^{-1}\left(\mX_n\vbeta + \vepsi_n \right)$, and applying Leontief Expansion (Lemma \ref{lemma:Leontief}), the expected value of the reduced form is:
\begin{equation}\label{eq:exp_instr}
  \E(\vy_n|\mX_n) = (\mI_n - \rho\mW_n)^{-1}\mX_n\vbeta = \left[\mI_n + \rho\mW_n + \rho^2\mW_n^2 + \cdots \right]\mX_n\vbeta = \left[\sum_{l = 1}^{\infty}\rho^{l}\mW^l_n\right]\mX_n\vbeta.
\end{equation}

The challenge lies in approximating  $\E(\vy_n|\mX_n)$ without directly inverting the $n\times n$ matrix $(\mI_n - \rho\mW_n)$. Equation \eqref{eq:exp_instr} reveals that $\E(\vy_n|\mX_n)$ can be expressed as a linear function of $\mX_n, \mW_n\mX_n, \mW^2_n\mX_n, \ldots$ As a result, and given that the roots of $\rho\mW_n$ are less than one in absolute value (or $\rho$ is in its parameter space), the expectation can also be written as:
\begin{equation*}
  \begin{aligned}
\E(\mW_n\vy_n|\mX_n) &  = \mW_n\left(\mI_n - \rho \mW_n\right)^{-1}\mX_n\vbeta, \\
               & = \mW_n\left[\mI_n + \rho\mW_n + \rho^2\mW^2_n + \rho^3\mW^3_n + \cdots\right]\mX_n\vbeta, \\
               & = \mW_n\left[\sum_{l = 1}^{\infty}\rho_0^{l}\mW^l_n\right]\mX_n\vbeta, \\
               & = \mW_n\mX_n\vbeta + \mW^2_n\mX_n(\rho\vbeta) + \mW^3_n\mX_n(\rho^2\vbeta) + \mW^4_n\mX_n(\rho^3\vbeta) + \cdots
  \end{aligned}
\end{equation*}

To avoid issues associated with the computation of the inverse of $(\mI_n - \rho\mW_n)$,  \cite{kelejian1998generalized, kelejian1999generalized} propose an approximation of the optimal instruments. Recognizing that $\E(\vy_n|\mX_n)$ is linear in $\mX_n, \mW_n\mX_n, \mW^2_n\mX_n, \ldots$, they suggest using a set of instruments $\mH_n$ which consists of the linearly independent (LI) columns of  $\mX_n, \mW_n\mX_n, \mW^2_n\mX_n, \ldots, \mW^l_n\mX_n$,  where $l$ is a pre-selected finite constant and is generally set to 2 in applied studies. Thus, if $l =2$, the instrument matrix becomes:
\begin{equation*}
  \mH_n = (\mX_n, \mW_n\mX_n, \mW^2_n\mX_n).
\end{equation*}

\begin{remark}
The intuition behind the instruments is the following: Since $\mX_n$ determines $\vy_n$, then it must be true that $\mW_n\mX_n, \mW^2_n\mX_n, \ldots$ determines $\mW_n\vy_n$. Furthermore, since $\mX_n$ is uncorrelated with $\vepsi_n$, then $\mW_n\mX_n$ must be also uncorrelated with $\vepsi_n$.
\end{remark}

%========================================================================
\subsection{Defining the S2SLS Estimator}
%========================================================================

With the instrument matrix $\mH_n$ defined, we can now apply the standard two-stage least squares (2SLS) procedure, modified to account for the asymptotic properties of the spatial weight matrices $\mW_n$ and $(\mI_n - \rho\mW_n)$. Due to the inclusion of these spatial components, this method is referred to as the spatial two stage least squares (S2SLS) \citep{kelejian1998generalized}. 

We begin by introducing the assumptions concerning the structure of the error term. Specifically, we assume that the errors form triangular arrays and exhibit heteroskedasticity. Note that \cite{kelejian1998generalized} derived the asymptotic properties under the assumption of homoskedastic errors, while \cite{kelejian2010specification} extended this framework to accommodate heteroskedasticity. 

%In this wo, we adopt the heteroskedasticity assumption of \cite{kelejian2010specification}, recognizing that homoskedasticity represents a special case within this broader framework.

%An important distinction between this approach and the maximum likelihood (ML) framework is that we do not require a full specification of the error term's distribution. Instead, we focus on its moments.

%--------------------------------------------------------------------------
\begin{assumption}[Heterokedastic Errors \citep{kelejian2010specification}]\label{assu:errors_triang}
The errors terms $\left\lbrace \epsilon_{i,n}, 1 \leq i \leq n, n\geq 1\right\rbrace$ satisfy $\E(\epsilon_{i,n})= 0$, $\E(\epsilon_{i,n}^2) = \sigma^2_{i,n}$, with $0 < \underline{a}^\sigma \leq \sigma^2_{i,n}\leq \overline{a}^\sigma<\infty$. Additionally the errors are assumed to possess fourth moments, that is $\sup_{1\leq i \leq n, n\geq 1}\E\left|\epsilon_{i, n}\right|^{4 + \eta}$ for some $\eta > 0$.  Furthermore, for each $n\geq 1$ the random variables $\epsilon_{1, n}, \ldots, \epsilon_{n, n}$ are totally independent. 
\end{assumption}
%--------------------------------------------------------------------------

Assumption \ref{assu:errors_triang} characterizes the error terms' first two moments without imposing assumptions on their full distribution. It explicitly allows for heteroskedasticity, meaning the unobserved variables may have different variances across spatial units\index{Heteroskedasticity!error term}. Additionally, this assumption accommodates cases where the innovations depend on the sample size $n$ by requiring the errors to form a \textbf{triangular arrays}. See our discussion in Section \ref{sec:triangular-array} about triangular arrays. 

For reference, we also present the assumption of homoskedastic errors as initially proposed by \cite{kelejian1998generalized}:

%-----------------------------------------------------
\begin{assumption}[Homoskedastic Errors \citep{kelejian1998generalized}]\label{assu:errors_homoskedastic}
The errors  $\left\lbrace \epsilon_{i,n}, 1 \leq i \leq n, n\geq 1\right\rbrace$ are distributed identically. Further, the errors $\left\lbrace \epsilon_{i,n}, 1 \leq i \leq n\right\rbrace$ are for each $n$ distributed jointly independent with  $\E(\epsilon_{i,n})= 0$ and $\E(\epsilon_{i,n}^2) = \sigma^2_{\epsilon}$, with $0 < \sigma^2_{\epsilon}  < b <\infty$. Additionally the errors are assumed to possess fourth moments.
\end{assumption}
%-----------------------------------------------------

We now outline several key assumptions regarding the behavior of the spatial weight matrix, $\mW_n$.

%-----------------------------------------------------
\begin{assumption}[Diagonal elements of $\mW_n$ \citep{kelejian1998generalized}]\label{assu:diag_W}
All diagonal elements of the spatial weighting matrix $\mW_n$ are zero.
\end{assumption}
%-----------------------------------------------------

Assumption \ref{assu:diag_W} (Diagonal elements of $\mW_n$) is a normalization of the model and it also implies that no spatial unit is viewed as its own neighbor. 

%-----------------------------------------------------
\begin{assumption}[Nonsingularity \citep{kelejian1998generalized}]\label{assu:non_singularity}
The matrix $(\mI_n - \rho_0\mW_n)$ is nonsingular with $\left|\rho_0 \right|<1$.
\end{assumption}
%-----------------------------------------------------

The nonsingularity condition in Assumption \ref{assu:non_singularity} allows us to express the reduced form of the true model as:
\begin{equation*}
  \vy_n = (\mI_n - \rho_0\mW_n)^{-1}\mX_n\vbeta_0 + (\mI_n - \rho_0\mW_n)^{-1}\vepsi_n.
\end{equation*}

This assumption ensures that the model is well-defined, enabling us to solve for $\vy_n$. Additionally, \cite{kelejian1998generalized} note that the elements of $(\mI_n - \rho_0\mW_n)^{-1}$ depend on the sample size $n$, even if the elements of $\mW_n$ do not vary with $n$. Therefore, the elements of $\vy_n$ also depend on $n$, meaning that $\vy_n$ forms a triangular array, even in cases where the errors $\epsilon_{i,n}$ are independent of $n$. 

Furthermore, Assumption \ref{assu:errors_triang} (Heteroskedastic Errors) implies that the population variance-covariance matrix of $\vy_n$ is given by:
\begin{equation}\label{eq:variance_of_y_slm}
  \E(\vy_n\vy^\top_n) = \mOmega_{y_n}= (\mI_n - \rho_0\mW_n)^{-1}\mSigma_n(\mI_n - \rho_0\mW^\top_n)^{-1},
\end{equation}
%
where $\mSigma_n = \textrm{Diag}(\sigma^2_{i, n})$, and $\textrm{Diag}(\cdot)$ is the operator that generates a diagonal matrix. 

Under the homoskedasticity assumption (Assumption \ref{assu:errors_homoskedastic}), this variance-covariance matrix simplifies to:
\begin{equation}\label{eq:variance_of_y_slm-homo}
  \E(\vy_n\vy^\top_n) = \mOmega_{y_n} = \sigma^2_{\epsilon}(\mI_n - \rho_0\mW_n)^{-1}(\mI_n - \rho_0\mW^\top_n)^{-1}.
\end{equation}

%------------------------------------------------
\begin{assumption}[Bounded matrices \citep{kelejian1998generalized}]\label{assu:bounded_matrix}
The row and column sums of the matrices $\mW_n$ and $(\mI_n - \rho_0\mW_n)$ are bounded uniformly in absolute value.
\end{assumption}
%------------------------------------------------


Assumption \ref{assu:bounded_matrix} ensures that the variance of $\vy_n$  in Equation (\ref{eq:variance_of_y_slm}), which depends on $\mW_n$ and $(\mI_n - \rho_0\mW_n)$, are uniformly bounded in absolute value as $n$ goes to infinity.  This limits the degree of correlation among the elements of both $\vepsi_n$ and $\vy_n$. This technical assumption is crucial for deriving the large-sample properties of the regression parameter estimators. 

\begin{remark}
Applied to $\mW_n$ Assumption \ref{assu:bounded_matrix} (Bounded matrices) means that each cross-sectional unit can only have a limited number of neighbors. When applied to $\left(\mI_n - \rho _0\mW_n\right)$, it limits the degree of spatial correlation among units. 
\end{remark}

%-------------------------------------------
\begin{assumption}[No Perfect Multicolinearity \citep{kelejian1998generalized}]\label{assu:regressors}
The regressor matrices $\mX_n$ have full column rank (for $n$ large enough). Furthermore, the elements of the matrices $\mX_n$ are uniformly bounded in absolute value.
\end{assumption}
%-------------------------------------------

This assumption ensures that the regressors are not perfectly collinear, which is essential for identifying the parameters of the model. Uniform boundedness further ensures the stability of the regression coefficients in large samples

We now introduce the assumptions concerning the instrument matrix, $\mH_n$.

%-------------------------------------------
\begin{assumption}[Rank Instruments, \citep{kelejian1998generalized}]\label{assu:iv_instr}
The instrument matrices $\mH_n$ have full column rank $p \geq k + 1$ for all $n$ large enough. Furthermore, the elements of the matrices $\mH_n$ are uniformly bounded in absolute value. They are composed of a subset of the linearly independent columns of $(\mX_n, \mW_n\mX_n, \mW^2_n\mX_n, \ldots)$.
\end{assumption}
%-------------------------------------------


%-------------------------------------------
\begin{assumption}[Limits of Instruments \citep{kelejian1998generalized} ]\label{assu:iv_instr_lim}
Let $\mH_n$ be a matrix of instruments, then:
\begin{enumerate}
  \item $\lim_{n\to \infty} n^{-1}\mH_n^\top\mH_n = \mQ_{HH}$ where $\mQ_{HH}$ is finite and nonsingular. 
  \item $\plim_{n\to \infty} n^{-1}\mH_n^\top\mZ_n = \mQ_{HZ}$ where $\mQ_{HZ}$ is finite and has full column rank.
\end{enumerate}
\end{assumption}
%-------------------------------------------

The Rank Condition of Assumption \ref{assu:iv_instr} establishes that there exists a least $p$ columns that are linearly independent such that $p \leq k + 1$. That is, the model can be just- or over-identified. In addition, since $\mH_n$ is composed of a subset of linearly independent columns of $(\mX_n, \mW_n\mX_n, \mW^2_n\mX_n, \ldots)$, it does not depend on the parameters of the model, and hence, it is non-stochastic. This simplifies the asymptotic properties of the 2SLS estimator. This contrast with the Best 2SLS estimator, which requires consistent estimate of the parameters. See Section \ref{sec:BS2SLS}.   

Assumption \ref{assu:iv_instr_lim} establishes conditions for \textbf{identification} of the model. The first condition in Assumption \ref{assu:iv_instr_lim} ensures that the instrument matrix $\mH_n$ remains well-conditioned as the sample size increases, preventing issues such as multicollinearity among the instruments. Importantly, since $\mH_n$ contains spatially lagged explanatory variables, this condition also implies that implies that $\mW_n\mX_n$ and $\mX_n$ cannot be linearly dependent. This condition would be violated if for example $\mW_n\mX_n$ included a spatial lag of a constant term or if the model is the pure SLM.  

The second condition in Assumption \ref{assu:iv_instr_lim} (Limits of Instruments) ensures a nonzero correlation between the instruments and the explanatory variables. Specifically, it guarantees that the instruments are valid in the sense of being correlated with the endogenous components of the model.  

Note that $n^{-1}\mH_n^\top\mZ_n = \left[n^{-1}\mH_n^\top\mX_n, n^{-1}\mW_n\vy_n\right]$. Then part (b) of Assumption \ref{assu:iv_instr_lim} implies:
\begin{equation}\label{eq:implication-of-plimHQ}
\plim_{n\to\infty} \frac{1}{n}\mH_n^\top\mW_n\vy_n = \lim_{n\to \infty}\frac{1}{n}\mH_n^\top\E(\mW_n\vy_n). 
\end{equation}

%-------------------------------------------------
\begin{proof}\label{proof:consistency-plimHQ}
We need to show that the expectation of $n^{-1}\mH_n^\top\mW_n\vy_n$ exists and its variance converges to zero. The result then follows from Theorem \ref{teo:chebyshev} (Consistency of Unbiased Estimator).

Let $\vpsi_n = \frac{1}{n}\mH_n^\top\mW_n\vy_n$. Since $\E(\vy_n) = \mA_0^{-1}\mX_n\vbeta_0$, where $\mA_0 = \left(\mI_n - \rho_0\mW_n\right)$:
\begin{equation*}
\E(\vpsi_n) =\E\left(\frac{1}{n}\mH_n^\top\mW_n\vy_n\right) = \frac{1}{n}\mH_n^\top\mW_n\E(\vy_n) = \frac{1}{n}\mH_n^\top\mW_n\mA_0^{-1}\mX_n\vbeta_0.
\end{equation*}

This expectation exists as long as all its elements are uniformly bounded and $\mA_0$ is invertible as $n\to\infty$. 

The variance of $\vpsi_n$ is 
\begin{equation*}
\begin{aligned}
\var\left(\frac{1}{n}\mH_n^\top\mW_n\vy_n\right) & = \frac{1}{n^2}\mH_n^\top\mW_n\E(\vy_n\vy_n^\top)\mW_n^\top\mH_n, \\
& = \frac{1}{n^2}\mH_n^\top\mW_n\mOmega_{\vy_n}\mW_n^\top\mH_n, \quad \mbox{using Equation \eqref{eq:variance_of_y_slm-homo}} \\
& = \frac{1}{n^2}\mH_n^\top\mD_n\mH_n,
\end{aligned}
\end{equation*}
%
where $\mD_n = \mW_n\mOmega_{\vy_n}\mW_n^\top$. 
Assumption \ref{assu:bounded_matrix} implies that the row and column sums of $\mD_n$ are uniformly bounded in absolute value. Using Definition \ref{def:Bounded_Matrices}, then there exists a constant $c_d$ such that $\max_{1 \leq j\leq n}\sum_{i = 1}^n\left|d_{n, ij}\right| \leq c_d$ and $\max_{1 \leq i\leq n}\sum_{j = 1}^n\left|d_{n, ij}\right| \leq c_d$. By Assumption \ref{assu:regressors} and \ref{assu:bounded_matrix}, the elements of $\mH_n = \left[\mX_n, \mW_n\mX_n, \mW_n^2\mX_n, \ldots \right]$ are uniformly bounded in absolute value by some finite constant, say $c_h$. Let $\delta_{ij,n}$ be the $(i,j)$ element of $\var\left(\frac{1}{n}\mH_n^\top\mW_n\vy_n\right)$. Then
  \begin{equation*}
    \begin{aligned}
      \delta_{ij,n} & = n^{-2}\sum_{r = 1}^n\sum_{s = 1}^n h_{si,n}d_{sr,n}h_{rj,n}, \\
      \left|\delta_{ij, n}\right| & = n^{-2}\left|\sum_{r = 1}^n\sum_{s = 1}^n h_{si,n}d_{sr,n}h_{rj,n}\right|, \\
      \left|\delta_{ij, n}\right| & \leq n^{-2}\sum_{r = 1}^n\sum_{s = 1}^n \left|h_{si,n}d_{sr,n}h_{rj,n}\right| \; \mbox{by triangle inequality \ref{def:triangle-inequality}}, \\
       & \leq n^{-2}c_h^2\sum_{r = 1}^n\sum_{s = 1}^n  \left|d_{sr}\right|, \\
        & \leq n^{-2}c_h^2\sum_{r = 1}^n c_d, \\
        & \leq n^{-2}c_h^2c_dn, \\
        & = n^{-1}c_h^2c_d = o(1).
    \end{aligned}
  \end{equation*}
Since $\var\left(\frac{1}{n}\mH_n^\top\mW_n\vy_n\right)\to \vzeros$ as $n\to\infty$, then by Theorem \ref{teo:chebyshev}:
\begin{equation*}
\frac{1}{n}\mH_n^\top\mW_n\vy_n\pto \lim_{n\to\infty}n^{-1}\mH_n^\top\E(\mW_n\vy_n).
\end{equation*}
\end{proof}
%--------------------------

Given all these assumptions we can define the S2SLS estimator as follows.

%----------------------------------------------------------------------------------
\begin{definition}[Spatial Two Stage Least Square Estimator]
Let $\mH_n$ be the matrix $(n\times p)$ of instruments. Then the S2SLS is given by:
\begin{equation}\label{eq:2sls_estimator}
  \widehat{\vdelta}_{S2SLS} =  \left(\widehat{\mZ}_n^\top\mZ_n\right)^{-1}\widehat{\mZ}^\top_n\vy_n,
\end{equation}
%
where:
\begin{equation}\label{eq:2sls_first_stage}
\widehat{\mZ}_n = \mH_n(\mH^\top_n\mH_n)^{-1}\mH^\top_n\mZ_n = \mP_{H, n}\mZ_n,
\end{equation}
%
where the projection matrix $\mP_{H, n}$ is defined as
\begin{equation}\label{eq:Phn}
\mP_{H, n}= \mH_n(\mH^\top_n\mH_n)^{-1}\mH^\top_n.
\end{equation}

Alternatively, it can also be expressed as
\begin{equation}\label{eq:2sls_first_stage2}
\begin{aligned}
\widehat{\vdelta}_{S2SLS}  & =\left[\mZ_n^\top \mH_n\left(\mH_n^\top\mH_n\right)^{-1}\mH_n^\top\mZ_n\right]^{-1}\mZ_n^\top \mH_n\left(\mH_n^\top\mH_n\right)^{-1}\mH_n^\top\vy_n, \\
 & = \left[\mZ_n^\top\mP_{H,n}\mZ_n\right]^{-1}\mZ_n^\top\mP_{H,n}\vy_n. 
\end{aligned}
\end{equation}
\end{definition}
%----------------------------------------------------------------------------------

The S2SLS estimator in \eqref{eq:2sls_estimator} is conceptually similar to the standard 2SLS, with adjustment to account fro the spatial structure of the model. The estimation proceeds as follows:
\begin{enumerate}
  \item \textbf{First stage}: The first stage involves regressing the endogenous variables $\mZ_n$ on the instruments $\mH_n$ via OLS. This regression can be expressed as $\mZ_n = \mH_n\vtheta + \vxi_n$, where $\widehat{\vtheta}_n = (\mH^\top_n\mH_n)^{-1}\mH^\top_n\mZ_n$. The predicted values $\widehat{\mZ}_n$ are then obtained using Equation \eqref{eq:2sls_first_stage} where $\mP_{H,n}$ is the projection matrix, which symmetric and idempotent, and hence nonsingular. It is also important to note that the projection matrix does not affect $\mX_n$, but it does affect the endogenous variable $\mW_n\vy_n$:
\begin{equation}
  \mP_{H, n}\mZ_n = \left[\mX_n, \mP_{H, n}\mW_n\vy_n\right] = \left[\mX_n, \widehat{\mW_n\vy_n}\right].
\end{equation}

\item \textbf{Second stage}: In the second stage, the regression of $\vy_n$ on $\widehat{\mZ}_n$ is used to estimate the parameters, yielding the S2SLS estimator as defined in Equation \eqref{eq:2sls_estimator} or \eqref{eq:2sls_first_stage2}.
\end{enumerate}

%--------------------------------------------------------------
\subsection{Additional Endogenous Variables}\index{Endogeneity!additional endogenous variables}
%--------------------------------------------------------------

In the specification considered so far, the only endogenous variable has been the spatially lagged dependent variable $\mW\vy$. However, in practice, other explanatory variables may also exhibit endogeneity, necessitating additional instruments beyond the spatially lagged exogenous variables required for $\mW\vy$.

For example, \cite{anselin2008errors} analyzed the effect of improved air quality on house prices. Since air quality variables were derived using interpolated air pollution measures, they argued that these measures could suffer from an ``error in variable''\index{Endogeneity!error in variables} problem, introducing an additional source of endogeneity alongside the spatial lag. Specifically, they considered the following model:
\begin{equation*}
  y_i = \rho \sum_{j = 1}^nw_{ij}y_j + \vx_i'\vbeta + \gamma_1\texttt{pol}_i^1 + \gamma_2\texttt{pol}_i^2 + \epsilon_i, 
\end{equation*}
%
where $y_i$ is the house price,  $\vx_i$ is a vector of controls,  $\texttt{pol}_i^1$ and $\texttt{pol}_i^2$ are the air quality variables and $\epsilon_i$ is the error term. Since the actual pollution is not observed at locations $i$ of the house transaction, it is replaced by a spatially interpolated value, such as the result of a \textbf{kriging prediction}. This interpolated value measures the true pollution with error causing simultaneous equation bias, so they needed proper instruments for these variables. They instrumentalize these endogenous variables using the latitude, longitude and their product as the instruments. 

In particular, we can write the general model with additional endogenous variables
\begin{equation*}
  \vy = \rho\mW\vy + \mX_1\vbeta + \mY\vgamma + \vepsi,
\end{equation*}
%
where $\mY$ is a $n\times q$ matrix the endogenous explanatory variables and $\mX_1$ is a $n\times k_1$ matrix of exogenous variables. In a spatial lag model, an additional question is whether these instruments (for the endogenous explanatory variables) should be included in spatially lagged form as well, similar to what is done for the exogenous variables. As before, the rationale for this comes from the structure of the reduced form.  In this case the reduced form is given by:
\begin{equation*}
\E\left[\left.\mW\vy\right|\mZ\right] = \mW\left(\mI - \rho\mW\right)^{-1}\mX_1\vbeta + \mW\left(\mI - \rho\mW\right)^{-1}\mY\vgamma,
\end{equation*}
%
where $\mZ = \left[\mX_1, \mY\right]$. The problem here is that the $\mY$ are endogenous, and thus they do not belong on the right hand side of the reduced form!  If they are replaced by their instruments, then the presence of the term $\mW\left(\mI - \rho\mW\right)^{-1}$ would suggest the need for spatial lags to be included as well. In other words, since the system determining $\vy$ and $\mY$ is not completely specified, the optimal instruments are not known \citep{spdep}. If there exists a matrix $n \times k_1$ of additional pre-determined variables, say $\mX_2$, the instruments should be:
\begin{equation}
\mH = \left(\mX_1, \mW\mX_1, \ldots, \mW^l\mX_1, \mX_2, \mW\mX_2, \ldots, \mW^l\mX_2\right)_{LI}
\end{equation}

%--------------------------------------------------------------
\subsection{Consistency of S2SLS Estimator}\index{S2SLS!consistency}
%--------------------------------------------------------------

In this section, we provide a sketch of the proof for the consistency of the S2SLS estimator. For a formal proof see \cite{kelejian1998generalized} or \cite{kelejian2010specification}. To begin, recall that the $n\times n$ matrix  $\mH_n(\mH_n^\top\mH_n)^{-1}\mH_n^\top$ is symmetric and idempotent, implying that $\widehat{\mZ}_n^\top\mZ_n= \widehat{\mZ}_n^\top\widehat{\mZ}_n$. Using this property, we express the S2SLS estimator in terms of the population error term as follows: 
\begin{equation}\label{eq:sampling_error_s2sls}
  \begin{aligned}
     \widehat{\vdelta}_n & =  \vdelta_0 + \left(\widehat{\mZ}^\top_n\widehat{\mZ}_n\right)^{-1}\widehat{\mZ}^\top_n\vepsi_n, \\
     & = \vdelta_0 + \left[\left(\mH_n(\mH^\top_n\mH_n)^{-1}\mH^\top_n\mZ_n\right)^\top\left(\mH_n(\mH^\top_n\mH_n)^{-1}\mH^\top_n\mZ_n\right)\right]^{-1}\left(\mH_n(\mH^\top_n\mH_n)^{-1}\mH^\top_n\mZ_n\right)^\top\vepsi_n, \\
     & = \vdelta_0 + \left[\mZ^\top_n \mH_n (\mH^\top_n\mH_n)^{-1}\mH^\top_n\mZ_n\right]^{-1}\mZ^\top_n\mH_n(\mH^\top_n\mH_n)^{-1}\mH^\top_n\vepsi_n.
  \end{aligned}
\end{equation}
%
Solving for $\widehat{\vdelta}_{n} - \vdelta_0$, we obtain:
\begin{equation}\label{eq:asy_2sls_pr}
\begin{aligned}
(\widehat{\vdelta}_{n} - \vdelta_0) & = \left[\left(\frac{1}{n}\mH^\top_n\mZ _n\right)^\top\left(\frac{1}{n}\mH^\top_n\mH_n\right)^{-1}\left(\frac{1}{n}\mH^\top_n\mZ_n\right)\right]^{-1}\left(\frac{1}{n}\mH^\top_n\mZ_n\right)^\top\left(\frac{1}{n}\mH^\top_n\mH_n\right)^{-1}\left(\frac{1}{n}\mH^\top_n\vepsi_n\right), \\
             & = \widetilde{\mP}^\top_n\left(\frac{1}{n}\mH^\top_n\vepsi_n\right),
\end{aligned}
\end{equation}
where:
\begin{equation*}
  \widetilde{\mP}_n = \left(\frac{1}{n}\mH^\top_n\mH_n\right)^{-1}\left(\frac{1}{n}\mH_n^\top\mZ_n \right)\left[\left(\frac{1}{n}\mH^\top_n\mZ_n\right)^\top\left(\frac{1}{n}\mH^\top_n\mH_n\right)^{-1}\left(\frac{1}{n}\mH^\top_n\mZ_n\right)\right]^{-1}.
\end{equation*}

From Assumption \ref{assu:iv_instr_lim} (Limits of Instruments), we know that:
\begin{eqnarray*}
\lim_{n\to \infty} n^{-1}\mH_n^\top\mH_n &=& \mQ_{HH},\\
\plim_{n\to \infty} n^{-1}\mH_n^\top\mZ_n &=& \mQ_{HZ}.
\end{eqnarray*}

Therefore,  $\widetilde{\mP}_n\pto \mP_0$, where $\mP_0=\mQ_{HH}^{-1}\mQ_{HZ}\left(\mQ_{HZ}^\top\mQ_{HH}^{-1}\mQ_{HZ}\right)^{-1}$ is a finite matrix and exists because $\mQ_{HH}$ is invertible by Assumption \ref{assu:iv_instr_lim} and $(\mQ_{HZ}^\top\mQ_{HH}^{-1}\mQ_{HZ})^{-1}$ exists because $\mQ_{HZ}$ has full column rank by Assumption \ref{assu:iv_instr_lim}. Thus,
\begin{equation}\label{eq:P_conver}
\widetilde{\mP}_n - \mP_0 = o_p(1)\implies \widetilde{\mP}_n= \mP_0 + o_p(1), 
\end{equation}
%
where $\mP_0 = O(1)$ (why?).

By Assumption \ref{assu:regressors} and \ref{assu:bounded_matrix}, the elements of $\mH_n$ are uniformly bounded in absolute value by some finite constant. Assumption \ref{assu:errors_triang} (Heteroskedastic Errors) implies that $\epsilon_{i,n}$ forms a triangular array of identically distributed random variables. Furthermore, Assumption \ref{assu:errors_triang} states that  $\E(\vepsi_n) = \vzeros$ and $\var(\vepsi_n) = \mSigma_n =\Diag(\sigma^2_{i, n})$. Thus,
\begin{equation*}
  \begin{aligned}
    \E\left(\frac{1}{n}\mH^\top_n\vepsi_n\right)   & = \vzeros, \\
    \var\left(\frac{1}{n}\mH^\top_n\vepsi_n\right) & = \frac{1}{n^2}\mH^\top_n\mSigma_n\mH_n.
  \end{aligned}
\end{equation*}

Since $\var\left(\frac{1}{n}\mH^\top_n\vepsi_n\right)\to 0$ as $n\to \infty$,  and the elements of $\frac{1}{n}\mH_n^\top\mSigma_n\mH_n$ are uniformly bounded in absolute value (see proof \ref{proof:consistency-plimHQ}) by Chebyshev's Theorem \ref{teo:chebyshev}, $n^{-1}\mH^\top_n\vepsi_n\pto \vzeros$ and $\widehat{\vdelta}_n\pto \vdelta_0$.


%--------------------------------------------------------------
\subsection{Asymptotic Distribution of S2SLS Estimator}\index{S2SLS!Asymptotic distribution}
%--------------------------------------------------------------

Multiplying Equation (\ref{eq:asy_2sls_pr}) by $\sqrt{n}$ we obtain:
\begin{equation}\label{eq:asy_2sls_pr2}
\begin{aligned}
\sqrt{n}(\widehat{\vdelta}_{n} - \vdelta_0) & = \left[\left(\frac{1}{n}\mH^\top_n\mZ_n\right)^\top\left(\frac{1}{n}\mH^\top_n\mH_n\right)^{-1}\left(\frac{1}{n}\mH^\top_n\mZ_n\right)\right]^{-1}\left(\frac{1}{n}\mH^\top_n\mZ_n\right)^\top\left(\frac{1}{n}\mH^\top_n\mH_n\right)^{-1}\frac{1}{\sqrt{n}}\mH^\top_n\vepsi_n, \\
             & = \widetilde{\mP}^\top_n\left(\frac{1}{\sqrt{n}}\mH^\top_n\vepsi_n\right).
\end{aligned}
\end{equation}

From previous section we now that $\widetilde{\mP}_n\pto \mP_0$, where $\mP_0=\mQ_{HH}^{-1}\mQ_{HZ}\left(\mQ_{HZ}^\top\mQ_{HH}^{-1}\mQ_{HZ}\right)^{-1}$ is a finite matrix and exists. By Assumption \ref{assu:bounded_matrix} and \ref{assu:regressors}, the elements of $\mH_n$ are uniformly bounded in absolute value by some finite constant. Then, by CLT for triangular arrays with heteroskedastic innovations \ref{teo:clt_quadratic}:
\begin{equation*}
\frac{1}{\sqrt{n}}\mH_n^\top\vepsi_n \dto \rN\left(\vzeros, \mDelta_0\right),
\end{equation*}
%
where $\mDelta_0 = \lim_{n\to\infty} \frac{1}{n}\mH^\top_n\mSigma_n\mH_n$. 

Finally :
\begin{equation*}
\sqrt{n}(\widehat{\vdelta}_{n} - \vdelta_0) \dto \rN(\vzeros, \mOmega_0), 
\end{equation*}
%
where
\begin{equation*}
  \begin{aligned}
      \mOmega_0& = \left(\mQ_{HZ}^\top\mQ_{HH}^{-1}\mQ_{HZ}\right)^{-1}\mQ_{HZ}^\top\mQ_{HH}^{-1}\mDelta_0 \mQ_{HH}^{-1}\mQ_{HZ}\left(\mQ_{HZ}^\top\mQ_{HH}^{-1}\mQ_{HZ}\right)^{-1}.
  \end{aligned}
\end{equation*}

If the error terms are homoskedatic, then by CLT for triangular arrays with homoskedastic innovations \ref{teo:CLT_tri_arr}:
\begin{equation*}
\frac{1}{\sqrt{n}}\mH_n^\top\vepsi_n \dto \rN\left(\vzeros, \sigma^2_0\mQ_{HH}\right),
\end{equation*}
%
where $\mQ_{HH} = \lim_{n \to \infty}n^{-1}\mH_n^\top\mH_n$. Thus:
\begin{equation*}
\sqrt{n}(\widehat{\vdelta}_{n} - \vdelta_0) \dto \rN(\vzeros, \mOmega_0^o),
\end{equation*}
%
where 
\begin{equation*}
\mOmega_0^o = \sigma_{0}^2\left(\mQ_{HZ}^\top\mQ_{HH}^{-1}\mQ_{HZ}\right)^{-1}.
\end{equation*}


Now, we present a formal Theorem for the asymptotic properties of the 2SLS Estimator for SLM.

%-------------------------------------------------------------------------
\begin{theorem}[Spatial 2SLS Estimator for SLM]\label{teo:S2SLS_est_slm}
  Suppose that Assumptions \ref{assu:errors_triang}, \ref{assu:diag_W},  \ref{assu:non_singularity}, \ref{assu:bounded_matrix}, \ref{assu:regressors}, \ref{assu:iv_instr}, \ref{assu:iv_instr_lim} hold. Then the S2SLS estimator defined as
  \begin{equation*}
    \widehat{\vdelta}_n = \left(\widehat{\mZ}_n^\top\widehat{\mZ}_n\right)^{-1}\widehat{\mZ}_n^\top\vy_n, 
  \end{equation*}
%
is consistent, and its asymptotic distribution is:
\begin{equation*}
\sqrt{n}(\widehat{\vdelta}_{n} - \vdelta_0) \dto \rN(\vzeros, \mOmega_0)
\end{equation*}
%
where
\begin{equation}\label{eq:var-cov-het-2sls}
\mOmega_0 = \left(\mQ_{HZ}^\top\mQ_{HH}^{-1}\mQ_{HZ}\right)^{-1}\mQ_{HZ}^\top\mQ_{HH}^{-1}\mDelta_0 \mQ_{HH}^{-1}\mQ_{HZ}\left(\mQ_{HZ}^\top\mQ_{HH}^{-1}\mQ_{HZ}\right)^{-1}, 
\end{equation}
%
with 
\begin{equation*}
\begin{aligned}
 \mQ_{HH} = &  \lim_{n\to \infty} n^{-1}\mH_n^\top\mH_n,\\
\mQ_{HZ}  = &  \plim_{n\to \infty} n^{-1}\mH_n^\top\mZ_n, \\
\mDelta_0 = & \lim_{n\to \infty} n^{-1}\mH_n^\top \mSigma_n \mH_n.
\end{aligned}
\end{equation*}

If Assumption \ref{assu:errors_triang} is replaced by Assumption \ref{assu:errors_homoskedastic}, then
\begin{equation*}
\sqrt{n}(\widehat{\vdelta}_{n} - \vdelta_0) \dto \rN(\vzeros, \mOmega_0^o),
\end{equation*}
%
where 
\begin{equation}\label{eq:var-cov-homo-2sls}
\mOmega_0^o = \sigma_{0}^2\left(\mQ_{HZ}^\top\mQ_{HH}^{-1}\mQ_{HZ}\right)^{-1}.
\end{equation}
\end{theorem}
%-------------------------------------------------------------------------

The variance-covariance under homoskedasticity in Equation \eqref{eq:var-cov-het-2sls} can be estimated as:
\begin{equation}\label{eq:v-s2sls-hetero}
\begin{aligned}
\widehat{\mOmega}_n = &  \left[\left(\frac{1}{n}\mH_n^\top\mZ_n\right)^\top\left(\frac{1}{n}\mH_n^\top\mH_n\right)^{-1}\left(\frac{1}{n}\mH_n^\top\mZ_n\right)\right]^{-1} \\
 & \times \left[\left(\frac{1}{n}\mH_n^\top\mZ_n\right)^\top\left(\frac{1}{n}\mH_n^\top\mH_n\right)^{-1}\widehat{\mDelta}_n\left(\frac{1}{n}\mH_n^\top\mH_n\right)^{-1}\left(\frac{1}{n}\mH_n^\top\mZ_n\right)\right] \\
 & \times \left[\left(\frac{1}{n}\mH_n^\top\mZ_n\right)^\top\left(\frac{1}{n}\mH_n^\top\mH_n\right)^{-1}\left(\frac{1}{n}\mH_n^\top\mZ_n\right)\right]^{-1},
\end{aligned}
\end{equation}
%
where 
\begin{equation}\label{eq:2sls-delta-hat}
\widehat{\mDelta}_n = \frac{1}{n}\sum_{i = 1}^n\widehat{\epsilon}_i^2\vh_i\vh_i^\top, 
\end{equation}
%
where $\vh_i$ is the $p\times 1$ vector of instruments for $i$ observation. This is unknown as the White's, Robust or Sandwich estimator. 

Under homoskedasticity, the variance-covariance matrix is estimated as
\begin{equation}\label{eq:v-s2sls-homo}
\widehat{\mOmega}^o = \widehat{\sigma}^2_{n}\left[\left(\frac{1}{n}\mH_n^\top\mZ_n\right)^\top\left(\frac{1}{n}\mH_n^\top\mH_n\right)^{-1}\left(\frac{1}{n}\mH_n^\top\mZ_n\right)\right]^{-1},
\end{equation}
%
where:
\begin{equation}\label{eq:2sls-sigma-hat}
  \widehat{\sigma}^2_n = \frac{\widehat{\vepsi}_n^\top\widehat{\vepsi}_n}{n},\quad \widehat{\vepsi}_n = \vy_n - \widehat{\vy}_n.
\end{equation}

%==================================
\subsection{Coding S2SLS in R}\index{S2SLS!stsls function}\index{S2SLS!example}
%==================================

In this section, we demonstrate how to implement a custom function to estimate the S2SLS estimator. This implementation includes creating the instrument matrix, $\mH$, estimating the model parameters, and providing variance-covariance matrix options for homoskedastic and robust errors. Additionally, we compare our implementation with the \texttt{stsls} function from the \textbf{spatialreg} package \citep{spatialregcit} to ensure consistency.


We start by defining a function to create the instrument matrix $\mH$, which includes spatial lags of the exogenous variables up to a user-specified order.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Function that creates WXs}
\hlstd{make.H} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{W}\hlstd{,} \hlkwc{X}\hlstd{,} \hlkwc{l} \hlstd{=} \hlnum{3}\hlstd{)\{}
  \hlcom{# This function creates the instruments (WX, ...,W^lX)}
  \hlcom{# Drop constant (if any)}
  \hlstd{names.x} \hlkwb{<-} \hlkwd{colnames}\hlstd{(X)}
  \hlkwa{if} \hlstd{(names.x[}\hlnum{1}\hlstd{]} \hlopt{==} \hlstr{"(Intercept)"}\hlstd{) X} \hlkwb{<-} \hlkwd{matrix}\hlstd{(X[ ,}\hlopt{-}\hlnum{1}\hlstd{],}
                                               \hlkwd{dim}\hlstd{(X)[}\hlnum{1}\hlstd{],}
                                               \hlkwd{dim}\hlstd{(X)[}\hlnum{2}\hlstd{]} \hlopt{-} \hlnum{1}\hlstd{)} \hlcom{#Drop first column}
  \hlstd{names.x} \hlkwb{<-} \hlstd{names.x[}\hlkwd{which}\hlstd{(names.x} \hlopt{!=} \hlstr{"(Intercept)"}\hlstd{)]}
  \hlcom{# Create lagged X variables}
  \hlstd{sq1} \hlkwb{<-} \hlkwd{seq}\hlstd{(}\hlnum{1}\hlstd{,} \hlkwd{ncol}\hlstd{(X)} \hlopt{*} \hlstd{l,} \hlkwd{ncol}\hlstd{(X))}
  \hlstd{sq2} \hlkwb{<-} \hlkwd{seq}\hlstd{(}\hlkwd{ncol}\hlstd{(X),} \hlkwd{ncol}\hlstd{(X)} \hlopt{*} \hlstd{l,} \hlkwd{ncol}\hlstd{(X))}
  \hlstd{Hmat} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{NA}\hlstd{,} \hlkwc{nrow} \hlstd{=} \hlkwd{nrow}\hlstd{(X),} \hlkwc{ncol} \hlstd{=} \hlkwd{ncol}\hlstd{(X)} \hlopt{*} \hlstd{l)}
  \hlstd{names.ins} \hlkwb{<-} \hlkwd{c}\hlstd{()}
  \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{l) \{}
    \hlstd{Hmat[, sq1[i]}\hlopt{:}\hlstd{sq2[i]]} \hlkwb{<-} \hlkwd{as.matrix}\hlstd{(W} \hlopt{%*%} \hlstd{X)}
    \hlstd{X} \hlkwb{<-} \hlstd{Hmat[, sq1[i]}\hlopt{:}\hlstd{sq2[i]]}
    \hlstd{names.ins} \hlkwb{<-} \hlkwd{c}\hlstd{(names.ins,}
                   \hlkwd{paste}\hlstd{(}\hlkwd{paste}\hlstd{(}\hlkwd{replicate}\hlstd{(i,} \hlstr{"W"}\hlstd{),} \hlkwc{collapse} \hlstd{=} \hlstr{""}\hlstd{),}
                         \hlstd{names.x,} \hlkwc{sep} \hlstd{=} \hlstr{"*"}\hlstd{))}
  \hlstd{\}}
  \hlkwd{colnames}\hlstd{(Hmat)} \hlkwb{<-} \hlstd{names.ins}
  \hlkwd{return}\hlstd{(Hmat)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

This function takes as arguments the spatial weight matrix \texttt{W}, which is assumed to be of class \texttt{matrix}, the $n\times K$ matrix of exogenous variables, \texttt{X}, and order of spatial lags \texttt{l}. Then, it provides the matrix of instruments, $\mW\mX, \mW^2\mX^2, \ldots, \mW^l\mX^l$, recursively. Note that the spatial lag of the constant is not included (why?). 

We create an artificial DGP similar to \cite{lee2007gmm} for testing the S2SLS implementation:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Generate DGP}
\hlkwd{library}\hlstd{(}\hlstr{"spatialreg"}\hlstd{)}
\end{alltt}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Loading required package: spData}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# To access larger datasets in this package, install the spDataLarge\\\#\# package with: `install.packages('spDataLarge',\\\#\# repos='https://nowosad.github.io/drat/', type='source')`}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Loading required package: Matrix}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Loading required package: sf}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Linking to GEOS 3.10.2, GDAL 3.4.2, PROJ 8.2.1; sf\_use\_s2() is TRUE}}\begin{alltt}
\hlkwd{library}\hlstd{(}\hlstr{"spdep"}\hlstd{)}
\end{alltt}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# Attaching package: 'spdep'}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# The following objects are masked from 'package:spatialreg':\\\#\# \\\#\# \ \ \ \ get.ClusterOption, get.coresOption, get.mcOption,\\\#\# \ \ \ \ get.VerboseOption, get.ZeroPolicyOption, set.ClusterOption,\\\#\# \ \ \ \ set.coresOption, set.mcOption, set.VerboseOption,\\\#\# \ \ \ \ set.ZeroPolicyOption}}\begin{alltt}
\hlkwd{set.seed}\hlstd{(}\hlnum{1986}\hlstd{)}
\hlstd{n}      \hlkwb{<-} \hlnum{529}
\hlstd{rho}    \hlkwb{<-} \hlnum{0.6}
\hlstd{W.nb2}  \hlkwb{<-} \hlkwd{cell2nb}\hlstd{(}\hlkwd{sqrt}\hlstd{(n),} \hlkwd{sqrt}\hlstd{(n))}
\hlstd{W}      \hlkwb{<-} \hlkwd{nb2mat}\hlstd{(W.nb2)}

\hlcom{# Exogenous variables}
\hlstd{x1}     \hlkwb{<-} \hlkwd{rnorm}\hlstd{(n)}
\hlstd{x2}     \hlkwb{<-} \hlkwd{rnorm}\hlstd{(n)}
\hlstd{x3}     \hlkwb{<-} \hlkwd{rnorm}\hlstd{(n)}

\hlcom{# DGP parameters}
\hlstd{b0} \hlkwb{<-} \hlnum{0} \hlstd{; b1} \hlkwb{<-} \hlopt{-}\hlnum{1}\hlstd{; b2} \hlkwb{<-} \hlnum{0}\hlstd{; b3} \hlkwb{<-} \hlnum{1}
\hlstd{sigma2} \hlkwb{<-} \hlnum{2}
\hlstd{epsilon} \hlkwb{<-} \hlkwd{rnorm}\hlstd{(n,} \hlkwc{mean} \hlstd{=} \hlnum{0}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlkwd{sqrt}\hlstd{(sigma2))}

\hlcom{# Simulate the dependent variable}
\hlstd{y} \hlkwb{<-} \hlkwd{solve}\hlstd{(}\hlkwd{diag}\hlstd{(n)} \hlopt{-}  \hlstd{rho} \hlopt{*} \hlstd{W)} \hlopt{%*%} \hlstd{(b0} \hlopt{+} \hlstd{b1}\hlopt{*}\hlstd{x1} \hlopt{+} \hlstd{b2}\hlopt{*}\hlstd{x2} \hlopt{+} \hlstd{b3}\hlopt{*}\hlstd{x3} \hlopt{+} \hlstd{epsilon)}

\hlcom{# Data as data.frame}
\hlstd{data} \hlkwb{<-} \hlkwd{as.data.frame}\hlstd{(}\hlkwd{cbind}\hlstd{(y, x1, x2, x3))}
\hlkwd{names}\hlstd{(data)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"y"}\hlstd{,} \hlstr{"x1"}\hlstd{,} \hlstr{"x2"}\hlstd{,} \hlstr{"x3"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

In the following lines, we show how the function \texttt{make.H} works using $l = 3$. 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{X} \hlkwb{<-} \hlkwd{cbind}\hlstd{(}\hlnum{1}\hlstd{, x1, x2, x3)}
\hlkwd{colnames}\hlstd{(X)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"(Intercept)"}\hlstd{,} \hlstr{"x1"}\hlstd{,} \hlstr{"x2"}\hlstd{,} \hlstr{"x3"}\hlstd{)}
\hlstd{H} \hlkwb{<-} \hlkwd{make.H}\hlstd{(}\hlkwc{W} \hlstd{= W,} \hlkwc{X} \hlstd{= X,} \hlkwc{l} \hlstd{=} \hlnum{3}\hlstd{)}
\hlkwd{head}\hlstd{(H)}
\end{alltt}
\begin{verbatim}
##            W*x1       W*x2        W*x3       WW*x1       WW*x2       WW*x3
## [1,] -0.1838084  1.4871684  1.06237375 -0.19093708 -0.66721937  0.11419352
## [2,] -0.1734224 -0.4801317  0.00889001 -0.18346010  1.16724914  0.84907535
## [3,] -0.1376933  0.7858546  0.65742663 -0.07600407 -0.06559888 -0.33095264
## [4,]  0.2516524  0.5410386 -0.66094372 -0.17572619  0.04624316  0.12822625
## [5,] -0.2524966 -0.7357899 -0.27611007  0.53392070  0.46568956 -0.40883400
## [6,]  0.7127103  0.4896574 -0.25597660 -0.08935028 -0.65274040 -0.05759884
##          WWW*x1     WWW*x2      WWW*x3
## [1,] -0.2364339  1.1506372  0.80406622
## [2,] -0.1724420 -0.4360736 -0.07942842
## [3,] -0.1916439  0.6032002  0.46356720
## [4,]  0.2365006  0.1309099 -0.37543431
## [5,] -0.1367966 -0.4243089 -0.02981086
## [6,]  0.4918535  0.2858139 -0.35198454
\end{verbatim}
\end{kframe}
\end{knitrout}

Thus, the matrix \texttt{H} contains the spatial lag of the exogenous variables up to the third order. 

The main function for estimating the S2SLS model is defined as follows:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Main function to estimate S2SLS estimator}
\hlstd{slm.2sls} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{formula}\hlstd{,} \hlkwc{data}\hlstd{,} \hlkwc{W}\hlstd{,} \hlkwc{instruments} \hlstd{=} \hlnum{2}\hlstd{)\{}
  \hlcom{# Model Frame}
  \hlstd{callT}    \hlkwb{<-} \hlkwd{match.call}\hlstd{(}\hlkwc{expand.dots} \hlstd{=} \hlnum{TRUE}\hlstd{)}
  \hlstd{mf}       \hlkwb{<-} \hlstd{callT}
  \hlstd{m}        \hlkwb{<-} \hlkwd{match}\hlstd{(}\hlkwd{c}\hlstd{(}\hlstr{"formula"}\hlstd{,} \hlstr{"data"}\hlstd{),} \hlkwd{names}\hlstd{(mf),} \hlnum{0L}\hlstd{)}
  \hlstd{mf}       \hlkwb{<-} \hlstd{mf[}\hlkwd{c}\hlstd{(}\hlnum{1L}\hlstd{, m)]}
  \hlstd{mf[[}\hlnum{1L}\hlstd{]]} \hlkwb{<-} \hlkwd{as.name}\hlstd{(}\hlstr{"model.frame"}\hlstd{)}
  \hlstd{mf}       \hlkwb{<-} \hlkwd{eval}\hlstd{(mf,} \hlkwd{parent.frame}\hlstd{())}

  \hlcom{# Get variables and globals}
  \hlstd{y}  \hlkwb{<-} \hlkwd{model.response}\hlstd{(mf)}
  \hlstd{X}  \hlkwb{<-} \hlkwd{model.matrix}\hlstd{(formula, mf)}
  \hlstd{n}  \hlkwb{<-} \hlkwd{nrow}\hlstd{(X)}
  \hlstd{Wy} \hlkwb{<-} \hlstd{W} \hlopt{%*%} \hlstd{y}
  \hlstd{sn} \hlkwb{<-} \hlkwd{nrow}\hlstd{(W)}
  \hlkwa{if} \hlstd{(n} \hlopt{!=} \hlstd{sn)} \hlkwd{stop}\hlstd{(}\hlstr{"number of spatial units in W is different to the number of data"}\hlstd{)}

  \hlcom{# Generate matrix of instruments H = [X, WX, ... ]}
  \hlcom{# and select LI vars}
  \hlstd{H} \hlkwb{<-} \hlkwd{cbind}\hlstd{(X,} \hlkwd{make.H}\hlstd{(}\hlkwc{W} \hlstd{= W,} \hlkwc{X} \hlstd{= X,} \hlkwc{l} \hlstd{= instruments))}
  \hlstd{H} \hlkwb{<-} \hlstd{H[,} \hlkwd{qr}\hlstd{(H)}\hlopt{$}\hlstd{pivot[}\hlkwd{seq_len}\hlstd{(}\hlkwd{qr}\hlstd{(H)}\hlopt{$}\hlstd{rank)]]}

  \hlcom{# Get S2SLS estimates}
  \hlstd{Z}           \hlkwb{<-} \hlkwd{cbind}\hlstd{(X, Wy)}
  \hlkwd{colnames}\hlstd{(Z)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlkwd{colnames}\hlstd{(X),} \hlstr{"Wy"}\hlstd{)}

  \hlcom{# Create projection matrix}
  \hlstd{HH}    \hlkwb{<-} \hlkwd{crossprod}\hlstd{(H)}
  \hlstd{PH}    \hlkwb{<-} \hlstd{H} \hlopt{%*%} \hlkwd{solve}\hlstd{(HH)} \hlopt{%*%} \hlkwd{t}\hlstd{(H)}

  \hlcom{# Compute S2SLS coefficients}
  \hlstd{Z_hat}  \hlkwb{<-} \hlstd{PH} \hlopt{%*%} \hlstd{Z}
  \hlstd{b_2sls} \hlkwb{<-} \hlkwd{solve}\hlstd{(}\hlkwd{crossprod}\hlstd{(Z_hat))} \hlopt{%*%} \hlkwd{crossprod}\hlstd{(Z_hat, y)}
  \hlstd{y_hat}  \hlkwb{<-} \hlstd{Z} \hlopt{%*%} \hlstd{b_2sls}
  \hlstd{e_hat}  \hlkwb{<-} \hlstd{y} \hlopt{-} \hlstd{y_hat}

  \hlcom{# Save results}
  \hlstd{results} \hlkwb{<-} \hlkwd{structure}\hlstd{(}
    \hlkwd{list}\hlstd{(}
      \hlkwc{coefficients} \hlstd{= b_2sls,}
      \hlkwc{call}         \hlstd{= callT,}
      \hlkwc{X}            \hlstd{= X,}
      \hlkwc{H}            \hlstd{= H,}
      \hlkwc{Z}            \hlstd{= Z,}
      \hlkwc{y}            \hlstd{= y,}
      \hlkwc{PH}           \hlstd{= PH,}
      \hlkwc{e_hat}        \hlstd{= e_hat}
    \hlstd{),}
    \hlkwc{class} \hlstd{=} \hlstr{'mys2sls'}
  \hlstd{)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

The function \texttt{slm.2sls} provides the S2SLS estimates using the matrix of instruments $\mH$ suggested by \cite{kelejian1998generalized}. The 2SLS estimates are obtained using Equation \eqref{eq:2sls_estimator}. It then returns an object of class \texttt{mys2sls} along with some elements as a list such as the estimated coefficients, the call of the model, and different matrix that can be used in other functions. 

In the next lines, we create the S3 method \texttt{vcov} for an object with class \texttt{mys2sls}:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# S3 Method vcov}
\hlstd{vcov.mys2sls} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{object}\hlstd{,} \hlkwc{tse} \hlstd{=} \hlkwd{c}\hlstd{(}\hlstr{"homo"}\hlstd{,} \hlstr{"rob"}\hlstd{),} \hlkwc{...}\hlstd{)\{}
    \hlstd{tse}    \hlkwb{<-} \hlkwd{match.arg}\hlstd{(tse)}
    \hlstd{n}      \hlkwb{<-} \hlkwd{nrow}\hlstd{(object}\hlopt{$}\hlstd{Z)}
    \hlstd{df}     \hlkwb{<-} \hlstd{n} \hlopt{-} \hlkwd{ncol}\hlstd{(object}\hlopt{$}\hlstd{Z)}
    \hlstd{Q.HZ}   \hlkwb{<-} \hlstd{(}\hlkwd{t}\hlstd{(object}\hlopt{$}\hlstd{H)} \hlopt{%*%} \hlstd{object}\hlopt{$}\hlstd{Z)} \hlopt{/} \hlstd{n}
    \hlstd{Q.HH.i} \hlkwb{<-} \hlkwd{solve}\hlstd{(}\hlkwd{crossprod}\hlstd{(object}\hlopt{$}\hlstd{H)} \hlopt{/} \hlstd{n)}
    \hlkwa{if} \hlstd{(tse} \hlopt{==} \hlstr{"homo"}\hlstd{)\{}
      \hlstd{s2}  \hlkwb{<-} \hlkwd{crossprod}\hlstd{(object}\hlopt{$}\hlstd{e_hat)} \hlopt{/} \hlstd{df}
      \hlstd{var} \hlkwb{<-} \hlkwd{drop}\hlstd{(s2)} \hlopt{*} \hlkwd{solve}\hlstd{(}\hlkwd{t}\hlstd{(Q.HZ)} \hlopt{%*%} \hlstd{Q.HH.i} \hlopt{%*%} \hlstd{Q.HZ)} \hlopt{/} \hlstd{n}
    \hlstd{\}} \hlkwa{else} \hlstd{\{}
      \hlstd{Delta.hat} \hlkwb{<-} \hlnum{0}
      \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{nrow}\hlstd{(object}\hlopt{$}\hlstd{Z))\{}
        \hlstd{Delta.hat} \hlkwb{<-} \hlstd{Delta.hat} \hlopt{+} \hlkwd{drop}\hlstd{(object}\hlopt{$}\hlstd{e_hat[i]} \hlopt{^}\hlnum{2}\hlstd{)} \hlopt{*} \hlkwd{tcrossprod}\hlstd{(object}\hlopt{$}\hlstd{H[i, ])}
      \hlstd{\}}
      \hlstd{bread}  \hlkwb{<-} \hlkwd{solve}\hlstd{(}\hlkwd{t}\hlstd{(Q.HZ)} \hlopt{%*%} \hlstd{Q.HH.i} \hlopt{%*%} \hlstd{Q.HZ)}
      \hlstd{cheese} \hlkwb{<-} \hlkwd{t}\hlstd{(Q.HZ)} \hlopt{%*%} \hlstd{Q.HH.i} \hlopt{%*%} \hlstd{(Delta.hat} \hlopt{/} \hlstd{n)}  \hlopt{%*%} \hlstd{Q.HH.i} \hlopt{%*%} \hlstd{Q.HZ}
      \hlstd{var}    \hlkwb{<-} \hlstd{(bread} \hlopt{%*%} \hlstd{cheese} \hlopt{%*%} \hlstd{bread)} \hlopt{/} \hlstd{n}
    \hlstd{\}}
    \hlkwd{return}\hlstd{(var)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

This function computes the estimated variance-covariance (VC) matrix for the S2SLS estimators. If \texttt{tse = "homo"}, it returns the VC matrix under homoskedastic error, following Equation \eqref{eq:v-s2sls-homo}. In this case, $\widehat{\sigma}^2_n$ is estimated using Equation \eqref{eq:2sls-sigma-hat}, applying a small-sample by dividing by $n-k$ instead of $n$. If \texttt{tse = "rob"}, the function returns the robust VC matrix, estimated using Equation \eqref{eq:v-s2sls-hetero}, where $\widehat{\mDelta}$ is computed according to  Equation \eqref{eq:2sls-delta-hat}. In both cases, the VC matrices are divided by $n$ for finite sample approximation.

In the following lines, we code the S3 functions \texttt{summary} and \texttt{print.summary} for the class of our model.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# S3 method summary}
\hlstd{summary.mys2sls} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{object}\hlstd{,}
                              \hlkwc{tse} \hlstd{=} \hlkwd{c}\hlstd{(}\hlstr{"homo"}\hlstd{,} \hlstr{"rob"}\hlstd{),}
                              \hlkwc{table} \hlstd{=} \hlnum{TRUE}\hlstd{,}
                              \hlkwc{digits} \hlstd{=} \hlkwd{max}\hlstd{(}\hlnum{3}\hlstd{, .Options}\hlopt{$}\hlstd{digits} \hlopt{-} \hlnum{3}\hlstd{),}
                              \hlkwc{...}\hlstd{)\{}
    \hlstd{tse}       \hlkwb{<-} \hlkwd{match.arg}\hlstd{(tse)}
    \hlstd{n}         \hlkwb{<-} \hlkwd{nrow}\hlstd{(object}\hlopt{$}\hlstd{Z)}
    \hlstd{df}        \hlkwb{<-} \hlstd{n} \hlopt{-} \hlkwd{ncol}\hlstd{(object}\hlopt{$}\hlstd{Z)}
    \hlstd{b}         \hlkwb{<-} \hlstd{object}\hlopt{$}\hlstd{coefficients}
    \hlstd{std.err}   \hlkwb{<-} \hlkwd{sqrt}\hlstd{(}\hlkwd{diag}\hlstd{(}\hlkwd{vcov}\hlstd{(object,} \hlkwc{tse} \hlstd{= tse)))}
    \hlstd{z}         \hlkwb{<-} \hlstd{b} \hlopt{/} \hlstd{std.err}
    \hlstd{p}         \hlkwb{<-} \hlnum{2} \hlopt{*} \hlkwd{pt}\hlstd{(}\hlopt{-}\hlkwd{abs}\hlstd{(z),} \hlkwc{df} \hlstd{= df)}
    \hlstd{CoefTable} \hlkwb{<-} \hlkwd{cbind}\hlstd{(b, std.err, z, p)}
    \hlkwd{colnames}\hlstd{(CoefTable)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"Estimate"}\hlstd{,} \hlstr{"Std.Error"}\hlstd{,} \hlstr{"t-value"}\hlstd{,} \hlstr{"Pr(>|t|)"}\hlstd{)}
    \hlstd{result} \hlkwb{<-} \hlkwd{structure}\hlstd{(}
      \hlkwd{list}\hlstd{(}
        \hlkwc{CoefTable} \hlstd{= CoefTable,}
        \hlkwc{digits}    \hlstd{= digits,}
        \hlkwc{call}      \hlstd{= object}\hlopt{$}\hlstd{call),}
      \hlkwc{class} \hlstd{=} \hlstr{'summary.mys2sls'}
    \hlstd{)}
    \hlkwd{return}\hlstd{(result)}
\hlstd{\}}

\hlcom{# S3 method print.summary }
\hlstd{print.summary.mys2sls} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{,}
                                   \hlkwc{digits} \hlstd{= x}\hlopt{$}\hlstd{digits,}
                                   \hlkwc{na.print} \hlstd{=} \hlstr{""}\hlstd{,}
                                   \hlkwc{symbolic.cor} \hlstd{= p} \hlopt{>} \hlnum{4}\hlstd{,}
                                   \hlkwc{signif.stars} \hlstd{=} \hlkwd{getOption}\hlstd{(}\hlstr{"show.signif.stars"}\hlstd{),}
                                   \hlkwc{...}\hlstd{)}
\hlstd{\{}
  \hlkwd{cat}\hlstd{(}\hlstr{"\textbackslash{}nCall:\textbackslash{}n"}\hlstd{)}
  \hlkwd{cat}\hlstd{(}\hlkwd{paste}\hlstd{(}\hlkwd{deparse}\hlstd{(x}\hlopt{$}\hlstd{call),} \hlkwc{sep} \hlstd{=} \hlstr{"\textbackslash{}n"}\hlstd{,} \hlkwc{collapse} \hlstd{=} \hlstr{"\textbackslash{}n"}\hlstd{),} \hlstr{"\textbackslash{}n\textbackslash{}n"}\hlstd{,} \hlkwc{sep} \hlstd{=} \hlstr{""}\hlstd{)}

  \hlkwd{cat}\hlstd{(}\hlstr{"\textbackslash{}nCoefficients:\textbackslash{}n"}\hlstd{)}
  \hlkwd{printCoefmat}\hlstd{(x}\hlopt{$}\hlstd{CoefTable,} \hlkwc{digit} \hlstd{= digits,} \hlkwc{P.value} \hlstd{=} \hlnum{TRUE}\hlstd{,} \hlkwc{has.Pvalue} \hlstd{=} \hlnum{TRUE}\hlstd{)}
  \hlkwd{invisible}\hlstd{(}\hlkwa{NULL}\hlstd{)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

Now, we can apply our function to estimate the S2SLS model using homoskedastic and robust standard errors. By default, the \texttt{summary} method reports results with homoskedastic standard errors. To obtain robust standard errors, we specify \texttt{tse = "rob"} in the \texttt{summary} function.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Estimate S2SLS model}
\hlstd{s2sls.e} \hlkwb{<-} \hlkwd{slm.2sls}\hlstd{(y} \hlopt{~} \hlstd{x1} \hlopt{+} \hlstd{x2} \hlopt{+} \hlstd{x3,} \hlkwc{data} \hlstd{= data,} \hlkwc{W} \hlstd{= W,} \hlkwc{instruments} \hlstd{=} \hlnum{2}\hlstd{)}
\hlkwd{summary}\hlstd{(s2sls.e)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## slm.2sls(formula = y ~ x1 + x2 + x3, data = data, W = W, instruments = 2)
## 
## 
## Coefficients:
##             Estimate Std.Error t-value Pr(>|t|)    
## (Intercept) -0.06983   0.06656  -1.049    0.295    
## x1          -1.00574   0.06347 -15.846   <2e-16 ***
## x2          -0.04353   0.06276  -0.694    0.488    
## x3           1.01485   0.06720  15.102   <2e-16 ***
## Wy           0.64731   0.06222  10.404   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}
\begin{alltt}
\hlkwd{summary}\hlstd{(s2sls.e,} \hlkwc{tse} \hlstd{=} \hlstr{"rob"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## slm.2sls(formula = y ~ x1 + x2 + x3, data = data, W = W, instruments = 2)
## 
## 
## Coefficients:
##             Estimate Std.Error t-value Pr(>|t|)    
## (Intercept) -0.06983   0.06682  -1.045    0.296    
## x1          -1.00574   0.06587 -15.269   <2e-16 ***
## x2          -0.04353   0.06290  -0.692    0.489    
## x3           1.01485   0.06669  15.217   <2e-16 ***
## Wy           0.64731   0.05892  10.986   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}
\end{kframe}
\end{knitrout}

Now we check our results using \texttt{stsls} function:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{spreg1} \hlkwb{<-} \hlkwd{stsls}\hlstd{(y} \hlopt{~} \hlstd{x1} \hlopt{+} \hlstd{x2} \hlopt{+} \hlstd{x3,} \hlkwc{data} \hlstd{= data,} \hlkwc{listw} \hlstd{=} \hlkwd{mat2listw}\hlstd{(W,} \hlkwc{style} \hlstd{=} \hlstr{"W"}\hlstd{))}
\hlstd{spreg2} \hlkwb{<-} \hlkwd{stsls}\hlstd{(y} \hlopt{~} \hlstd{x1} \hlopt{+} \hlstd{x2} \hlopt{+} \hlstd{x3,} \hlkwc{data} \hlstd{= data,} \hlkwc{listw} \hlstd{=} \hlkwd{mat2listw}\hlstd{(W,} \hlkwc{style} \hlstd{=} \hlstr{"W"}\hlstd{),}
                \hlkwc{robust} \hlstd{=} \hlnum{TRUE}\hlstd{,} \hlkwc{HC} \hlstd{=} \hlstr{"HC0"}\hlstd{)}
\hlkwd{cbind}\hlstd{(}\hlkwd{as.numeric}\hlstd{(}\hlkwd{coef}\hlstd{(spreg1)[}\hlkwd{c}\hlstd{(}\hlnum{2}\hlstd{,} \hlnum{3}\hlstd{,} \hlnum{4}\hlstd{,} \hlnum{5}\hlstd{,} \hlnum{1}\hlstd{)]),} \hlkwd{as.numeric}\hlstd{(s2sls.e}\hlopt{$}\hlstd{coefficients))}
\end{alltt}
\begin{verbatim}
##             [,1]        [,2]
## [1,] -0.06983166 -0.06983166
## [2,] -1.00573883 -1.00573883
## [3,] -0.04352912 -0.04352912
## [4,]  1.01485440  1.01485440
## [5,]  0.64730746  0.64730746
\end{verbatim}
\begin{alltt}
\hlkwd{cbind}\hlstd{(}\hlkwd{sqrt}\hlstd{(}\hlkwd{diag}\hlstd{(spreg1}\hlopt{$}\hlstd{var)[}\hlkwd{c}\hlstd{(}\hlnum{2}\hlstd{,} \hlnum{3}\hlstd{,} \hlnum{4}\hlstd{,} \hlnum{5}\hlstd{,} \hlnum{1}\hlstd{)]),}
      \hlkwd{sqrt}\hlstd{(}\hlkwd{diag}\hlstd{(}\hlkwd{vcov}\hlstd{(s2sls.e))),}
      \hlkwd{sqrt}\hlstd{(}\hlkwd{diag}\hlstd{(spreg2}\hlopt{$}\hlstd{var)[}\hlkwd{c}\hlstd{(}\hlnum{2}\hlstd{,} \hlnum{3}\hlstd{,} \hlnum{4}\hlstd{,} \hlnum{5}\hlstd{,} \hlnum{1}\hlstd{)]),}
      \hlkwd{sqrt}\hlstd{(}\hlkwd{diag}\hlstd{(}\hlkwd{vcov}\hlstd{(s2sls.e,} \hlkwc{tse} \hlstd{=} \hlstr{"rob"}\hlstd{))))}
\end{alltt}
\begin{verbatim}
##                   [,1]       [,2]       [,3]       [,4]
## (Intercept) 0.06655890 0.06655890 0.06681918 0.06681918
## x1          0.06346892 0.06346892 0.06586903 0.06586903
## x2          0.06276194 0.06276194 0.06290024 0.06290024
## x3          0.06720175 0.06720175 0.06669244 0.06669244
## Rho         0.06221908 0.06221908 0.05892332 0.05892332
\end{verbatim}
\end{kframe}
\end{knitrout}

%--------------------------------------------------------------
\subsection{Best S2SLS Estimator}\index{BS2SLS}\label{sec:BS2SLS}
%--------------------------------------------------------------

\cite{lee2003best} suggested the so-called optimal instruments matrix, which gives rise to the Best Spatial Two Stage Least Square (BS2SLS) Estimator. Instead of using the IV matrices that are composed of a subset of the linearly independent columns of $(\mX_n, \mW_n\mX_n, \mW^2_n\mX_n, \ldots)$, \cite{lee2003best} suggests the instrument matrix\index{Instrumental Variables!optimal instruments}:
\begin{equation*}
\widetilde{\mH}^{*}_n =\left[\mX_n, \mW_n(\mI_n - \widetilde{\rho}_n\mW_n)^{-1}\mX_n\widetilde{\vbeta}_n\right],
\end{equation*}
%
which requires the use of consistent first-stage estimates for $\rho_0$ and $\vbeta_0$.\footnote{In \cite{Keliejian2004}, a similar approach is outlined where the matrix inverse is replaced by the power expansion. This yield an instruments matrix as $\mH =\left[\mX, \mW\left(\sum_{l = 1}^{\infty}\rho_0^{l}\mW^l\right)\mX\vbeta\right]$. In practice, the power expansion must be truncated at a finite value to ensure feasibility.} \cite{lee2003best} shows that these instruments are asymptotically optimal in the sense that they provide the smallest asymptotic variance among all the IV estimators of the SLM.\footnote{It is important to note that \cite{lee2003best} provides the BS2SLS estimator in the context of the SAC model. Thus, his estimator is termed Best Generalized Spatial Two Stage Least Squares (BGS2SLS) estimator. }

The resulting S2SLS estimator is called the Best Spatial Two Stage Least Squares (BS2SLS) estimator. However, since $\widetilde{\mH}^{*}_n$ is an $n \times (k + 1)$ matrix, the model is just-identified and the B2SLS estimator is an IV estimator given by
\begin{equation}\label{eq:bs2sls-estimator}
\widehat{\vdelta}_n = \left[\widetilde{\mH}^{*\top}_n\mZ_n\right]^{-1}\widetilde{\mH}^{*\top}_n\vy_n.
\end{equation}

We summarize \cite{lee2003best}'s assumptions as follows:

%---------------------------------------------------------
\begin{assumption}[Assumptions for the BS2SLSE of the SLM \citep{lee2003best}]\label{assumption:lee-2003}
Assume the following:
\begin{enumerate}
\item The errors  $\left\lbrace \epsilon_{i,n}, 1 \leq i \leq n, n\geq 1\right\rbrace$ are distributed identically. Further, the errors $\left\lbrace \epsilon_{i,n}, 1 \leq i \leq n\right\rbrace$ are for each $n$ distributed jointly independent with  $\E(\epsilon_{i,n})= 0$ and $\E(\epsilon_{i,n}^2) = \sigma^2_{\epsilon}$, with $0 < \sigma^2_{\epsilon}  < b <\infty$. Additionally the errors are assumed to possess fourth moments.
  \item The matrices $(\mI_n - \rho_0\mW_n)$ are nonsingular. 
  \item The row and column sums of the matrices $\mW_n$ and $(\mI_n - \rho_0\mW_n)^{-1}$ are uniformly bounded in absolute value. 
  \item The elements of the matrices $\mX_n$ are uniformly bounded in absolute value. 
  \item Let $\mG_0 = \mW_n(\mI_n - \rho_0\mW_n)^{-1}$. The limit $\mJ^* = \lim_{n \to \infty}(1/n)\mH_n^{*\top}\mH_n^{*}$ exists and is nonsingular, where
  \begin{equation*}
    \mH_n^* = \left[\mX_n, \mW_n(\mI_n - \rho_0\mW_n)^{-1}\mX_n\vbeta_0\right] = \left[\mX_n, \mG_0\mX_n\vbeta_0\right].
  \end{equation*}
  \item $\widetilde{\rho}_n\pto \rho_0$ and $\widetilde{\vbeta}_n \pto \vbeta_0$.\footnote{This assumption is stronger than Assumption 6 in \cite{lee2003best}. \cite{lee2003best} shows consistency under the assumption that $\widetilde{\rho}_n$ is $n^\gamma$-consistent for some $\gamma>0$.}
\end{enumerate}  
\end{assumption}
%---------------------------------------------------------

Most of the assumptions in Assumption \ref{assumption:lee-2003} are standard in spatial settings. However, unlike the S2SLSE from previous sections, the BS2SLSE must account for the stochastic nature of $\widetilde{\mH}^{*}_n$, which depends nonlinearly on the initial consistent estimate $\widetilde{\rho}_n$. In particular, the uniform boundedness properties in (c) are imposed only at the true value of $\rho_0$, i.e., a single point in the parameter space. To ensure validity, we require uniform boundedness to hold for all parameter values in a neighborhood of $\rho_0$. Fortunately, \citet[pag. 313][]{lee2003best}'s Lemma shows that condition (c) is sufficient to guarantee that the uniform boundedness property extends to a small neighborhood of the true parameter value. 

%---------------------------------------
\begin{lemma}\label{lemma:lee2003-ub}
Suppose that $\{\lVert \mW_n\rVert\}$ and $\{\lVert \left(\mI_n - \rho_0\mW_n\right)^{-1}\rVert\}$, where $\lVert \cdot \rVert$ is a matrix norm, are bounded. Then, $\{\lVert \left(\mI_n - \eta\mW_n\right)^{-1}\rVert\}$ are uniformly bounded in $\eta$ in a neighborhood of $\rho_0$.
\end{lemma}
%---------------------------------------

\begin{proof}[Proof Lemma \ref{lemma:lee2003-ub}]
First, we rewrite the inverse $\left(\mI_n - \eta\mW_n\right)^{-1}$ in terms of $\left(\mI_n - \rho_0\mW_n\right)^{-1}$. Let $\Delta = \eta-\rho_0$, then
\begin{equation*}
\begin{aligned}
  \mI_n - \eta\mW_n  & = \mI_n - \left(\Delta + \rho_0\right)\mW_n, \\
                     & = \mI_n - \rho_0\mW_n - \Delta \mW_n, \\
                     & = \mS_{0} - \Delta \mW_n, \\
                     & = \mS_{0}\left(\mI_n - \Delta \mW_n\mS_{0}^{-1}\right), \\
                     & = \mS_{0}\left(\mI_n - \Delta \mG_0\right),
  \end{aligned}
\end{equation*}
%
where $\mS_{0} =  \mI_n - \rho\mW_n$ and $\mG_0 = \mW_n\mS_0^{-1}$ provided that $\mG_0$ is invertible.
Using Neumann series expansion for small $\Delta$, we obtain
\begin{equation*}
\begin{aligned}
  \left(\mI_n - \Delta \mG_0\right)^{-1} & = \sum_{k = 0}^{\infty}\left(\eta-\rho_0\right)^k\mG_0^k.
\end{aligned}
\end{equation*}

From assumption, $\{\lVert \mW_n\rVert\}$ is bounded, say by $c_a$, sot that $\lVert \mW_n\rVert\leq c_a$ for all $n$. Similarly, by assumption, $\{\lVert \left(\mI_n - \rho_0\mW_n\right)^{-1}\rVert\}$ is bounded, say by $c_b$, so that $\lVert \left(\mI_n - \rho_0\mW_n\right)^{-1}\rVert\leq c_b$. Thus, for sufficiently small $|\Delta|$, and since the geometric series $\sum_{k = 0}^{\infty}\left(\eta-\rho_0\right)^k\mG_0^k$ converges to $(1 - |\Delta|c_ac_b)^{-1}$:
\begin{equation*}
\begin{aligned}
\lVert  \left(\mI_n - \Delta \mG_0\right)^{-1}\rVert & \leq \sum_{k = 0}^{\infty}\left|\eta-\rho_0\right|^k\rVert \mG_0\rVert^k, \\
&\leq \sum_{k = 0}^{\infty}\left|\eta-\rho_0\right|^k c_a^kc_b^k, \\
& = \frac{1}{1 - |\Delta|c_ac_b}, \\
& < \infty,
\end{aligned}
\end{equation*}
for sufficiently small $|\Delta|$. Thus
\begin{equation*}
\begin{aligned}
  \sup_{\eta \in \calB}\lVert \mI_n - \eta\mW_n \rVert & \leq \rVert \mS_0^{-1} \lVert \cdot \sup_{\eta \in \calB}\lVert  \left(\mI_n - \Delta \mG_0\right)^{-1}\rVert \\
  & \leq \sup_{\eta \in \calB} c_b \cdot \frac{1}{1 - |\Delta|c_ac_b}, \\
  & < \infty,
\end{aligned}  
\end{equation*}
%
taking a close neighborhood $\calB$ contained in $B_1 = \{\eta: |\eta - \rho_0|< 1/(c_ac_b)\}$.
\end{proof}

The following theorem provides the asymptotic distribution of the BS2SLSE for the SLM model. 

%------------------------------------------------------------------------
\begin{theorem}[Asymptotic Distribution of BS2SLSE for SLM]\label{teo:BS2SLS_est_slm}
Under Assumption \ref{assumption:lee-2003}, the BS2SLS estimator defined in Equation \eqref{eq:bs2sls-estimator} is consistent and
\begin{equation*}
  \sqrt{n}\left(\widehat{\vdelta}_n - \vdelta_0\right) \dto \rN(\vzeros, \mPsi_0^o), 
\end{equation*}
where
\begin{equation}\label{eq:limiting-var-b2sls}
\mPsi_0^o = \sigma_{\epsilon}^2\left[\lim_{n\to\infty} \frac{1}{n}\mH^{*\top}_n \mH_n^*\right]^{-1} = \sigma_{\epsilon}^2\mJ^{*-1}.
\end{equation}
\end{theorem}
%------------------------------------------------------------------------

The limiting variance $\mPsi_0^o$ in Equation \eqref{eq:limiting-var-b2sls} can be compared with the limiting distribution in Equation \eqref{eq:var-cov-homo-2sls}, which can be written as
\begin{equation*}
\mOmega_0^o = \sigma_{\epsilon}^2\left[\plim_{n\to\infty}\frac{1}{n}\mZ^\top_n\mH_n\left(\mH_n^\top\mH_n\right)^{-1}\mH_n^\top\mZ_n\right].
\end{equation*}

In proof \ref{proof:consistency-plimHQ}, we showed that:
\begin{equation*}
\plim_{n\to\infty}\frac{1}{n}\mH_n^\top\mZ_n = \plim_{n\to\infty}\frac{1}{n}\mH_n^\top\E(\mZ_n)=\plim_{n\to\infty}\mH_n^\top\mH_n^*.
\end{equation*}

In addition, the generalized Schwartz inequality implies that:\footnote{For any matrices $\mA$ and $\mB$, the generalized Schwarz inequality states that:
\begin{equation*}
\mA^\top\mB\left(\mB^\top\mB\right)^{-1}\mB^\top\mA\leq \mA^\top\mA.
\end{equation*}
This follows from the fact that the projection matrix $\mB\left(\mB^\top\mB\right)^{-1}\mB^\top$ is positive semi-definite and represents an orthogonal projection onto the column space of $\mB$, which cannot increase the norm of any vector.
}
\begin{equation*}
\mH_n^{*\top}\mH_n\left(\mH_n^\top\mH_n\right)^{-1}\mH_n^\top\mH_n^{*}\leq \mH_n^{*\top}\mH_n^*.
\end{equation*}

Then,
\begin{equation*}
\mOmega_0^o =  \sigma_{\epsilon}^2\left[\plim_{n\to\infty}\frac{1}{n}\mZ^\top_n\mH_n\left(\mH_n^\top\mH_n\right)^{-1}\mH_n^\top\mZ_n\right] \geq \sigma_{\epsilon}^2\left[\lim_{n\to\infty} \frac{1}{n}\mH^{*\top}_n \mH_n^*\right]^{-1} = \mPsi_0^o.
\end{equation*}

This inequality states that the variance matrix of the S2SLS estimator using $\mH_n$ as instruments is at least as large as the variance matrix of an estimator that directly uses the optimal instruments $\mH_n^*$. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}[Sketch of Proof for Asymptotic Normality of BS2SLSE]
This sketch of proof is based on \cite{lee2003best}. The sampling error of the BS2SLSE is
\begin{equation}\label{eq:asy_2sls_pr2}
\begin{aligned}
\sqrt{n}(\widehat{\vdelta}_{n} - \vdelta_0) & = \left(\frac{1}{n}\widetilde{\mH}^{*\top}_n \mZ_n\right)^{-1}\left(\frac{1}{\sqrt{n}}\widetilde{\mH}^{*\top}_n\vepsi_n\right),
\end{aligned}
\end{equation}
%
where 
\begin{equation*}
\widetilde{\mH}^{*}_n =\left[\mX_n, \mW_n(\mI_n - \widetilde{\rho}_n\mW_n)^{-1}\mX_n\widetilde{\vbeta}_n\right].
\end{equation*}

\textbf{First step:} First, we show that:
\begin{equation*}
\plim \left(\frac{1}{n}\widetilde{\mH}^{*\top}_n \mZ_n\right)=\plim \frac{1}{n}\mH^{*\top}_n\mH_n^* = \mJ^*, 
\end{equation*}
%
where $\mZ_n = \left[\mX_n, \mW_n\vy_n\right]$, and in the limit $\mH^{*}_n = \left[\mX_n, \mG_0 \mX_n\vbeta_0\right] = \E(\mZ_n)$ is the population expectation of the reduced form equation, where $\mG_0 = \mW_n\mS_0^{-1}$ and $\mS_0 = \left(\mI_n - \rho_0\mW_n\right)$.

Let $\mW_n\left(\mI_n - \widetilde{\rho}\mW_n\right)^{-1} = \mW_n\mS_n^{-1}(\widetilde{\rho}_n) = \mG_n(\widetilde{\rho}_n)$. By definition of $\widetilde{\mH}^{*}_n$:
\begin{equation*}
  \frac{1}{n}\widetilde{\mH}^{*\top}_n \mZ_n = \frac{1}{n}\left[\mX_n, \mG_n(\widetilde{\rho}_n)\mX_n\widetilde{\vbeta}_n\right]^\top\mZ_n. 
\end{equation*}

Since $\mZ_n = \left[\mX_n, \mW_n\vy_n\right]$, and the reduced form equation at the true parameters is $\vy_n = \mS_0^{-1}\mX_n\vbeta_0 + \mS_0^{-1}\vepsi_n$, the previous expression can be written as
\begin{equation*}
\begin{aligned}
\frac{1}{n}\widetilde{\mH}^{*\top}_n \mZ_n & = \frac{1}{n}\left[\mX_n, \mG_n(\widetilde{\rho}_n)\mX_n\widetilde{\vbeta}\right]^\top\left[\mX_n, \mG_0\mX_n\vbeta_0 + \mG_0\vepsi_n\right].
\end{aligned}
\end{equation*}

We now analyze the asymptotic behavior of $\frac{1}{n}\widetilde{\mH}^{*\top}_n \mZ_n$ as $n\to\infty$. Clearly, 
\begin{equation*}
 \frac{1}{n}\mX_n^\top\mX_n \to \lim_{n\to\infty}\frac{1}{n}\mX_n^\top\mX_n, 
\end{equation*}
%
which is bounded by Assumption \ref{assumption:lee-2003}(d). Similarly, since $\E(\vepsi_n) = \vzeros$,  
\begin{equation*}
\frac{1}{n}\left(\mX_n^\top\mG_0\mX_n\vbeta_0 + \mX_n^\top\mG_0\vepsi_n\right)\pto \frac{1}{n}\mX_n^\top\mG_0\mX_n\vbeta_0 + \frac{1}{n}\mX_n^\top\mG_0\E(\vepsi_n) = \frac{1}{n}\mX_n^\top\mG_0\mX_n\vbeta_0.
\end{equation*}

Unlike the S2SLSE proposed by \cite{kelejian1998generalized}, the matrix of instruments is stochastic. Thus,  $\frac{1}{n}\widetilde{\vbeta}^\top_n\mX_n^\top \mG_n(\widetilde{\rho}_n)^\top\mX_n$ needs to be expanded around $\rho_0$ to establish whether $\mG_n(\widetilde{\rho}_n)\pto \mG_0$. Note that $\mS_n(\widetilde{\rho}_n)^{-1} - \mS_0^{-1}$ can be written as
\begin{equation}\label{eq:p-bs2sls-1}
\begin{aligned}
\mS_n(\widetilde{\rho}_n)^{-1} - \mS_0^{-1} & = \mS_n(\widetilde{\rho}_n)^{-1}\mS_0\mS_0^{-1} - \mS_n(\widetilde{\rho}_n)^{-1}\mS_n(\widetilde{\rho}_n)\mS_0^{-1}, \\
    & = \mS_n(\widetilde{\rho}_n)^{-1}\left(\mS_0\mS_0^{-1} - \mS_n(\widetilde{\rho}_n)\mS_0^{-1}\right), \\
    & = \mS_n(\widetilde{\rho}_n)^{-1}\left[\mS_0 - \mS_n(\widetilde{\rho}_n)\right]\mS_0^{-1}.
\end{aligned}
\end{equation}

Since $\partial \mS(\eta) /\partial \eta = -\mW_n$, a \textbf{first-order expansion} for $\mS_n(\widetilde{\rho}_n)$ around $\rho_0$ is
\begin{equation}\label{eq:p-bs2sls-2}
\mS_n(\widetilde{\rho}_n) = \mS_0 - \mW_n(\widetilde{\rho}_n - \rho_0).
\end{equation}

Inserting Equation \eqref{eq:p-bs2sls-2} into \eqref{eq:p-bs2sls-1}, and pre-multiplying by $\mW_n$ yields:
\begin{equation*}
\begin{aligned}
\mW_n\left(\mI_n - \widetilde{\rho}_n\mW_n\right)^{-1} & = \mW_n\left(\mI_n - \rho_0\mW_n\right)^{-1} + \mW_n\mS_n(\widetilde{\rho}_n)^{-1}\mW_n\mS_0^{-1}\left(\widetilde{\rho}_n - \rho_0\right), \\
\mG_n(\widetilde{\rho}_n) & = \mG_0 + \left(\widetilde{\rho}_n - \rho_0\right)\mG_{n}(\widetilde{\rho}_n)\mG_0.
\end{aligned}
\end{equation*}

Because $\mG_0$ is uniformly bounded in both row and column sum and $\mG_n(\eta)$ is \textbf{uniformly bounded} in row sums, uniformly in $\eta$ belonging to its parameter space, the matrices in the previous expansion are uniformly bounded.\footnote{Since $\mG_n(\widetilde{\rho}_n)$ is a non-linear function of $\widehat{\rho}_n$ we need to assume uniformly boundedness in a neighborhood of $\rho_0$. See Lemma 2 in \cite{lee2003best}.} Furthermore, since $\left(\widetilde{\rho}_n - \rho_0\right) = o_p(1)$, we have that:
\begin{equation*}
\mG_n(\widetilde{\rho}_n) -  \mG_0  = o_p(1).
\end{equation*}

Therefore, since $\widetilde{\vbeta}_n\pto \vbeta_0$
\begin{equation*}
\begin{aligned}
\frac{1}{n}\widetilde{\vbeta}^\top_n\mX_n^\top \mG_n(\widetilde{\rho}_n)^\top\mX_n & \pto \frac{1}{n}\vbeta_0^\top\mX_n^\top \mG_0^\top\mX_n, \\
\frac{1}{n}\widetilde{\vbeta}^\top_n\mX_n^\top \mG_n(\widetilde{\rho}_n)^\top\mG_0\vepsi_n & \pto \frac{1}{n}\vbeta_0^\top\mX_n^\top \mG_0^\top\mG_0\E(\vepsi_n) = \vzeros.
\end{aligned}
\end{equation*}

Collecting the results, we have
\begin{equation*}
\begin{aligned}
  \frac{1}{n}\widetilde{\mH}^{*\top}_n \mZ_n & \pto \lim_{n \to \infty}\frac{1}{n}\left[ \mX_n^\top,  \vbeta_0^\top\mX_n^\top\mG_0^\top\right] \left[\mX_n, \mG_0\mX_n\vbeta_0\right], \\
   & = \lim_{n \to \infty}\frac{1}{n}\left[\mX_n, \mW_n\left(\mI_n - \rho_0\mW_n\right)^{-1}\mX_n\vbeta_0\right]^\top\E(\mZ_n), \\
   & = \lim_{n \to \infty} \frac{1}{n}\mH^{*\top}_n\mH^{*}_n,
  \end{aligned}
\end{equation*}
%
which can also be written as
\begin{equation*}
\plim \frac{1}{n}\widetilde{\mH}^{*\top}_n \mZ_n  = \lim_{n \to \infty} \frac{1}{n}\mH^{*\top}_n\mH^{*}_n = \mJ^*
\end{equation*}


\textbf{Second step:} We need to show that
\begin{equation*}
\frac{1}{\sqrt{n}}\widetilde{\mH}_n^{*\top}\vepsi_n \dto \rN(\vzeros, \sigma_{\epsilon}^2\mJ^{*-1}). 
\end{equation*}

Since $\widehat{\vbeta}_n \pto \vbeta_0$, 
\begin{equation*}
\frac{1}{\sqrt{n}}\widetilde{\mH}_n^{*\top}\vepsi_n = \frac{1}{\sqrt{n}}\left[\mX_n, \mG_0\mX_n\vbeta_0\right]^\top \vepsi_n + o_p(1) = \frac{1}{\sqrt{n}}\mH_n^{*\top}\vepsi_n + o_p(1).
\end{equation*}

Since $\lim_{n\to\infty}n^{-1}\mH_n^{*\top}\mH_n^{*}$ exists and it is invertible, by Theorem \ref{teo:CLT_tri_arr}, we have limiting distribution.
\end{proof}

%--------------------------------------------------------------
\subsection{Coding BS2SLS Estimator}
%--------------------------------------------------------------

To compute the BS2SLS estimator, we modify our previous function, \texttt{slm.2sls}, and create a new function called \texttt{slm.b2sls}. 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Main function to estimate S2SLS estimator}
\hlstd{slm.b2sls} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{formula}\hlstd{,} \hlkwc{data}\hlstd{,} \hlkwc{W}\hlstd{,} \hlkwc{instruments} \hlstd{=} \hlnum{2}\hlstd{)\{}
  \hlcom{# Model frame setup}
  \hlstd{callT}    \hlkwb{<-} \hlkwd{match.call}\hlstd{(}\hlkwc{expand.dots} \hlstd{=} \hlnum{TRUE}\hlstd{)}
  \hlstd{mf}       \hlkwb{<-} \hlstd{callT}
  \hlstd{m}        \hlkwb{<-} \hlkwd{match}\hlstd{(}\hlkwd{c}\hlstd{(}\hlstr{"formula"}\hlstd{,} \hlstr{"data"}\hlstd{),} \hlkwd{names}\hlstd{(mf),} \hlnum{0L}\hlstd{)}
  \hlstd{mf}       \hlkwb{<-} \hlstd{mf[}\hlkwd{c}\hlstd{(}\hlnum{1L}\hlstd{, m)]}
  \hlstd{mf[[}\hlnum{1L}\hlstd{]]} \hlkwb{<-} \hlkwd{as.name}\hlstd{(}\hlstr{"model.frame"}\hlstd{)}
  \hlstd{mf}       \hlkwb{<-} \hlkwd{eval}\hlstd{(mf,} \hlkwd{parent.frame}\hlstd{())}

  \hlcom{# Get variables and check dimensions}
  \hlstd{y}  \hlkwb{<-} \hlkwd{model.response}\hlstd{(mf)}
  \hlstd{X}  \hlkwb{<-} \hlkwd{model.matrix}\hlstd{(formula, mf)}
  \hlstd{n}  \hlkwb{<-} \hlkwd{nrow}\hlstd{(X)}
  \hlstd{Wy} \hlkwb{<-} \hlstd{W} \hlopt{%*%} \hlstd{y}
  \hlstd{sn} \hlkwb{<-} \hlkwd{nrow}\hlstd{(W)}
  \hlkwa{if} \hlstd{(n} \hlopt{!=} \hlstd{sn)} \hlkwd{stop}\hlstd{(}\hlstr{"number of spatial units in W is different to the number of data"}\hlstd{)}

  \hlcom{# Obtain first (and consistent)  estimates}
  \hlstd{H} \hlkwb{<-} \hlkwd{cbind}\hlstd{(X,} \hlkwd{make.H}\hlstd{(}\hlkwc{W} \hlstd{= W,} \hlkwc{X} \hlstd{= X,} \hlkwc{l} \hlstd{= instruments))}

  \hlcom{# Compute S2SLSE}
  \hlstd{Z}           \hlkwb{<-} \hlkwd{cbind}\hlstd{(X, Wy)}
  \hlkwd{colnames}\hlstd{(Z)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlkwd{colnames}\hlstd{(X),} \hlstr{"Wy"}\hlstd{)}
  \hlstd{HH}          \hlkwb{<-} \hlkwd{crossprod}\hlstd{(H, H)}
  \hlstd{PH}          \hlkwb{<-} \hlstd{H} \hlopt{%*%} \hlkwd{solve}\hlstd{(HH)} \hlopt{%*%} \hlkwd{t}\hlstd{(H)}
  \hlstd{Z_hat}       \hlkwb{<-} \hlstd{PH} \hlopt{%*%} \hlstd{Z}
  \hlstd{b_2sls}      \hlkwb{<-} \hlkwd{solve}\hlstd{(}\hlkwd{crossprod}\hlstd{(Z_hat))} \hlopt{%*%} \hlkwd{crossprod}\hlstd{(Z_hat, y)}

  \hlcom{# Second step: BS2SLS estimation}
  \hlstd{beta.hat} \hlkwb{<-} \hlstd{b_2sls[}\hlnum{1}\hlopt{:}\hlkwd{ncol}\hlstd{(X)]}
  \hlstd{rho.hat}  \hlkwb{<-} \hlkwd{drop}\hlstd{(}\hlkwd{tail}\hlstd{(b_2sls,} \hlkwc{n} \hlstd{=} \hlnum{1L}\hlstd{))}
  \hlstd{H.lee}    \hlkwb{<-} \hlstd{W} \hlopt{%*%} \hlkwd{solve}\hlstd{(}\hlkwd{diag}\hlstd{(n)} \hlopt{-} \hlstd{rho.hat} \hlopt{*} \hlstd{W)} \hlopt{%*%} \hlstd{X} \hlopt{%*%} \hlstd{beta.hat}
  \hlstd{H.star}   \hlkwb{<-} \hlkwd{cbind}\hlstd{(X, H.lee)}
  \hlstd{b_2sls}   \hlkwb{<-} \hlkwd{solve}\hlstd{(}\hlkwd{crossprod}\hlstd{(H.star, Z))} \hlopt{%*%} \hlkwd{crossprod}\hlstd{(H.star, y)}

  \hlcom{# Compute residuals}
  \hlstd{y_hat}  \hlkwb{<-} \hlstd{Z} \hlopt{%*%} \hlstd{b_2sls}
  \hlstd{e_hat}  \hlkwb{<-} \hlstd{y} \hlopt{-} \hlstd{y_hat}

  \hlcom{# Save results}
  \hlstd{results} \hlkwb{<-} \hlkwd{structure}\hlstd{(}
    \hlkwd{list}\hlstd{(}
      \hlkwc{coefficients} \hlstd{= b_2sls,}
      \hlkwc{call}         \hlstd{= callT,}
      \hlkwc{X}            \hlstd{= X,}
      \hlkwc{H}            \hlstd{= H.star,}
      \hlkwc{Z}            \hlstd{= Z,}
      \hlkwc{y}            \hlstd{= y,}
      \hlkwc{PH}           \hlstd{= PH,}
      \hlkwc{e_hat}        \hlstd{= e_hat,}
      \hlkwc{W}            \hlstd{= W}
    \hlstd{),}
    \hlkwc{class} \hlstd{=} \hlstr{'mybs2sls'}
  \hlstd{)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

The \texttt{slm.b2sls} function first obtains the S2SLS estimates using the instruments proposed by \cite{kelejian1998generalized} instruments. It then, uses these consistent estimates to construct the optimal instrument $\widetilde{\mH}_n^*$. The BS2SLSE is then obtained using Equation \eqref{eq:bs2sls-estimator}.

The S3 method \texttt{vcov} is constructed using Equation \eqref{eq:limiting-var-b2sls} and can be applied to either the initial or final round estimates. 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# S3 Method vcov}
\hlstd{vcov.mybs2sls} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{object}\hlstd{,} \hlkwc{estimate} \hlstd{=} \hlkwd{c}\hlstd{(}\hlstr{"initial"}\hlstd{,} \hlstr{"final"}\hlstd{),} \hlkwc{...}\hlstd{)\{}
  \hlstd{estimate} \hlkwb{<-} \hlkwd{match.arg}\hlstd{(estimate)}
  \hlstd{X}        \hlkwb{<-} \hlstd{object}\hlopt{$}\hlstd{X}
  \hlstd{n}        \hlkwb{<-} \hlkwd{nrow}\hlstd{(X)}
  \hlstd{k}        \hlkwb{<-} \hlkwd{ncol}\hlstd{(X)}
  \hlstd{df}       \hlkwb{<-} \hlstd{n} \hlopt{-} \hlstd{(k} \hlopt{+} \hlnum{1}\hlstd{)}
  \hlstd{s2}       \hlkwb{<-} \hlkwd{crossprod}\hlstd{(object}\hlopt{$}\hlstd{e_hat)} \hlopt{/} \hlstd{df}
  \hlkwa{if} \hlstd{(estimate} \hlopt{==} \hlstr{"final"}\hlstd{)\{}
    \hlstd{b}       \hlkwb{<-} \hlstd{object}\hlopt{$}\hlstd{coefficients}
    \hlstd{b.hat}   \hlkwb{<-} \hlstd{b[}\hlnum{1}\hlopt{:}\hlstd{k]}
    \hlstd{rho.hat} \hlkwb{<-} \hlkwd{drop}\hlstd{(}\hlkwd{tail}\hlstd{(b,} \hlkwc{n} \hlstd{=} \hlnum{1L}\hlstd{))}
    \hlstd{W}       \hlkwb{<-} \hlstd{object}\hlopt{$}\hlstd{W}
    \hlstd{H.lee}   \hlkwb{<-} \hlstd{W} \hlopt{%*%} \hlkwd{solve}\hlstd{(}\hlkwd{diag}\hlstd{(n)} \hlopt{-} \hlstd{rho.hat} \hlopt{*} \hlstd{W)} \hlopt{%*%} \hlstd{X} \hlopt{%*%} \hlstd{b.hat}
    \hlstd{H.star}  \hlkwb{<-} \hlkwd{cbind}\hlstd{(X, H.lee)}
  \hlstd{\}} \hlkwa{else} \hlstd{\{}
    \hlstd{H.star}  \hlkwb{<-} \hlstd{object}\hlopt{$}\hlstd{H}
  \hlstd{\}}
  \hlstd{var}    \hlkwb{<-} \hlkwd{drop}\hlstd{(s2)} \hlopt{*} \hlkwd{solve}\hlstd{(}\hlkwd{crossprod}\hlstd{(H.star)} \hlopt{/} \hlstd{n)} \hlopt{/} \hlstd{n}
  \hlkwd{return}\hlstd{(var)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

The S3 methods for \texttt{summary} are the following:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# S3 methods for summary}
\hlstd{summary.mybs2sls} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{object}\hlstd{,}
                             \hlkwc{estimate} \hlstd{=} \hlkwd{c}\hlstd{(}\hlstr{"initial"}\hlstd{,} \hlstr{"final"}\hlstd{),}
                             \hlkwc{table} \hlstd{=} \hlnum{TRUE}\hlstd{,}
                             \hlkwc{digits} \hlstd{=} \hlkwd{max}\hlstd{(}\hlnum{3}\hlstd{, .Options}\hlopt{$}\hlstd{digits} \hlopt{-} \hlnum{3}\hlstd{),}
                             \hlkwc{...}\hlstd{)\{}
  \hlstd{estimate}  \hlkwb{<-} \hlkwd{match.arg}\hlstd{(estimate)}
  \hlstd{n}         \hlkwb{<-} \hlkwd{nrow}\hlstd{(object}\hlopt{$}\hlstd{Z)}
  \hlstd{df}        \hlkwb{<-} \hlstd{n} \hlopt{-} \hlkwd{ncol}\hlstd{(object}\hlopt{$}\hlstd{Z)}
  \hlstd{b}         \hlkwb{<-} \hlstd{object}\hlopt{$}\hlstd{coefficients}
  \hlstd{std.err}   \hlkwb{<-} \hlkwd{sqrt}\hlstd{(}\hlkwd{diag}\hlstd{(}\hlkwd{vcov}\hlstd{(object,} \hlkwc{estimate} \hlstd{= estimate)))}
  \hlstd{z}         \hlkwb{<-} \hlstd{b} \hlopt{/} \hlstd{std.err}
  \hlstd{p}         \hlkwb{<-} \hlnum{2} \hlopt{*} \hlkwd{pt}\hlstd{(}\hlopt{-}\hlkwd{abs}\hlstd{(z),} \hlkwc{df} \hlstd{= df)}
  \hlstd{CoefTable} \hlkwb{<-} \hlkwd{cbind}\hlstd{(b, std.err, z, p)}
  \hlkwd{colnames}\hlstd{(CoefTable)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"Estimate"}\hlstd{,} \hlstr{"Std.Error"}\hlstd{,} \hlstr{"t-value"}\hlstd{,} \hlstr{"Pr(>|t|)"}\hlstd{)}
  \hlstd{result} \hlkwb{<-} \hlkwd{structure}\hlstd{(}
    \hlkwd{list}\hlstd{(}
      \hlkwc{CoefTable} \hlstd{= CoefTable,}
      \hlkwc{digits}    \hlstd{= digits,}
      \hlkwc{call}      \hlstd{= object}\hlopt{$}\hlstd{call),}
    \hlkwc{class} \hlstd{=} \hlstr{'summary.mybs2sls'}
  \hlstd{)}
  \hlkwd{return}\hlstd{(result)}
\hlstd{\}}

\hlstd{print.summary.mybs2sls} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{,}
                                   \hlkwc{digits} \hlstd{= x}\hlopt{$}\hlstd{digits,}
                                   \hlkwc{na.print} \hlstd{=} \hlstr{""}\hlstd{,}
                                   \hlkwc{symbolic.cor} \hlstd{= p} \hlopt{>} \hlnum{4}\hlstd{,}
                                   \hlkwc{signif.stars} \hlstd{=} \hlkwd{getOption}\hlstd{(}\hlstr{"show.signif.stars"}\hlstd{),}
                                   \hlkwc{...}\hlstd{)}
\hlstd{\{}
  \hlkwd{cat}\hlstd{(}\hlstr{"\textbackslash{}nCall:\textbackslash{}n"}\hlstd{)}
  \hlkwd{cat}\hlstd{(}\hlkwd{paste}\hlstd{(}\hlkwd{deparse}\hlstd{(x}\hlopt{$}\hlstd{call),} \hlkwc{sep} \hlstd{=} \hlstr{"\textbackslash{}n"}\hlstd{,} \hlkwc{collapse} \hlstd{=} \hlstr{"\textbackslash{}n"}\hlstd{),} \hlstr{"\textbackslash{}n\textbackslash{}n"}\hlstd{,} \hlkwc{sep} \hlstd{=} \hlstr{""}\hlstd{)}

  \hlkwd{cat}\hlstd{(}\hlstr{"\textbackslash{}nCoefficients:\textbackslash{}n"}\hlstd{)}
  \hlkwd{printCoefmat}\hlstd{(x}\hlopt{$}\hlstd{CoefTable,} \hlkwc{digit} \hlstd{= digits,} \hlkwc{P.value} \hlstd{=} \hlnum{TRUE}\hlstd{,} \hlkwc{has.Pvalue} \hlstd{=} \hlnum{TRUE}\hlstd{)}
  \hlkwd{invisible}\hlstd{(}\hlkwa{NULL}\hlstd{)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

Using the same DGP as for the S2SLS, we obtain the BS2SLS estimates:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Using the BS2SLS estimator}
\hlstd{b2sls} \hlkwb{<-} \hlkwd{slm.b2sls}\hlstd{(y} \hlopt{~} \hlstd{x1} \hlopt{+} \hlstd{x2} \hlopt{+} \hlstd{x3,} \hlkwc{data} \hlstd{= data,} \hlkwc{W} \hlstd{= W,} \hlkwc{instruments} \hlstd{=} \hlnum{2}\hlstd{)}
\hlkwd{summary}\hlstd{(b2sls,} \hlkwc{estimate} \hlstd{=} \hlstr{"initial"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## slm.b2sls(formula = y ~ x1 + x2 + x3, data = data, W = W, instruments = 2)
## 
## 
## Coefficients:
##             Estimate Std.Error t-value Pr(>|t|)    
## (Intercept) -0.08156   0.06604  -1.235    0.217    
## x1          -1.01266   0.06417 -15.781   <2e-16 ***
## x2          -0.04195   0.06291  -0.667    0.505    
## x3           1.02569   0.06655  15.413   <2e-16 ***
## Wy           0.61460   0.05803  10.591   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}
\begin{alltt}
\hlkwd{summary}\hlstd{(b2sls,} \hlkwc{estimate} \hlstd{=} \hlstr{"final"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## slm.b2sls(formula = y ~ x1 + x2 + x3, data = data, W = W, instruments = 2)
## 
## 
## Coefficients:
##             Estimate Std.Error t-value Pr(>|t|)    
## (Intercept) -0.08156   0.06631  -1.230    0.219    
## x1          -1.01266   0.06405 -15.811   <2e-16 ***
## x2          -0.04195   0.06290  -0.667    0.505    
## x3           1.02569   0.06638  15.452   <2e-16 ***
## Wy           0.61460   0.06032  10.189   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}
\end{kframe}
\end{knitrout}



% %==================================
% \subsection{S2SLS Estimation in R}\index{S2SLS!stsls function}\index{S2SLS!example}
% %==================================
% 
% In this section we continue our example from Section \ref{sec:Anselin-example}. In particular, we will estimate the following SLM model:
% \begin{equation*}
% \vy = \rho\mW\vy + \mX\vbeta + \vepsi,
% \end{equation*}
% %
% where $\vy$ is our crime variable and $\mX$ contains a vector of ones and the variables \code{INC} and \code{HOVAL}. We will estimate this model again by ML procedure and then compare it with the S2SLS procedure. In \proglang{R} there exists two functions in order to compute the  S2SLS procedure. The first one is the \code{stsls} from \pkg{spdep} and \code{stslshac} from \pkg{sphet} package \citep{sphetp}. The latter allows estimating also S2SLS with heterokedasticity using HAC estimators. 
% 
% We first load the required packages and dataset:
% 
% <<s2sls-in-r>>=
% # Load packages and data
% library("memisc") 
% library("spdep")
% library("spatialreg")
% library("sphet")
% data("columbus")
% listw <- nb2listw(col.gal.nb)
% source("getSummary.sarlm.R")
% @
% 
% Now we estimate the SLM model by ML using Ord's eigen approximation of the determinant and S2SLS with homokedastic and robust standard errors. 
% 
% <<s2sls-models>>=
% # Estimate models
% slm      <- lagsarlm(CRIME ~ INC + HOVAL, 
%                      data = columbus,
%                      listw, 
%                      method = "eigen")
% s2sls    <- stsls(CRIME ~ HOVAL + INC, 
%                   data =  columbus,
%                   listw = listw,
%                   robust = FALSE,
%                   W2X = TRUE)
% s2sls_rob <- stsls(CRIME ~ HOVAL + INC, 
%                    data =  columbus,
%                    listw = listw,
%                    robust = TRUE,
%                    W2X = TRUE)
% s2sls_pir <- stslshac(CRIME ~ INC + HOVAL, 
%                       data = columbus, 
%                       listw = listw, 
%                       HAC = FALSE)
% @
% 
% \code{stsls} function fits SLM model by S2SLS, with the option of adjusting the results for heteroskedasticity. Note that the arguments are similar to \code{lagsarlm} from \pkg{spdep}. The \code{robust} option of \code{stsls} is set \code{FALSE} as default. If \code{TRUE} the function applies a heteroskedasticity correction to the coefficient covariances. Note that the third model \code{s2sls\_rob} uses this option. The argument \code{W2X} controls the number of instruments. When \code{W2X = FALSE} only $\mW\mX$ are used as instruments, however when \code{W2X = TRUE}  $\mW\mX$ and $\mW^2\mX$ are used as instruments for $\mW\vy$. The function \code{stslshac} from \pkg{sphet} with the argument \code{HAC = FALSE} estimate the S2SLS estimates with homokedastic standard errors without adjusting for heteroskedasticity.
% 
% Some caution should be expressed regarding the standard errors. When the argument \code{robust = FALSE} is used, the variance-covariance matrix is computed as:
% 
% 
% \begin{equation*}
% \widehat{\var}(\widehat{\delta}_{2SLS}) = \widehat{\sigma}^2_{\epsilon}\left[\mZ^\top\mZ\right]^{-1}
% \end{equation*}
% %
% where:
% 
% \begin{equation*}
%   \widehat{\sigma}^2 = \frac{\widehat{\vepsi}^\top\widehat{\vepsi}}{n - K},\quad \widehat{\vepsi} = \vy - \widehat{\vy}
% \end{equation*}
% 
% Note that the error variance is calculated with a degrees of freedom correction (i.e., dividing by $n-K$). When  \code{robust = TRUE} the variance-covariance matrix is computed as we have previously stated. That is:
% 
% \begin{equation*}
% \widehat{\var}(\widehat{\delta}_{2SLS}) = \widehat{\sigma}^2_{\epsilon}\left[\mZ^\top \mH (\mH^\top\mH)^{-1}\mH^\top\mZ\right]^{-1}
% \end{equation*}
% 
% The results are presented in Table \ref{tab:columbus-models2}.
% 
% \citet[][pag. 24]{lesage2014regional} points out that researcher should consider performance of estimation procedures, not simply point estimates. That is, when comparing models we should also focus on the scalar summaries of the partial derivatives (direct/indirect effects estimates) and their standard errors. That is, methods that seems superior in terms of bias of the parameters might performance worse in terms of partial effects. 
% 
% 
% Now we compare the direct and indirect effects\index{Spillover effects!for S2SLS}: 
% 
% <<>>=
% im_ml    <- impacts(slm, listw = listw, R = 200)
% im_s2sls <- impacts(s2sls_rob, listw = listw, R = 200)
% summary(im_ml, zstats = TRUE, short = TRUE)
% summary(im_s2sls, zstats = TRUE, short = TRUE)
% @
% 
% 
% \begin{table}[ht]
% \caption{Spatial Models for Crime in Columbus: ML vs S2SLS}\label{tab:columbus-models2}
% \centering
% <<echo = FALSE, results = 'asis', warning=FALSE>>=
% table_2 <- mtable("SLM"   = slm,
%                   "S2SLS" = s2sls,
%                   "S2SLSR" = s2sls_rob,
%        summary.stats = c("N"),
%        coef.style = "default")
% table_2 <- relabel(table_2,
%                    "(Intercept)" = "\\emph{Constant}",
%                    "rho" = "$\\rho$") 
% toLatex(table_2, compact = TRUE, useBooktabs =  TRUE)
% @
% \end{table}


%==================================
\section{GMM Estimator of SLM}\label{sec:gmm-slm}
%==================================

%*************************************************
\subsection{GMM Estimator Under Homoskedasticity}\label{sec:gmm-slm-homo}
%*************************************************

In the previous sections, we analyzed the S2SLS and BS2SLS estimators for the Spatial Lag Model (SLM). Both estimators belong to the broader class of Generalized Method of Moments (GMM) estimators. This section reviews the GMM estimator for the SLM, as proposed by \cite{lee2007gmm}, and derives the GMM and Optimal GMM (OGMM) estimators for the model. This GMM estimator can be considered as an one-step GMM estimator (see Section \ref{sec:gmm-one-step}).

To construct the moment conditions, \cite{lee2007gmm} uses linear and quadratic moments. Let $\mH_n$ denote an $n \times k_x$ matrix of instrumental variables (IVs) constructed as a function of $\mX_n$ and $\mW_n$. This matrix can be constructed using the instruments proposed by \cite{kelejian1998generalized}, such that
\begin{equation*}
\mH_n = \left[\mX_n, \mW_n\mX_n, \mW_n^2\mX_n, \cdots\right],
\end{equation*}
%
or using the asymptotically optimal instruments proposed by \cite{lee2003best}:
\begin{equation*}
\mH_n = \left[\mX_n,  \mW_n(\mI_n - \widetilde{\rho}_n\mW_n)^{-1}\mX_n\widetilde{\vbeta}_n\right],
\end{equation*}
%
where $\widetilde{\rho}_n$ and $\widetilde{\vbeta}_n$ are consistent estimates of $\rho_0$ and $\vbeta_0$.

Recall the error term for the SLM is defined as
\begin{equation*}
\vepsi_n(\vtheta) = \vy_n - \mZ_n\vdelta_n = (\mI_n - \rho \mW_n)\vy_n - \mX_n\vbeta.
\end{equation*}

Unlike the S2SLSE, the GMME uses additional quadratic moments. Because of this, the optimally weighted GMM estimator can be asymptotically efficient relative to the S2SLSE. For the quadratic population moments, \cite{lee2007gmm} proposes:
\begin{equation}\label{eq:quadractic-moment-lee}
\E(n^{-1}\vepsi_n^\top \mP_{jn}\vepsi_n) = 0, 
\end{equation}
%
where $\mP_{jn}$ are $n\times n$ matrix of constant having \textbf{zero trace}. \cite{lee2007gmm} refers to this class of matrices as $\calP_{1n} = \{\mP: \text{$\mP$ is $n\times n$ matrix, $\tr(\mP) = 0$}\}$. A subclass $\calP_{2n}$ arises for $n\times n$ matrices having zero diagonal: $\calP_{2n} = \{\mP: \text{$\mP$ is $n\times n$ matrix, $\diag(\mP) = \vzeros$}\}$. 

To understand the role played by $\mP_{jn}$, note for any nonstochastic matrix $\mA_{n}$, the quadratic moments are given by:
\begin{equation*}
\begin{aligned}
 \E\left(\vepsi_n^\top \mA_n\vepsi_n\right)& = \tr\left[\E\left(\vepsi_n^\top \mA_n\vepsi_n\right)\right], \\
 & = \E\left[\tr\left(\vepsi_n^\top \mA_n\vepsi_n\right)\right], \\
 & = \E\left[\tr\left(\mA_n\vepsi_n\vepsi_n^\top\right)\right], \\
  & = \tr\left[\E\left(\mA_n\vepsi_n\vepsi_n^\top\right)\right], \\
 & = \sigma_0^2\tr\left(\mA_n\right). 
\end{aligned}
\end{equation*}

Thus $\E\left(\vepsi_n^\top \mA_n\vepsi_n\right)\neq 0$, unless $\tr(\mA_n) = 0$. If $\tr(\mA_n) \neq 0$, we can always create for example $\mP_n = \mA_n - \Diag(\mP_n)$ such that $\tr(\mP_n) = 0$. 

It is important to stress the intuitions of the instruments. The regressor $\mW_n\vy_n$ is endogenous, as
\begin{equation*}
\E\left[\left(\mW_n\vy_n\right)^\top \vepsi_n\right] = \sigma_0^2\tr\left(\mG_n(\rho_0)\right) \neq 0, 
\end{equation*}
%
where $\mG_n(\rho_0) = \mW_n\mS_0^{-1}$, i.e., the elements of $\mW_n\vy_n$ are correlated with the elements of $\vepsi_n$. This can be observed from the expression $\mW_n\vy_n = \mG_n(\rho_0)\mX_n\vbeta_0 + \mG_n(\rho_0)\vepsi_n$, which follows from the reduced form $\vy_n = \mS_0^{-1}\mX_n\vbeta + \mS_0^{-1}\vepsi_n$. The linear instruments $\mH_n$ is correlated with $\mG_n(\rho_0)\mX_n\vbeta_0$ (since $\mH_n$ are mean of $\mW_n\vy_n$),  but uncorrelated with $\vepsi_n$, because $\E(\mH_n^\top\vepsi_n) = \mH_n^\top\E(\vepsi_n) = \vzeros$. Note that we require $\mP_{jn}$ to be correlated with $\mG_0$ in the sense that $\tr(\mP_{jn}\mG_0)\neq 0$. As long as $\tr(\mP_{jn}) = 0$, $\mP_{jn}\vepsi_n$ is uncorrelated with $\vepsi_n$, and thus it may be used as an instrument of $\mW_n\vy_n$. 

The quadratic moments in Equation \eqref{eq:quadractic-moment-lee} can be derived under homoskedasticity, where $\E(\epsilon_{in}^2) = \sigma_0^2$ \citep{lee2001generalized}. For example, we can use the following moment conditions provided by \cite{kelejian1999generalized}:
\begin{equation}\label{eq:moments-kp-1999}
  \begin{aligned}
  \E\left[\frac{1}{n}\vepsi_n^\top \vepsi_n\right]           & = \sigma^2_0, \\
   \E\left[\frac{1}{n}\vepsi_n^\top\mW_n\mW_n\vepsi_n\right] & = \sigma^2_0\frac{1}{n}\tr\left(\mW_n^\top\mW_n\right), \\
   \E\left[\frac{1}{n}\vepsi_n^\top \mW_n \vepsi_n\right]    & =  0,
   \end{aligned}
\end{equation}

These three moment conditions can be reduced to two. Substituting out $\sigma^2_0$ into the second moment equation yields:
\begin{equation*}
  \begin{aligned}
    \E\left[\frac{1}{n}\vepsi^\top_n\mW_n\mW_n\vepsi_n\right] - \E\left[\frac{1}{n}\vepsi^\top_n \vepsi_n\right] \frac{1}{n}\tr\left(\mW^\top_n\mW_n\right) & = 0, \\
    \frac{1}{n}\E\left[\vepsi^\top_n\mW_n\mW_n\vepsi_n - \vepsi^\top_n \vepsi_n\frac{1}{n}\tr\left(\mW^\top_n\mW_n\right)\right] & = 0, \\
     \frac{1}{n}\E\left[\vepsi^\top_n\mW_n\mW_n\vepsi_n - \vepsi^\top_n \frac{1}{n}\tr\left(\mW^\top_n\mW_n\right)\vepsi_n\right] & = 0,\\
     \frac{1}{n}\E\left[\vepsi^\top_n\left(\mW_n\mW_n - \frac{1}{n}\tr\left(\mW^\top_n\mW_n\right)\mI_n\right)\vepsi_n\right] & = 0, \\
     \frac{1}{n}\E\left[\vepsi^\top_n\mP_{2n}\vepsi\right] & = 0, 
  \end{aligned}
\end{equation*}
%
where $\mP_{2n}$ is symmetric with $\tr(\mP_{2n}) = 0$, but its diagonal elements are non zero (In the heteroskedasticity case it is!). Based on these results, \cite{lee2007gmm} proposes:
\begin{align*}
\mP_{1n} & = \mW_n, \\
\mP_{2n} & = \mW^2_n - \left(\tr(\mW^2_n)/n\right)\mI_n.
\end{align*}

The vector of moments functions is then defined as:
\begin{equation}\label{eq:moments-lee-2007}
\underset{(2 + k_x)\times 1}{\vg_n(\vtheta)} = \begin{pmatrix}
\vepsi_n(\vtheta)^\top\mP_{1n}\vepsi_n(\vtheta) \\
\vepsi_n(\vtheta)^\top\mP_{2n}\vepsi_n(\vtheta) \\
\mH_n^\top\vepsi_n(\vtheta)
\end{pmatrix},
\end{equation}
%
which has dimension $(2 + k_k) \times 1$. It is important to note that the moment conditions at the population hold. At $\vtheta_0$:
\begin{equation*}
\begin{aligned}
\E(\mH_n^\top\vepsi_n) & = \mH_n^\top\E(\vepsi_n) = \vzeros,  \\
\E(\vepsi_n^\top \mP_{jn}\vepsi_n) & = \sigma_0^2\tr(\mP_{jn}) = \vzeros\;\mbox{for $j = 1,2$}
\end{aligned}
\end{equation*}

Let $\mUpsilon_n$ be some $(2 + k_k) \times (2 + k_k)$ symmetric positive semidefinite weighting matrix, then the corresponding GMM estimator is defined as:
\begin{equation}\label{eq:gmm_slm}
\widehat{\vtheta}_{n} = \underset{\vtheta \in \mTheta}{\argmin}\;\;Q_n(\vtheta) = \vg_n(\vtheta)^\top\mUpsilon_n\vg_n(\vtheta),
\end{equation}
%
where $\vtheta = (\rho , \vbeta^\top)^\top$ is an $(1 + k)$-dimensional vector, and where $\vg_n(\vtheta)$ is the $(2 + k_x)\times 1$ vector of moments. 

Since $\E(\vg_n) =  \vzeros$, the variance-covariance matrix of the population moment functions are given by:
\footnotesize
\begin{equation*}
\begin{aligned}
\mOmega_n & = \var(\vg_n) \\
& = \E(\vg_n\vg_n^\top)\\
& = \E\begin{pmatrix}
\vepsi_n(\vtheta)^\top\mP_{1n}\vepsi_n(\vtheta) \cdot \left(\vepsi_n(\vtheta)^\top\mP_{1n}\vepsi_n(\vtheta)\right)^\top   & \vepsi_n(\vtheta)^\top\mP_{1n}\vepsi_n(\vtheta) \cdot \left(\vepsi_n(\vtheta)^\top\mP_{2n}\vepsi_n(\vtheta)\right)^\top & \vepsi_n(\vtheta)^\top\mP_{1n}\vepsi_n(\vtheta) \cdot \vepsi_{n}(\vtheta)^\top\mH_n \\
\vepsi_n(\vtheta)^\top\mP_{2n}\vepsi_n(\vtheta) \cdot \left(\vepsi_n(\vtheta)^\top\mP_{1n}\vepsi_n(\vtheta)\right)^\top   & \vepsi_n(\vtheta)^\top\mP_{2n}\vepsi_n(\vtheta) \cdot \left(\vepsi_n(\vtheta)^\top\mP_{2n}\vepsi_n(\vtheta)\right)^\top & \vepsi_n(\vtheta)^\top\mP_{2n}\vepsi_n(\vtheta) \cdot \vepsi_{n}(\vtheta)^\top\mH_n \\
\mH_n^\top\vepsi_n(\vtheta) \cdot \left(\vepsi_n(\vtheta)^\top\mP_{1n}\vepsi_n(\vtheta)\right)^\top   & \mH_n^\top\vepsi_n(\vtheta) \cdot \left(\vepsi_n(\vtheta)^\top\mP_{2n}\vepsi_n(\vtheta)\right)^\top & \mH_n^\top\vepsi_n(\vtheta) \cdot \vepsi_{n}(\vtheta)^\top\mH_n 
\end{pmatrix}
\end{aligned}
\end{equation*}
\normalsize

To find these moments, we use Lemma \ref{lemma:O-lemma-lee-quadratic}. Thus, using the fact that $\tr(\mP_{jn}) = 0$, for $j = 1, 2$: 
\begin{equation*}
\begin{aligned}
\E\left[\mH_n^\top\vepsi_n(\vtheta) \cdot \vepsi_n(\vtheta)^\top\mP_{jn}\vepsi_n(\vtheta)\right] & = \mH_n^\top\diag(\mP_{jn})\mu_3, \\
\E\left[\vepsi(\vtheta)^\top\mP_{jn}\vepsi_n(\vtheta) \cdot \vepsi(\vtheta)^\top\mP_{ln}\vepsi_n(\vtheta)\right]& = (\mu_4 - 3\sigma^4_0)\diag(\mP_{jn})^\top\diag(\mP_{ln}) + \sigma^4_0\left[\tr(\mP_{jn})\tr(\mP_{ln}) + \tr(\mP_{jn}\mP_{ln}^s)\right], \\
& = (\mu_4 - 3\sigma^4_0)\diag(\mP_{jn})^\top\diag(\mP_{ln}) + \sigma^4_0\tr(\mP_{jn}\mP_{ln}^s), \\
\end{aligned}
\end{equation*}
%
where $\mu_3 = \E(\epsilon_{ni}^3)$,  $\mu_4 = \E(\epsilon_{ni}^4)$, and $\mP_{jn}^s = \mP_{jn} + \mP_{jn}^\top$. Thus, we can write:
\begin{equation*}
\underset{(2 + k_x) \times (2 + k_k)}{\mOmega_n} = \begin{pmatrix}
              \underset{(2 \times 2)}{(\mu_4 - 3\sigma_0^4)\vomega_n^\top\vomega_n} & \underset{(2 \times k_x)}{\mu_3\vomega^\top_n\mH_n} \\
              \underset{(k_x \times 2)}{\mu_3 \mH_n^\top\vomega_n} & \underset{(k_x \times k_x)}{\mZeros} 
            \end{pmatrix}
             + 
             \mV_n,
\end{equation*}
%
with $\vomega_n = \left[\diag(\mP_{1n}), \diag(\mP_{2n})\right]$ is $n\times 2$ and 
\begin{equation*}
  \mV_n = \sigma_0^4
          \begin{pmatrix}
          \underset{(1\times 1)}{\tr(\mP_{1n}\mP_{1n}^s)} & \underset{(1\times 1)}{\tr(\mP_{1n}\mP_{2n}^s)} & \underset{(1\times k_x)}{\vzeros} \\
          \tr(\mP_{2n}\mP_{1n}^s) & \tr(\mP_{2n}\mP_{2n}^s) & \underset{(1\times k_x)}{\vzeros} \\
          \underset{(k_x\times 1)}{\vzeros} & \underset{(k_x\times 1)}{\vzeros} & \underset{(k_x\times k_x)}{\frac{1}{\sigma_0^2}\mH_n^\top\mH_n}
          \end{pmatrix}.
\end{equation*}

As \cite{lee2007gmm} remarks, when $\vepsi_n$ is normally distributed, $\mOmega_n$ is simplified to $\mV_n$ because $\mu_3 = 0$, and $\mu_4 = 3\sigma_0^3$.

For further reference note that the first derivatives of the moment functions in Equation \eqref{eq:moments-lee-2007} are:
\begin{equation}\label{eq:Jacobian-gmm-slm}
\begin{aligned}
\underset{((2 + k_x) \times (k + 1))}{\frac{\partial \vg_n(\vtheta)}{\partial \vtheta^\top}} & = \begin{pmatrix}
                                                      \vepsi_n^\top\mP_{1n}^s \\
                                                      \vepsi_n^\top\mP_{2n}^s \\
                                                      \mH_n^\top
                                                      \end{pmatrix}
                                                      \frac{\partial \vepsi_n}{\partial \vtheta^\top}, \\
                                                & =  \underset{((2 + k_x)\times n)}{\begin{pmatrix}
                                                      \vepsi_n^\top\mP_{1n}^s \\
                                                      \vepsi_n^\top\mP_{2n}^s \\
                                                      \mH_n^\top
                                                      \end{pmatrix}}
                                                    \underset{(n\times(1 + k))}{\begin{pmatrix}
                                                      -\mW_n\vy_n & -\mX_n
                                                    \end{pmatrix}}.
\end{aligned}
\end{equation}


%----------------------------------------------------------------------------------
\begin{assumption}[Assumptions for GMM \citep{lee2007gmm}]\label{assumption:lee2007-1}
Assume the following:
\begin{enumerate}
  \item The $\epsilon_{ni}$ are i.i.d with zero mean, variance $\sigma_0^2$ and that a moment of order higher than the fourth exists. 
  \item The elements of $\mX_n$ are uniformly bounded constants, $\mX_n$ has the full rank $k$, and $\lim_{n\to\infty}(1/n)\mX_n^\top\mX_n$ exists and is nonsingular. 
  \item The spatial weights matrices $\{\mW_n\}$ and $\{(\mI_n - \rho\mW_n)^{-1}\}$ at $\rho = \rho_0$ are uniformly bounded in both row and column sums in absolute value.
  \item The matrices $\mP_{1n}$ and $\mP_{2n}$ are uniformly bonded in both row and column sums in absolute value, and elements of $\mH_n$ are uniformly bounded. 
  \item Either 
  \begin{enumerate}
    \item $\lim_{n\to\infty}(1/n)\mH_n^\top\left[\mG_0\mX_n\vbeta_0, \mX_n\right]$ has the full rank $(k + 1)$, or
    \item $\lim_{n\to\infty}(1/n)\mH_n^\top\mX_n$ has the full rank $k$, $\lim_{n\to\infty}(1/n)\tr\left(\mP_{jn}^s\mG_0\right)\neq 0$ for some $j$, $\mP_{jn}^s = \mP_{jn} + \mP_{jn}^\top$, and 
    \begin{equation*}
      \lim_{n\to\infty}(1/n)\left[\tr\left(\mP_{1n}^s\mG_0\right), \tr\left(\mP_{2n}^s\mG_0\right)\right]^\top, 
    \end{equation*}
    %
    is linearly independent of 
    \begin{equation*}
    \begin{aligned}
    \lim_{n\to\infty}(1/n)\left[\tr\left(\mG_0^\top\mP_{1n}\mG_0\right), \tr\left(\mG_0^\top\mP_{2n}\mG_0\right)\right]^\top
    \end{aligned}
    \end{equation*}
  %
  where $\mG_0 = \mW_n\mS_0^{-1}$.
  \end{enumerate}
\end{enumerate}
\end{assumption}
%---------------------------------------------------------------------------------

%----------------------------------------------------------------------------------
\begin{theorem}[GMM estimator for SLM under homoskedasticity \citep{lee2007gmm}]\label{teo:GMME-SLM}
Let $\mUpsilon_n\to \mUpsilon_0$. Under Assumptions \ref{assumption:lee2007-1}, suppose that $\mP_{jn}$ for $j = 1, 2$, are from $\calP_{1n}$ and $\mH_n$ is a $n\times k_k$ matrix so that
\begin{equation*}
\lim_{n \to \infty}\frac{1}{n}\E\left[\vg_n(\vtheta)\right] = \vzeros, 
\end{equation*}
%
has a unique root at $\vtheta_0$ in $\mTheta$. Then, the GMM estimator $\widehat{\vtheta}_n$ derived from $\min_{\vtheta \in \mTheta}\vg_n(\vtheta)^\top\mUpsilon_n\vg_n(\vtheta)$ is a consistent estimator of $\vtheta_0$, and $\sqrt{n}(\widehat{\vtheta}_n - \vtheta_0)\dto \rN(\vzeros, \mSigma_0)$, where
\begin{equation*}
\begin{aligned}
  \mSigma_0 = &  \lim_{n\to\infty}\left[\left(\frac{1}{n}\mD_n^\top\right)\mUpsilon_0\left(\frac{1}{n}\mD_n\right)\right]^{-1}\left(\frac{1}{n}\mD_n^\top\right)\mUpsilon_0\left(\frac{1}{n}\mOmega_n\right)\mUpsilon_0\left(\frac{1}{n}\mD_n\right) \\
  & \times \left[\left(\frac{1}{n}\mD_n^\top\right)\mUpsilon_0\left(\frac{1}{n}\mD_n\right)\right]^{-1},
  \end{aligned}
\end{equation*}
%
and
\begin{equation}\label{eq:D-gmm-slm}
\underset{(k_x + 2)\times (k + 1)}{\mD_n} = \frac{\partial \E\left[\vg_n(\vtheta_0)\right]}{\partial \vtheta^\top} = \begin{pmatrix}
           \underset{(1 \times 1)}{\sigma_0^2\tr(\mP^s_{1n}\mG_0)} & \underset{(1 \times k)}{\vzeros} \\
           \underset{(1 \times 1)}{\sigma_0^2\tr(\mP^s_{2n}\mG_0)} & \vzeros \\
           \underset{(k_x \times 1)}{\mH_n^\top\mG_0\mX_n\vbeta_0} & \underset{(k_x\times k)}{\mH^\top_n\mX_n}
         \end{pmatrix}
\end{equation}
%
where $\mG_0 = \mW_n\mS_0^{-1}$, and under the assumption that $\lim_{n\to\infty}\mD_n$ exists and has the full rank $k + 1$. 
\end{theorem}
%---------------------------------------------------------------------------------

The GMME uses as linear instruments
\begin{equation*}
\mH_n = \left[\mX_n, \mW_n\mX_n, \mW_n^2\mX_n, \cdots\right],
\end{equation*}
%
and
\begin{align*}
\mP_{1n} & = \mW_n, \\
\mP_{2n} & = \mW^2_n - \left(\tr(\mW^2_n)/n\right)\mI_n.
\end{align*}

Assuming that $\mUpsilon_n = \mI_{(2 + k_k)}$, the asymptotic variance can be estimated as
\begin{equation*}
\begin{aligned}
  \widehat{\mSigma} = &  \left[\left(\frac{1}{n}\widehat{\mD}_n^\top\right)\left(\frac{1}{n}\widehat{\mD}_n\right)\right]^{1}\left(\frac{1}{n}\widehat{\mD}_n^\top\right)\left(\frac{1}{n}\widehat{\mOmega}_n\right)\left(\frac{1}{n}\widehat{\mD}_n\right) \left[\left(\frac{1}{n}\widehat{\mD}_n^\top\right)\left(\frac{1}{n}\widehat{\mD}_n\right)\right]^{1},
  \end{aligned}
\end{equation*}
%
where $\widehat{\mOmega}_n$ is estimated as
\begin{equation}\label{eq:omega-slm-hat}
\widehat{\mOmega}_n = \begin{pmatrix}
              (\widehat{\mu}_4 - 3\widehat{\sigma}_n^4)\vomega_n^\top\vomega_n & \widehat{\mu}_3\vomega^\top_n\mH_n \\
              \widehat{\mu}_3 \mH_n^\top\vomega_n & \mZeros 
            \end{pmatrix}
             + 
             \widehat{\mV}_n,
\end{equation}
%
with $\widehat{\mu}_4 = (1/n)\sum_{i  = 1}^n\widehat{\epsilon}_{in}^4$, $\widehat{\mu}_3 = (1/n)\sum_{i  = 1}^n\widehat{\epsilon}_{in}^3$, $\widehat{\sigma}_n^2 = (1/n)\sum_{i = 1}^n\widehat{\epsilon}_{in}^2$,  $\widehat{\vepsi}_n = (\mI_n - \widehat{\rho}_n\mW_n)\vy_n - \mX_n\widehat{\vbeta}_n$, and 
\begin{equation*}
  \widehat{\mV}_n = \widehat{\sigma}_n^4
          \begin{pmatrix}
          \tr(\mP_{1n}\mP_{1n}^s) & \tr(\mP_{1n}\mP_{2n}^s) & \vzeros \\
          \tr(\mP_{2n}\mP_{1n}^s) & \tr(\mP_{2n}\mP_{2n}^s) & \vzeros \\
          \vzeros & \vzeros & \frac{1}{\widehat{\sigma}_n^2}\mH_n^\top\mH_n
          \end{pmatrix}
\end{equation*}

The matrix $\widehat{\mD}_n$ can be estimated by replacing $\widehat{\vtheta}_n$ in Equation \eqref{eq:D-gmm-slm} or using the Jacobian in Equation \eqref{eq:Jacobian-gmm-slm} evaluated at $\widehat{\vtheta}_n$. 

%-----------------------------------------------
\subsubsection{Consistency of GMM estimator}
%-----------------------------------------------

In the following lines, we provide and sketch for the proof of the asymptotic properties of the GMM estimator for SLM. First,  we need the following results. Note that for any $\rho$ in the parameter space (see Exercise \ref{exercise:expansion-res1})
\begin{equation}\label{eq:expansion-S-1}
\begin{aligned}
  \mS_n^{-1} & = \mI_n + \rho\mW_n + \rho^2\mW_n^2 + \rho^3\mW_n + \cdots, \\
  & = \mI_n + \rho\mW_n\left[\mI_n + \rho\mW_n + \rho^2\mW_n^2 + \cdots\right], \\
  & = \mI_n + \rho\mW_n\mS_n^{-1}.
\end{aligned}
\end{equation}

In addition, and using the result in Equation \eqref{eq:expansion-S-1}, the following expansion would be useful:
\begin{equation}\label{eq:expansion-S-2}
\begin{aligned}
  \mS_n\mS_0^{-1}& = \left(\mI_n - \rho \mW_n\right)\left(\mI_n + \rho\mW_n\mS_0^{-1}\right),  \\
                 & = \mI_n + \rho_0\mW_n\mS_0^{-1} - \rho\mW_n - \rho\mW_n\rho_0\mW_n\mS_0^{-1}, \\
                 & = \mI_n + \rho_0\mW_n\mS_0^{-1} - \left(\rho\mW_n\left[\mI_n + \rho_0\mW_n\mS_0^{-1}\right]\right) \\
                 & = \mI_n + \rho_0\mW_n\mS_0^{-1} - \rho\mW_n\mS_0^{-1} \\
                 & = \mI_n + (\rho_0 - \rho)\mW_n\mS_0^{-1} \\
                 & = \mI_n + (\rho_0 - \rho)\mG_0, \\
\end{aligned}                 
\end{equation}
%
where $\mG_0 = \mW_n\mS_0^{-1}$.

Consistency of GMM estimator relies on the conditions of consistency for extremum estimators \citep{hansen1982large, newey1994large}. Two important conditions are \textbf{identification} conditions and \textbf{uniform convergence} \citep{lee2003best, lee2007gmm}. 

To understand the conditions for identification, consider the moment functions given in Equation \eqref{eq:moments-lee-2007}. To analyze the asymptotic behavior of these moment functions, we need to write the error term as a function of the true parameters of the models. Using the definition of the error term for the SLM and the reduced form equation, the error term can be expressed as:
\begin{equation}\label{eq:error-pop-slm}
  \begin{aligned}
    \vepsi_n(\vtheta) & = \left(\mI_n - \rho \mW_n\right)\vy_n - \mX_n\vbeta, \\
                      & = \left(\mI_n - \rho \mW_n\right)\left(\mS_0^{-1}\mX_n\vbeta_0 + \mS_0^{-1}\vepsi_n\right) - \mX_n\vbeta, \\
                      & = \mS_n\mS_0^{-1}\mX_n\vbeta_0 - \mX_n\vbeta + \mS_n\mS_0^{-1}\vepsi_n, \\
                      & = \vd_n(\vtheta) + \mS_n\mS_0^{-1}\vepsi_n,
  \end{aligned}
\end{equation}
%
where $\mS_n = \left(\mI_n - \rho \mW_n\right)$, $\mS_0 = \left(\mI_n - \rho_0 \mW_n\right)$, and $\vd_n(\vtheta) = \mS_n\mS_0^{-1}\mX_n\vbeta_0 - \mX_n\vbeta$. Then, using the result in Equation \eqref{eq:expansion-S-2}, $\vd_n(\vtheta)$ in Equation \eqref{eq:error-pop-slm} can be written as
\begin{equation*}
\begin{aligned}
\vd_n(\vtheta) & = \mS_n\mS_0^{-1}\mX_n\vbeta_0 - \mX_n\vbeta,  \\
               & = \left[\mI_n + (\rho_0 - \rho)\mG_0\right]\mX_n\vbeta_0 - \mX_n\vbeta, \\
               & = \mX_n(\vbeta_0 - \vbeta) + (\rho_0 - \rho)\mG_0\mX_n\vbeta_0.
\end{aligned}
\end{equation*}

Using these expression, we can write the population moments using the linear instruments as
\begin{equation*}
\E\left(\mH_n^\top\vepsi_n\right) = \E\left[\mH_n^\top\left(\vd_n(\vtheta) + \mS_n\mS_0^{-1}\vepsi_n\right)\right] = \mH^\top_n\vd_n(\vtheta), 
\end{equation*}
%
whereas the population moments using the quadratic instruments can be written as
\begin{equation*}
  \begin{aligned}
   \E(\vepsi_n^\top \mP_{jn}\vepsi_n) & = \E\left[\left(\vd_n(\vtheta) + \mS_n\mS_0^{-1}\vepsi_n\right)^\top\mP_{jn}\left(\vd_n(\vtheta) + \mS_n\mS_0^{-1}\vepsi_n\right)\right], \\
   & = \vd_n(\vtheta)^\top\mP_{jn}\vd_n(\vtheta) +\E\left(\vepsi_n^\top\mS_0^{-1\top}\mS_n^\top\mP_{jn}\mS_n\mS_0^{-1}\vepsi_n\right), \\
   & =  \vd_n(\vtheta)^\top\mP_{jn}\vd_n(\vtheta) + \sigma_0^2\tr\left(\mS_0^{-1\top}\mS_n^\top\mP_{jn}\mS_n\mS_0^{-1}\right), 
  \end{aligned}
\end{equation*}
%
for $j = 1, 2$. Thus, for any possible value $\vtheta$, the population moment conditions can be written as
\begin{equation*}
\E\left[\vg_n(\vtheta)\right]  = \begin{pmatrix}
 \vd_n(\vtheta)^\top\mP_{1n}\vd_n(\vtheta) + \sigma_0^2\tr\left(\mS_0^{-1\top}\mS_n^\top\mP_{1n}\mS_n\mS_0^{-1}\right) \\
  \vd_n(\vtheta)^\top\mP_{2n}\vd_n(\vtheta) + \sigma_0^2\tr\left(\mS_0^{-1\top}\mS_n^\top\mP_{2n}\mS_n\mS_0^{-1}\right) \\
  \mH^\top\vd_n(\vtheta)
                       \end{pmatrix}.  
\end{equation*}

Identification condition requires $\lim_{n\to\infty}(1/n)\E\left[\vg_n(\vtheta)\right] = \vzeros$ at $\vtheta_0$. Consider the moment equations using $\mH_n$. In the limit, the moment equations are:
\begin{equation*}
\begin{aligned}
\lim_{n\to\infty}\frac{1}{n}\mH_n^\top\vd_n(\vtheta) & = \lim_{n\to\infty}\frac{1}{n}\mH_n^\top\left[\mX_n(\vbeta_0 - \vbeta) + (\rho_0 - \rho)\mG_0\mX_n\vbeta_0\right], \\
& = \lim_{n\to\infty}\frac{1}{n}\mH_n^\top\mX_n(\vbeta_0 - \vbeta) + \lim_{n\to\infty}\frac{1}{n}(\rho_0 - \rho)\mH_n^\top\mG_0\mX_n\vbeta_0, \\
\end{aligned}
\end{equation*}
%
which will be $\lim_{n\to\infty}\frac{1}{n}\mH_n^\top\vd_n = \vzeros$ at the true parameters $\vtheta_0$, if $\left[\mH_n^\top\mX_n, \mH_n^\top\mG_0\mX_n\vbeta_0\right]$ has a full column rank, that is, its ranks equals $k + 1$, for large enough $n$. Intuitively, the sufficient conditions requires that $\mH_n$ to be correlated with the endogenous variables $\mW\vy$. On the other hand,  \textbf{sufficient rank condition} implies the \textbf{necessary rank condition} that $\left[\mX_n, \mG_0\mX_n\vbeta_0\right]$ has a full column rank $(k + 1)$, and that $\mH_n$ has a rank \textbf{at least} $k + 1$. This will occur if $\mG_0\mX_n\vbeta_0$ and $\mX_n$ are not asymptotically linearly dependent. 

For the quadratic moments, note that
\begin{equation*}
  \lim_{n\to\infty}\frac{1}{n}\vd_n(\vtheta)^\top\mP_{jn}\vd_n(\vtheta) + \lim_{n\to\infty}\sigma_0^2\frac{1}{n}\tr\left(\mS_0^{-1\top}\mS_n^\top\mP_{jn}\mS_n\mS_0^{-1}\right).
\end{equation*}

The identification of $\rho_0$ requires that
\begin{equation*}
\lim_{n\to\infty}\frac{1}{n}\tr\left(\mS_0^{-1\top}\mS_n^\top\mP_{jn}\mS_n\mS_0^{-1}\right) = 0,
\end{equation*}
%
for $j= 1, 2$. \cite{lee2007gmm} states that the set of limiting quadratic moment equations has a unique solution at $\rho_0$ if 
\begin{equation*}
      \lim_{n\to\infty}(1/n)\left[\tr\left(\mP_{1n}^s\mG_0\right), \tr\left(\mP_{2n}^s\mG_0\right)\right]^\top, 
    \end{equation*}
    %
    is linearly independent of 
\begin{equation*}
    \begin{aligned}
    \lim_{n\to\infty}(1/n)\left[\tr\left(\mG_0^\top\mP_{1n}\mG_0\right), \tr\left(\mG_0^\top\mP_{2n}\mG_0\right)\right]^\top
    \end{aligned}
\end{equation*}
    
Next, we need to show that:
\begin{equation*}
\frac{1}{n}\vg_n(\vtheta) \pto \E\left[\vg_n(\vtheta)\right], 
\end{equation*}
%
uniformly in $\vtheta\in \mTheta$. In previous proofs, we have shown that for the linear instruments $\frac{1}{n}\mH_n^\top\vepsi_n \pto \frac{1}{n}\mH_n^\top\vd_n(\vtheta)$.

The analysis for the quadratic moments is a bit more complicated. Using the error term as function of the population parameters, we can decompose the quadratic moment as
\begin{equation*}
\begin{aligned}
\vepsi_n^\top\mP_{jn}\vepsi_n & = \vd_n(\vtheta)^\top \mP_{jn}\vd_n(\vtheta) + \vd_n(\vtheta)^\top\mP_{jn}\mS_n\mS_0^{-1}\vepsi_n + \vepsi_n^\top \mS_0^{-1\top}\mS_n^\top\vd_n(\vtheta) + \vepsi_n^\top\mS_0^{-1\top}\mS_n^\top\mP_{jn}\mS_n\mS_0^{-1}\vepsi_n \\
& = \vd_n(\vtheta)^\top \mP_{jn}\vd_n(\vtheta) + \vd_n(\vtheta)^\top\mP_{jn}\left(\mI_n + (\rho_0 - \rho)\mG_0\right)\vepsi_n \\
& + \vepsi_n^\top \left(\mI_n + (\rho_0 - \rho)\mG_0\right)^\top \vd_n(\vtheta) \\
& + \vepsi_n^\top\left(\mI_n + (\rho_0 - \rho)\mG_0\right)^\top\mP_{jn}\left(\mI_n + (\rho_0 - \rho)\mG_0\right)\vepsi_n \\
& = \vd_n(\vtheta)^\top \mP_{jn}\vd_n(\vtheta) + \underbrace{\vd_n(\vtheta)^\top\mP_{jn}^s(\vepsi_n + (\rho_0 - \rho)\mG_0\vepsi_n)}_{\vl_n(\vtheta)} \\
& + \underbrace{(\vepsi_n^\top + (\rho_0 - \rho)\vepsi_n^\top\mG_0^\top)\mP_{jn}(\vepsi_n + (\rho_0 - \rho)\mG_0\vepsi_n)}_{\vq_n(\vtheta)}
\end{aligned}
\end{equation*}

Focusing on $(1/n)\vl_n(\vtheta)$, and using $\vd_n(\vtheta)$ yields. 
\begin{equation*}
  \begin{aligned}
    \frac{1}{n}\vl_n(\vtheta) & = (\rho_0 - \rho)\frac{1}{n}\left(\mX_n\vbeta_0\right)^\top \mG_0^\top \mP_{jn}^s\vepsi_n + (\vbeta_0 - \vbeta)^\top \frac{1}{n}\mX_n^\top\mP_{jn}^s\vepsi_n \\
    & +  (\rho_0 - \rho)^2\frac{1}{n}\left(\mX_n\vbeta_0\right)^\top \mG_0^\top \mP_{jn}^s\mG_0\vepsi_n \\
    & + (\rho_0 - \rho)(\vbeta_0 - \vbeta)^\top\frac{1}{n}\mX_n^\top\mP_{jn}^s\mG_0\vepsi_{n}.
  \end{aligned}
\end{equation*}

Thus, because the previous expression is linear in $\vepsi_n$ it can be established that
\begin{equation*}
  \begin{aligned}
    \frac{1}{n}\vl_n(\vtheta) & \pto  (\rho_0 - \rho)\frac{1}{n}\left(\mX_n\vbeta_0\right)^\top \mG_0^\top \mP_{jn}^s\E(\vepsi_n) + (\vbeta_0 - \vbeta)^\top \frac{1}{n}\mX_n^\top\mP_{jn}^s\E(\vepsi_n) \\
    & +  (\rho_0 - \rho)^2\frac{1}{n}\left(\mX_n\vbeta_0\right)^\top \mG_0^\top \mP_{jn}^s\mG_0\E(\vepsi_n) \\
    & + (\rho_0 - \rho)(\vbeta_0 - \vbeta)^\top\frac{1}{n}\mX_n^\top\mP_{jn}^s\mG_0\E(\vepsi_{n}) \\
    & \pto \vzeros, 
  \end{aligned}
\end{equation*}
uniformly in $\vtheta \in \mTheta$. The uniform convergence in probability follows because $\frac{1}{n}\vl_n(\vtheta)$ is simply a quadratic function of $\rho$ and $\vbeta$ and $\mTheta$ is a bounded set. 

For the second term, operating over the matrix multiplications, and taking $\plim$ yields, 
\begin{equation*}
  \begin{aligned}
    \frac{1}{n}\vq_n(\vtheta) & \pto  \frac{1}{n}\E\left[\vepsi_n^\top\mP_{jn}\vepsi\right] + (\rho_0 - \rho)\frac{1}{n}\E\left[\vepsi_n^\top\mG_0^\top\mP_{jn}^s\vepsi_n\right]  + (\rho_0 - \rho)^2\frac{1}{n}\E\left[\vepsi_n^\top\mG_0^\top\mP_{jn}\mG_n\vepsi_n\right],  \\
    & \pto (\rho_0 - \rho) \frac{\sigma_0^2}{n}\tr(\mG_0^\top\mP_{js}^s) + (\rho_0-\rho)^2\frac{\sigma_0^2}{n}\tr(\mG_0^\top\mP_{js}^s\mG_0),  
  \end{aligned}
\end{equation*} 
%
which can be also be written as
\begin{equation*}
  \begin{aligned}
    \frac{1}{n}\vq_n(\vtheta) & = (\rho_0 - \rho) \frac{\sigma_0^2}{n}\tr(\mG_0^\top\mP_{js}^s) + (\rho_0-\rho)^2\frac{\sigma_0^2}{n}\tr(\mG_0^\top\mP_{js}^s\mG_0) + o_p(1), 
  \end{aligned}
\end{equation*} 

Collecting terms yields, 
\begin{equation*}
\begin{aligned}
\frac{1}{n}\vepsi_n^\top\mP_{jn}\vepsi_n \pto \frac{1}{n}\vd_n(\vtheta)^\top\mP_{jn}\vd_n(\vtheta) + (\rho_0 - \rho) \frac{\sigma_0^2}{n}\tr(\mG_0^\top\mP_{js}^s) + (\rho_0-\rho)^2\frac{\sigma_0^2}{n}\tr(\mG_0^\top\mP_{js}^s\mG_0),
\end{aligned}
\end{equation*}
%
uniformly in $\vtheta$ in $\mTheta$. 

Thus, 
\begin{equation*}
\begin{aligned}
\begin{pmatrix}
\frac{1}{n}\vepsi_n\mP_{1n}\vepsi_n \\
\frac{1}{n}\vepsi_n\mP_{2n}\vepsi_n \\
\frac{1}{n}\mH_n^\top \vepsi_n \\
\end{pmatrix}
&\pto \frac{1}{n}\E\left[\vg_n(\vtheta)\right] \\
& = \begin{pmatrix}
 \frac{1}{n}\vd_n(\vtheta)^\top\mP_{1n}\vd_n(\vtheta) + (\rho_0 - \rho) \frac{\sigma_0^2}{n}\tr(\mG_0^\top\mP_{1s}^s) + (\rho_0-\rho)^2\frac{\sigma_0^2}{n}\tr(\mG_0^\top\mP_{1s}^s\mG_0) \\
 \frac{1}{n}\vd_n(\vtheta)^\top\mP_{2n}\vd_n(\vtheta) + (\rho_0 - \rho) \frac{\sigma_0^2}{n}\tr(\mG_0^\top\mP_{2s}^s) + (\rho_0-\rho)^2\frac{\sigma_0^2}{n}\tr(\mG_0^\top\mP_{2s}^s\mG_0) \\
 \mH_n^\top\vd_n(\vtheta)
 \end{pmatrix}.
 \end{aligned}
\end{equation*}

As $\vg_n(\vtheta)$ is a quadratic function of $\vtheta$ and $\mTheta$ is bounded, $(1/n)\E\left[\vg_n(\vtheta)\right]$ is uniformly equicontinuous on $\mTheta$. The identification condition and the uniform equicontinuity of $(1/n)\E(\vg_n(\vtheta))$ imply that the identification uniqueness condition must be satisfied.
%-----------------------------------------------
\subsubsection{Asymptotic distribution of GMM estimator}
%-----------------------------------------------
  Assuming that $\widehat{\vtheta}_n$ is an interior point, the first-order conditions yields
  \begin{equation}\label{eq:proof-gmm-2sls-foc}
    \frac{\partial \vg_n(\widehat{\vtheta}_n)^\top\mUpsilon_n\vg_n(\widehat{\vtheta}_n)}{\partial \vtheta} = - \left(\frac{\partial \vg_n(\widehat{\vtheta}_n)}{\partial \vtheta^\top}\right)^\top \mUpsilon_n \vg_n(\widehat{\vtheta}_n) = \vzeros.
  \end{equation}
  By Taylor expansion around $\vtheta_0$ of $\vg_n(\widehat{\vtheta}_n)$:
  \begin{equation}\label{eq:proof-gmm-2sls-taylor}
  \vg_n(\widehat{\vtheta}_n) = \vg_n(\vtheta_0) + \frac{\partial \vg_n(\bar{\vtheta}_n)}{\partial \vtheta^\top}\left(\widehat{\vtheta}_n - \vtheta_0\right).
  \end{equation}

  
  Substituting \eqref{eq:proof-gmm-2sls-taylor} into \eqref{eq:proof-gmm-2sls-foc}, solving for $(\widehat{\vtheta}_n - \vtheta_0)$ and multiplying by $\sqrt{n}$ gives
  \begin{equation*}
  \begin{aligned}
   \sqrt{n}(\widehat{\vtheta}_n - \vtheta_0) & = -\left[\left(\frac{1}{n}\frac{\partial \vg_n(\widehat{\vtheta}_n)}{\partial \vtheta^\top}\right)^\top\mUpsilon_n\left(\frac{1}{n}\frac{\partial \vg_n(\bar{\vtheta}_n)}{\partial \vtheta^\top}\right)\right]^{-1}\left(\frac{1}{n}\frac{\partial \vg_n(\widehat{\vtheta}_n)}{\partial \vtheta^\top}\right)^\top\mUpsilon_n\sqrt{n}\frac{1}{n}\vg_n(\vtheta_0).
   \end{aligned}
  \end{equation*}
    %
  where $\bar{\vtheta}_n$ is some between value. 
  
  Consider the gradient
  \begin{equation*}
  \begin{aligned}
  \frac{\partial \vg_n(\vtheta)}{\partial \vtheta ^\top} & = 
  \begin{pmatrix}
    \vepsi_n^\top\mP_{1n}^s \\
    \vepsi_n^\top\mP_{2n}^s \\
    \mH_n^\top
    \end{pmatrix}
  \begin{pmatrix}
    -\mW_n\vy_n & -\mX_n
  \end{pmatrix} \\
  & =  - (\mP_{1n}^{s\top} \vepsi_n, \mP_{2n}^{s\top}\vepsi_n, \mH_n^\top)^\top(\mW_n\vy_n, \mX_n) \\
  & = - \begin{pmatrix}
  \vepsi_n^\top\mP_{1n}^s\mW_n\vy_n & \vepsi_n^\top\mP_{1n}^s\mX_n \\
  \vepsi_n^\top\mP_{2n}^s\mW_n\vy_n & \vepsi_n^\top\mP_{2n}^s\mX_n \\
  \mH_n^\top\mW_n\vy_n & \mH_n^\top\mX_n
      \end{pmatrix}.
  \end{aligned}
  \end{equation*}
  
  We will show that $\frac{\partial \vg_n(\vtheta)}{\partial \vtheta ^\top} = \frac{1}{n}\mD_n + o_p(1)$, where $\mD_n = \frac{\partial \E\left[\vg_n(\vtheta_0)\right]}{\partial \vtheta^\top}$.Note that for $j = 1, 2$, 
  \begin{equation*}
  \begin{aligned}
  \frac{1}{n}\vepsi_n^\top \mP_{jn}\mW_n\vy_n  & = \frac{1}{n}\vepsi_n^\top \mP_{jn}^s\mW_n(\mS_0^{-1}\mX_n\vbeta_0 + \mS_0^{-1}\vepsi_n) \\
  & =\frac{1}{n}\vepsi_n^\top \mP_{jn}^s\mG_0\mX_n\vbeta_0 + \frac{1}{n}\vepsi_n^\top\mP_{jn}^s\mG_0\vepsi_n.
  \end{aligned}
  \end{equation*}
  
For the first element, and using Equation \eqref{eq:error-pop-slm}:
\begin{equation*}
\begin{aligned}
\frac{1}{n}\vepsi_n^\top \mP_{jn}^s\mG_0\mX_n\vbeta_0  & = \frac{1}{n}\left(\vd_n(\vtheta) + \mS_n\mS_0^{-1}\vepsi_n\right)^\top (\mP_{jn}^s\mG_0\mX_n\vbeta_0), \\
& =\vd_n(\vtheta)^\top\mP_{jn}^s\mG_0\mX_n\vbeta_0 + \frac{1}{n}\vepsi_n^\top\left(\mS_n\mS_0^{-1}\right)^\top\mP_{jn}^s\mG_0\mX_n\vbeta_0, \\
& = \vd_n(\vtheta)^\top\mP_{jn}^s\mG_0\mX_n\vbeta_0  + \frac{1}{n}\vepsi_n^\top\left[\mI_n + (\rho_0 - \rho)\mG_0\right]^\top\mP_{jn}^s\mG_0\mX_n\vbeta_0, \\
& = \vd_n(\vtheta)^\top\mP_{jn}^s\mG_0\mX_n\vbeta_0 + \frac{1}{n}\vepsi_n^\top\mP_{jn}^s\mG_0\mX_n\vbeta_0 + \frac{1}{n}(\rho_0 - \rho)\vepsi_n^\top\mG_0^\top\mP_{jn}^s\mG_0\mX_n\vbeta_0.
\end{aligned}
\end{equation*}

Then, 
\begin{equation*}
\begin{aligned}
\frac{1}{n}\vepsi_n^\top \mP_{jn}^s\mG_0\mX_n\vbeta_0 &\pto \frac{1}{n}\vd_n(\vtheta)^\top\mP_{jn}^s\mG_0\mX_n\vbeta_0 + \frac{1}{n}\E(\vepsi_n^\top\mP_{jn}^s\mG_0\mX_n\vbeta_0) +\frac{1}{n}(\rho_0 - \rho)\E(\vepsi_n^\top\mG_0^\top\mP_{jn}^s\mG_0\mX_n\vbeta_0), \\
&\pto \frac{1}{n}\vd_n(\vtheta)^\top\mP_{jn}^s\mG_0\mX_n\vbeta_0, 
\end{aligned}
\end{equation*}
%
uniformly in $\vtheta \in \mTheta$, since $\E(\vepsi_n^\top\mP_{jn}^s\mG_0\mX_n\vbeta_0) = o_p(1)$ and $\E(\vepsi_n^\top\mG_0^\top\mP_{jn}^s\mG_0\mX_n\vbeta_0) = o_p(1)$. For the second term, note that
\begin{equation*}
\begin{aligned}
\frac{1}{n}\vepsi_n^\top\mP_{jn}^s\mG_0\vepsi_n & = \frac{1}{n}\vd_n(\vtheta)^\top\mP_{jn}^s\mG_0\vepsi_n + \frac{1}{n}\vepsi_n^\top\mP^s_{jn}\mG_0\vepsi_n + (\rho_0 - \rho)\frac{1}{n}\vepsi_n^\top\mG_0^\top\mP^s_{jn}\mG_{0}\vepsi_n, \\
& \pto \frac{1}{n}\E(\vd_n(\vtheta)^\top\mP_{jn}^s\mG_0\vepsi_n) + \frac{1}{n}\E(\vepsi_n^\top\mP^s_{jn}\mG_0\vepsi_n) + (\rho_0 - \rho)\frac{1}{n}\E(\vepsi_n^\top\mG_0^\top\mP^s_{jn}\mG_{0}\vepsi_n), \\
& = \frac{\sigma_0^2}{n}\tr(\mP_{jn}^s\mG_0) + (\rho_0 - \rho)\frac{\sigma_0^2}{n}\tr(\mG_0^\top\mP_{jn}^s\mG_0).
\end{aligned}
\end{equation*}

Continuing with the other elements of the gradient:
\begin{equation*}
  \begin{aligned}
    \frac{1}{n}\vepsi_n^\top\mP_{jn}^s\mX_n & = \frac{1}{n}\left[\vd_n(\vtheta)+ \mS_n\mS_0^{-1}\vepsi_n\right]^\top\mP_{jn}^s\mX_n, \\
    & = \frac{1}{n}\vd_n(\vtheta)^\top\mP_{jn}\mX_n  + \frac{1}{n}\vepsi_n^\top\mP_{jn}^s\mX_n + \frac{1}{n}(\rho_0 - \rho)\vepsi_n^\top\mG_0^\top\mP^s_{jn}\mX_n, \\
    & \pto  \frac{1}{n}\vd_n(\vtheta)^\top\mP_{jn}\mX_n.
  \end{aligned}
\end{equation*}

Similarly:
\begin{equation*}
  \begin{aligned}
    \frac{1}{n}\mH_n^\top\mW_n\vy_n &  = \frac{1}{n}\mH_n^\top\mW_n\left(\mS_0^{-1}\mX_n\vbeta_0 + \mS_0^{-1}\vepsi_n\right), \\
    & = \frac{1}{n}\mH_n^\top\mW_n\mS_0^{-1}\mX_n\vbeta_0 +  \frac{1}{n}\mH_n^\top\mW_n\mS_0^{-1}\vepsi_n, \\
    & \pto \frac{1}{n}\mH_n^\top\mG_0\mX_n\vbeta_0.
  \end{aligned}
\end{equation*}

Thus, 
\begin{equation*}
  \frac{1}{n}\frac{\partial \vg_n(\vtheta)}{\partial \vtheta^\top} \pto 
  \begin{pmatrix}
    \frac{1}{n}\vd_n(\vtheta)^\top\mP_{1s}^s\mG_0\mX_n\vbeta_0 + \frac{\sigma_0^2}{n}\tr(\mP_{1n}^s\mG_0) + (\rho_0 - \rho)\frac{\sigma_0^2}{n}\tr(\mG_0^\top\mP_{1n}^s\mG_0) & \frac{1}{n}\vd_n(\vtheta)^\top\mP_{1s}^s\mX_n \\
        \frac{1}{n}\vd_n(\vtheta)^\top\mP_{2s}^s\mG_0\mX_n\vbeta_0 + \frac{\sigma_0^2}{n}\tr(\mP_{2n}^s\mG_0) + (\rho_0 - \rho)\frac{\sigma_0^2}{n}\tr(\mG_0^\top\mP_{2n}^s\mG_0) & \frac{1}{n}\vd_n(\vtheta)^\top\mP_{2s}^s\mX_n \\
      \frac{1}{n}\mH_n^\top\mG_0\mX_n\vbeta_0 & \frac{1}{n}\mH_n^\top\mX_n  
  \end{pmatrix}
\end{equation*}

At $\vtheta = \vtheta_0$, $\vd_n(\vtheta) = \vzeros$, Then
\begin{equation*}
\begin{aligned}
  \frac{1}{n}\frac{\partial \vg_n(\vtheta)}{\partial \vtheta^\top} & \pto  -\frac{1}{n}\mD_n, \\
  &\pto \frac{1}{n}\begin{pmatrix}
  \sigma_0^2\tr(\mP_{1n}^s\mG_0) & \vzeros \\
   \sigma_0^2\tr(\mP_{2n}^s\mG_0)  & \vzeros \\
      \mH_n^\top\mG_0\mX_n\vbeta_0 & \mH_n^\top\mX_n  
  \end{pmatrix}.
        \end{aligned}
\end{equation*}
  
It follows that 
  \begin{equation*}
  \left(\frac{1}{n}\frac{\partial \vg_n(\bar{\vtheta}_n)}{\partial \vtheta^\top}\right) = -\frac{1}{n}\mD_n + o_p(1),
  \end{equation*}
  %
  where $\frac{\partial \vg_n(\widehat{\vtheta})}{\partial \vtheta^\top} = O_p(1)$ and $\frac{1}{n}\mD_n = O(1)$. 
  
  In addition, $\mUpsilon_n - \mUpsilon_0 = o_p(1)$. In addition, by central limit Theorem for linear-quadratic function \ref{teo:clt_quadratic}
  \begin{equation*}
    \begin{aligned}
      \frac{1}{\sqrt{n}}\vg_n(\vtheta_0) & = \frac{1}{\sqrt{n}}
                                              \begin{pmatrix}
                                               \vepsi_n(\vtheta_0)\mP_{1n}\vepsi_n(\vtheta_0)\\
                                               \vepsi_n(\vtheta_0)\mP_{2n}\vepsi_n(\vtheta_0)\\
                                               \mH_n^\top\vepsi_n(\vtheta_0).
                                              \end{pmatrix} \\
                                        & = \rN\left(\vzeros, \lim_{n\to\infty}\frac{1}{n}\mOmega_n\right),
                                        \end{aligned}
  \end{equation*}
  %
  and
  \begin{equation*}
    \begin{aligned}
      \vzeta_n = -\mOmega_n^{-1/2}\frac{1}{\sqrt{n}}\vg_n(\vtheta_0)\dto \rN(\vzeros, \mI_{2+k_x})
    \end{aligned}
      \end{equation*}
    Thus:
    \begin{equation*}
      \sqrt{n}\left(\widehat{\vtheta}_n - \vtheta_0\right) = \left[\left(\frac{1}{n}\mD_n^\top\right)\mUpsilon_0\left(\frac{1}{n}\mD_n\right)\right]^{-1}\left(\frac{1}{n}\mD_n^\top\right)\mUpsilon_0\vzeta_n + o_p(1).
    \end{equation*}


%*************************************************
\subsection{OGMM Estimator}
%*************************************************

Different weighting matrices $\mUpsilon_n$ lead to different consistent estimators, each with potentially different asymptotic covariance structures. As established in Theorem \ref{teo:GMME-SLM}, the GMME remains consistent for any sequence $\mUpsilon_n$, satisfying $\mUpsilon_n \to \mUpsilon_0$. However, among the class of GMM estimators indexed by $\mUpsilon_n$, our goal is to select the one that minimizes the asymptotic variance given the moment conditions $\vg_n(\vtheta)$. 

A particularly important case arises when the weighting matrix is chosen as $\mUpsilon_n = \left(\frac{1}{n}\widehat{\mOmega}_n\right)^{-1}$. Under this specification, the GMME attains its optimal asymptotic efficiency and is referred to as the Optimal GMM Estimator (OGMME). The following Theorem provides the asymptotic properties of the OGMME for the SLM under homoskedasticity \citep{lee2007gmm}.

%----------------------------------------------------------------------------------
\begin{theorem}[OGMM estimator for SLM under homoskedasticity \citep{lee2007gmm}]
In addition to  Assumptions \ref{assumption:lee2007-1}, suppose that 
\begin{enumerate}
  \item the limit of $(1/n)\mOmega_n$ exists and is a nonsingular matrix, 
  \item $\left(\widehat{\mOmega}_n / n\right)^{-1} -\left(\mOmega_n / n\right)^{-1} = o_p(1)$, 
\end{enumerate}
%
then the OGMM, $\widehat{\vtheta}_{n}$ derived from $\min_{\vtheta \in \mTheta}\vg_n^\top(\vtheta)\widehat{\mOmega}_n^{-1}\vg_n(\vtheta)$ with $\mP_{jn}'s$ from $\calP_{1n}$ has the asymptotic distribution
\begin{equation*}
\sqrt{n}\left(\widehat{\vtheta}_n - \vtheta_0\right) \dto \rN(\vzeros, \mSigma^o)
\end{equation*}
%
where
\begin{equation*}
\begin{aligned}
  \mSigma^o = &  \left[\lim_{n\to\infty}\frac{1}{n}\mD_n^\top\mOmega_n\mD_n\right]^{-1}.
  \end{aligned}
\end{equation*}

Furthermore:
\begin{equation*}
\vg_n^\top(\widehat{\vtheta})\widehat{\mOmega}_n^{-1}\vg_n(\widehat{\vtheta}) \dto \chi^2_{(2 + k_x)- (k + 1)}
\end{equation*}
\end{theorem}
%---------------------------------------------------------------------------------



%*************************************************
\subsection{Coding GMME and OGMME for SLM}
%*************************************************

First, we create a function that returns the moment conditions presented in  Equation \eqref{eq:moments-lee-2007}, and Jacobian matrix in Equation \eqref{eq:Jacobian-gmm-slm} (both divided by $n$). This function is next used to construct the objective function to be minimized. 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Trace function}
\hlstd{tr} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{A}\hlstd{)} \hlkwd{return}\hlstd{(}\hlkwd{sum}\hlstd{(}\hlkwd{diag}\hlstd{(A)))}

\hlcom{# Create moments as in Lee 2007}
\hlstd{moments.lee2007} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{theta}\hlstd{,} \hlkwc{y}\hlstd{,} \hlkwc{X}\hlstd{,} \hlkwc{H}\hlstd{,} \hlkwc{W}\hlstd{)\{}
  \hlcom{# y: vector of dependent variables}
  \hlcom{# X: matrix of exogenous variables}
  \hlcom{# H: matrix of linear instrument}
  \hlcom{# W: Spatial weight matrix}
  \hlstd{k}        \hlkwb{<-} \hlkwd{ncol}\hlstd{(X)}
  \hlstd{n}        \hlkwb{<-} \hlkwd{nrow}\hlstd{(X)}
  \hlstd{rho}      \hlkwb{<-} \hlstd{theta[}\hlnum{1L}\hlstd{]}
  \hlstd{beta}     \hlkwb{<-} \hlstd{theta[(}\hlnum{2L}\hlopt{:}\hlstd{(k} \hlopt{+} \hlnum{1}\hlstd{))]}
  \hlstd{I}        \hlkwb{<-} \hlkwd{diag}\hlstd{(n)}
  \hlstd{S}        \hlkwb{<-} \hlstd{I} \hlopt{-}  \hlstd{rho} \hlopt{*} \hlstd{W}
  \hlstd{epsi}     \hlkwb{<-} \hlkwd{crossprod}\hlstd{(}\hlkwd{t}\hlstd{(S), y)} \hlopt{-} \hlkwd{crossprod}\hlstd{(}\hlkwd{t}\hlstd{(X), beta)}
  \hlstd{P1}       \hlkwb{<-} \hlstd{W}
  \hlcom{#W2       <- crossprod(t(W), W)}
  \hlstd{W2}       \hlkwb{<-} \hlkwd{tcrossprod}\hlstd{(W, W)}
  \hlstd{P2}       \hlkwb{<-} \hlstd{W2} \hlopt{-} \hlstd{(}\hlkwd{tr}\hlstd{(W2)} \hlopt{/} \hlstd{n)} \hlopt{*} \hlstd{I}
  \hlstd{g.lin}    \hlkwb{<-} \hlkwd{crossprod}\hlstd{(H, epsi)}            \hlcom{# k_x * 1}
  \hlstd{g.q1}     \hlkwb{<-} \hlkwd{crossprod}\hlstd{(epsi, P1)} \hlopt{%*%} \hlstd{epsi}  \hlcom{# 1*1}
  \hlstd{g.q2}     \hlkwb{<-} \hlkwd{crossprod}\hlstd{(epsi, P2)} \hlopt{%*%} \hlstd{epsi}  \hlcom{# 1*1}
  \hlstd{g}        \hlkwb{<-} \hlkwd{rbind}\hlstd{(g.q1, g.q2, g.lin)}

  \hlcom{# Gradient }
  \hlstd{P1s} \hlkwb{<-} \hlstd{P1} \hlopt{+} \hlkwd{t}\hlstd{(P1)}
  \hlstd{P2s} \hlkwb{<-} \hlstd{P2} \hlopt{+} \hlkwd{t}\hlstd{(P2)}
  \hlcom{# (2 * k_k)* (k + 1)}
  \hlstd{D}   \hlkwb{<-}  \hlopt{-}\hlnum{1} \hlopt{*} \hlkwd{rbind}\hlstd{(}\hlkwd{crossprod}\hlstd{(epsi, P1s),}
                     \hlkwd{crossprod}\hlstd{(epsi, P2s),}
                     \hlkwd{t}\hlstd{(H))} \hlopt{%*%} \hlkwd{cbind}\hlstd{(W} \hlopt{%*%} \hlstd{y, X)}

  \hlcom{# Return results (note that they are divided by n)}
  \hlstd{out} \hlkwb{<-} \hlkwd{list}\hlstd{(}\hlkwc{g} \hlstd{= g} \hlopt{/}\hlstd{n ,} \hlkwc{D} \hlstd{= D} \hlopt{/} \hlstd{n)}
  \hlkwd{return}\hlstd{(out)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

Next, we define the function to be optimized: $Q_n(\vtheta) = \vg_n^\top\mUpsilon_n\vg_n(\vtheta)$. The function \texttt{Qmin} returns the negative of  $Q_n(\vtheta)$ along with its gradient to speed up the optimization process. The function's negative is returned because the optimization algorithm used latter maximizes rather than minimizes. 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Objective function to minimize}
\hlstd{Qmin} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{start}\hlstd{,} \hlkwc{y}\hlstd{,} \hlkwc{X}\hlstd{,} \hlkwc{H}\hlstd{,} \hlkwc{W}\hlstd{,} \hlkwc{Psi}\hlstd{,} \hlkwc{gradient}\hlstd{)\{}
  \hlcom{# Thus function returns the negative of:}
  \hlcom{#  - objective function g'Psi g}
  \hlcom{#  - gradient of g'Upsi g}
  \hlstd{g.hat} \hlkwb{<-} \hlkwd{moments.lee2007}\hlstd{(}\hlkwc{theta} \hlstd{= start,} \hlkwc{y} \hlstd{= y,} \hlkwc{X} \hlstd{= X,} \hlkwc{H} \hlstd{= H,} \hlkwc{W} \hlstd{= W)}
  \hlstd{Q}     \hlkwb{<-} \hlopt{-}\hlnum{1} \hlopt{*} \hlkwd{crossprod}\hlstd{(g.hat}\hlopt{$}\hlstd{g, Psi)} \hlopt{%*%} \hlstd{g.hat}\hlopt{$}\hlstd{g}
  \hlkwa{if} \hlstd{(gradient)\{}
    \hlstd{D}  \hlkwb{<-} \hlstd{g.hat}\hlopt{$}\hlstd{D} \hlcom{# D.hat is (2 * k_k)* (k + 1)}
    \hlstd{Gr} \hlkwb{<-} \hlopt{-}\hlnum{2} \hlopt{*} \hlkwd{crossprod}\hlstd{(D, Psi)} \hlopt{%*%} \hlstd{g.hat}\hlopt{$}\hlstd{g}
    \hlkwd{attr}\hlstd{(Q,} \hlstr{'gradient'}\hlstd{)} \hlkwb{<-} \hlkwd{as.vector}\hlstd{(Gr)}
  \hlstd{\}}
  \hlkwd{return}\hlstd{(Q)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

The following function returns the variance-covariance matrix of the moments given in Equation \eqref{eq:omega-slm-hat} (divided by $n$):
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Create var-cov of moments: Omega}
\hlstd{make.vmom} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{b.hat}\hlstd{,} \hlkwc{y}\hlstd{,} \hlkwc{X}\hlstd{,} \hlkwc{H}\hlstd{,} \hlkwc{W}\hlstd{)\{}
  \hlstd{k}        \hlkwb{<-} \hlkwd{ncol}\hlstd{(X)}
  \hlstd{n}        \hlkwb{<-} \hlkwd{nrow}\hlstd{(X)}
  \hlstd{k_x}      \hlkwb{<-} \hlkwd{ncol}\hlstd{(H)}
  \hlstd{rho}      \hlkwb{<-} \hlstd{b.hat[}\hlnum{1L}\hlstd{]}
  \hlstd{beta}     \hlkwb{<-} \hlstd{b.hat[(}\hlnum{2L}\hlopt{:}\hlstd{(k} \hlopt{+} \hlnum{1}\hlstd{))]}
  \hlstd{I}        \hlkwb{<-} \hlkwd{diag}\hlstd{(n)}
  \hlstd{S}        \hlkwb{<-} \hlstd{I} \hlopt{-}  \hlstd{rho} \hlopt{*} \hlstd{W}
  \hlstd{epsi}     \hlkwb{<-} \hlkwd{crossprod}\hlstd{(}\hlkwd{t}\hlstd{(S), y)} \hlopt{-} \hlkwd{crossprod}\hlstd{(}\hlkwd{t}\hlstd{(X), beta)}
  \hlstd{sigma2}   \hlkwb{<-} \hlkwd{as.numeric}\hlstd{(}\hlkwd{crossprod}\hlstd{(epsi)} \hlopt{/} \hlstd{n)}
  \hlstd{P1}       \hlkwb{<-} \hlstd{W}
  \hlcom{#W2       <- crossprod(t(W), W)}
  \hlstd{W2}       \hlkwb{<-} \hlkwd{tcrossprod}\hlstd{(}\hlkwd{t}\hlstd{(W), W)}
  \hlstd{P2}       \hlkwb{<-} \hlstd{W2} \hlopt{-} \hlstd{(}\hlkwd{tr}\hlstd{(W2)} \hlopt{/} \hlstd{n)} \hlopt{*} \hlstd{I}
  \hlstd{P1s}      \hlkwb{<-} \hlstd{P1} \hlopt{+} \hlkwd{t}\hlstd{(P1)}
  \hlstd{P2s}      \hlkwb{<-} \hlstd{P2} \hlopt{+} \hlkwd{t}\hlstd{(P2)}

  \hlcom{# Construct V: (2 + k.x) * (2 + k.x)}
  \hlstd{V22} \hlkwb{<-} \hlstd{(}\hlnum{1} \hlopt{/} \hlstd{sigma2)} \hlopt{*} \hlkwd{crossprod}\hlstd{(H)}         \hlcom{# k_x + k_x}
  \hlstd{Delta} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{0}\hlstd{,} \hlkwc{nrow} \hlstd{=} \hlnum{2}\hlstd{,} \hlkwc{ncol} \hlstd{=} \hlnum{2}\hlstd{)}
  \hlstd{Delta[}\hlnum{1}\hlstd{,} \hlnum{1}\hlstd{]} \hlkwb{<-} \hlkwd{tr}\hlstd{(P1} \hlopt{%*%} \hlstd{P1s)}
  \hlstd{Delta[}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{]} \hlkwb{<-} \hlkwd{tr}\hlstd{(P1} \hlopt{%*%} \hlstd{P2s)}
  \hlstd{Delta[}\hlnum{2}\hlstd{,} \hlnum{1}\hlstd{]} \hlkwb{<-} \hlkwd{tr}\hlstd{(P2} \hlopt{%*%} \hlstd{P1s)}
  \hlstd{Delta[}\hlnum{2}\hlstd{,} \hlnum{2}\hlstd{]} \hlkwb{<-} \hlkwd{tr}\hlstd{(P2} \hlopt{%*%} \hlstd{P2s)}
  \hlstd{V} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{0}\hlstd{,} \hlkwc{nrow} \hlstd{= (k_x} \hlopt{+} \hlnum{2}\hlstd{),} \hlkwc{ncol} \hlstd{= (k_x} \hlopt{+} \hlnum{2}\hlstd{))}
  \hlstd{V[}\hlnum{1}\hlopt{:}\hlnum{2}\hlstd{,} \hlnum{1}\hlopt{:}\hlnum{2}\hlstd{]} \hlkwb{<-} \hlstd{Delta}
  \hlstd{V[}\hlnum{3}\hlopt{:}\hlstd{(k_x} \hlopt{+} \hlnum{2}\hlstd{),} \hlnum{3}\hlopt{:}\hlstd{(k_x} \hlopt{+} \hlnum{2}\hlstd{)]} \hlkwb{<-} \hlstd{V22}
  \hlstd{V} \hlkwb{<-} \hlstd{sigma2}\hlopt{^}\hlnum{2} \hlopt{*} \hlstd{V}
  \hlcom{# Construct first part of Omega}
  \hlstd{omega}   \hlkwb{<-} \hlkwd{cbind}\hlstd{(}\hlkwd{diag}\hlstd{(P1),} \hlkwd{diag}\hlstd{(P2))} \hlcom{# n * 2}
  \hlstd{mu4.hat} \hlkwb{<-} \hlkwd{sum}\hlstd{(epsi}\hlopt{^}\hlnum{4}\hlstd{)} \hlopt{/} \hlstd{n}
  \hlstd{mu3.hat} \hlkwb{<-} \hlkwd{sum}\hlstd{(epsi}\hlopt{^}\hlnum{3}\hlstd{)} \hlopt{/} \hlstd{n}
  \hlstd{Vp1} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{0}\hlstd{,} \hlkwc{nrow} \hlstd{= (k_x} \hlopt{+} \hlnum{2}\hlstd{),} \hlkwc{ncol} \hlstd{= (k_x} \hlopt{+} \hlnum{2}\hlstd{))}
  \hlstd{Vp1[}\hlnum{1}\hlopt{:}\hlnum{2}\hlstd{,} \hlnum{1}\hlopt{:}\hlnum{2}\hlstd{]}         \hlkwb{<-} \hlstd{(mu4.hat} \hlopt{-} \hlnum{3} \hlopt{*} \hlstd{sigma2}\hlopt{^}\hlnum{2}\hlstd{)} \hlopt{*} \hlkwd{crossprod}\hlstd{(omega)}   \hlcom{# 2 * 2 }
  \hlstd{Vp1[}\hlnum{1}\hlopt{:}\hlnum{2}\hlstd{,} \hlnum{3}\hlopt{:}\hlstd{(k_x} \hlopt{+} \hlnum{2}\hlstd{)]} \hlkwb{<-} \hlstd{mu3.hat} \hlopt{*} \hlkwd{crossprod}\hlstd{(omega, H)}
  \hlstd{Vp1[}\hlnum{3}\hlopt{:}\hlstd{(k_x} \hlopt{+} \hlnum{2}\hlstd{),} \hlnum{1}\hlopt{:}\hlnum{2}\hlstd{]} \hlkwb{<-} \hlstd{mu3.hat} \hlopt{*} \hlkwd{crossprod}\hlstd{(H, omega)}
  \hlstd{Omega}                 \hlkwb{<-} \hlstd{Vp1} \hlopt{+} \hlstd{V}
  \hlstd{Omega}                 \hlkwb{<-} \hlstd{Omega} \hlopt{/} \hlstd{n}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

With all the previous function, we now crate the main function to estimate either the GMM or OGMM estimator. If \texttt{estimator = "gmm"}, the function uses an identity matrix as weighting matrix, $\mUpsilon_n = \mI_{2 + k_x}$, for the optimization procedure of $Q_n(\vtheta)$. If \texttt{estimator = "ogmm"}, the GMM estimates are used in a second step to construct $\widehat{\mOmega}_n$ and optimize $Q_n(\vtheta) = \vg_n(\vtheta)^\top\widehat{\mOmega}^{-1}_n\vg_n(\vtheta)$.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Function to estimate the SLM using GMME or OGMME}
\hlstd{slm.gmm} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{formula}\hlstd{,} \hlkwc{data}\hlstd{,} \hlkwc{W}\hlstd{,} \hlkwc{instruments} \hlstd{=} \hlnum{2}\hlstd{,}
                    \hlkwc{estimator} \hlstd{=} \hlkwd{c}\hlstd{(}\hlstr{"gmm"}\hlstd{,} \hlstr{"ogmm"}\hlstd{),}
                    \hlkwc{gradient} \hlstd{=} \hlnum{TRUE}\hlstd{)\{}
  \hlcom{# Model Frame}
  \hlstd{callT}    \hlkwb{<-} \hlkwd{match.call}\hlstd{(}\hlkwc{expand.dots} \hlstd{=} \hlnum{TRUE}\hlstd{)}
  \hlstd{mf}       \hlkwb{<-} \hlstd{callT}
  \hlstd{m}        \hlkwb{<-} \hlkwd{match}\hlstd{(}\hlkwd{c}\hlstd{(}\hlstr{"formula"}\hlstd{,} \hlstr{"data"}\hlstd{),} \hlkwd{names}\hlstd{(mf),} \hlnum{0L}\hlstd{)}
  \hlstd{mf}       \hlkwb{<-} \hlstd{mf[}\hlkwd{c}\hlstd{(}\hlnum{1L}\hlstd{, m)]}
  \hlstd{mf[[}\hlnum{1L}\hlstd{]]} \hlkwb{<-} \hlkwd{as.name}\hlstd{(}\hlstr{"model.frame"}\hlstd{)}
  \hlstd{mf}       \hlkwb{<-} \hlkwd{eval}\hlstd{(mf,} \hlkwd{parent.frame}\hlstd{())}

  \hlcom{# Estimator}
  \hlstd{estimator} \hlkwb{<-} \hlkwd{match.arg}\hlstd{(estimator)}

  \hlcom{# Get variables and globals}
  \hlstd{y}  \hlkwb{<-} \hlkwd{model.response}\hlstd{(mf)}
  \hlstd{X}  \hlkwb{<-} \hlkwd{model.matrix}\hlstd{(formula, mf)}
  \hlstd{n}  \hlkwb{<-} \hlkwd{nrow}\hlstd{(X)}
  \hlstd{Wy} \hlkwb{<-} \hlstd{W} \hlopt{%*%} \hlstd{y}
  \hlstd{sn} \hlkwb{<-} \hlkwd{nrow}\hlstd{(W)}
  \hlkwa{if} \hlstd{(n} \hlopt{!=} \hlstd{sn)} \hlkwd{stop}\hlstd{(}\hlstr{"number of spatial units in W is different to the number of data"}\hlstd{)}

  \hlcom{# Linear instruments}
  \hlstd{H} \hlkwb{<-} \hlkwd{cbind}\hlstd{(X,} \hlkwd{make.H}\hlstd{(}\hlkwc{W} \hlstd{= W,} \hlkwc{X} \hlstd{= X,} \hlkwc{l} \hlstd{= instruments))}

  \hlcom{# Starting values for optimization}
  \hlstd{start} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlkwd{cor}\hlstd{(Wy, y),} \hlkwd{coef}\hlstd{(}\hlkwd{lm}\hlstd{(y} \hlopt{~} \hlstd{X} \hlopt{-} \hlnum{1}\hlstd{)))}
  \hlkwd{names}\hlstd{(start)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"Wy"}\hlstd{,} \hlkwd{colnames}\hlstd{(X))}

  \hlcom{# GMM estimator with weighting matrix using a identity matrix  }
  \hlstd{k_x} \hlkwb{<-} \hlkwd{ncol}\hlstd{(H)}
  \hlstd{Psi} \hlkwb{<-} \hlkwd{diag}\hlstd{(}\hlnum{2} \hlopt{+} \hlstd{k_x)}
  \hlkwd{require}\hlstd{(}\hlstr{"maxLik"}\hlstd{)}
  \hlstd{opt} \hlkwb{<-} \hlkwd{maxLik}\hlstd{(}\hlkwc{logLik} \hlstd{= Qmin,}
                \hlkwc{start} \hlstd{= start,}
                \hlkwc{method} \hlstd{=} \hlstr{"bfgs"}\hlstd{,}
                \hlkwc{y} \hlstd{= y,}
                \hlkwc{X} \hlstd{= X,}
                \hlkwc{H} \hlstd{= H,}
                \hlkwc{W} \hlstd{= W,}
                \hlkwc{Psi} \hlstd{= Psi,}
                \hlkwc{gradient} \hlstd{= gradient,}
                \hlkwc{print.level} \hlstd{=} \hlnum{3}\hlstd{,}
                \hlkwc{finalHessian} \hlstd{=} \hlnum{FALSE}\hlstd{)}

  \hlcom{# OGMM: GMM estimator with weighting matrix using the inverse of the var-cov of moment functions}
  \hlkwa{if} \hlstd{(estimator} \hlopt{==} \hlstr{"ogmm"}\hlstd{)\{}
    \hlcom{# Compute Omega.hat/n}
    \hlstd{Omega.hat} \hlkwb{<-} \hlkwd{make.vmom}\hlstd{(}\hlkwd{coef}\hlstd{(opt),} \hlkwc{y} \hlstd{= y,} \hlkwc{X} \hlstd{= X,} \hlkwc{H} \hlstd{= H,} \hlkwc{W} \hlstd{= W)}
    \hlstd{Psi}  \hlkwb{<-} \hlkwd{solve}\hlstd{(Omega.hat)}
    \hlstd{opt} \hlkwb{<-} \hlkwd{maxLik}\hlstd{(}\hlkwc{logLik} \hlstd{= Qmin,}
                  \hlkwc{start} \hlstd{=} \hlkwd{coef}\hlstd{(opt),}
                  \hlkwc{method} \hlstd{=} \hlstr{"bfgs"}\hlstd{,}
                  \hlkwc{y} \hlstd{= y,}
                  \hlkwc{X} \hlstd{= X,}
                  \hlkwc{H} \hlstd{= H,}
                  \hlkwc{W} \hlstd{= W,}
                  \hlkwc{Psi} \hlstd{= Psi,}
                  \hlkwc{gradient} \hlstd{= gradient,}
                  \hlkwc{print.level} \hlstd{=} \hlnum{3}\hlstd{,}
                  \hlkwc{finalHessian} \hlstd{=} \hlnum{FALSE}\hlstd{)}
  \hlstd{\}}


  \hlstd{results} \hlkwb{<-} \hlkwd{structure}\hlstd{(}
    \hlkwd{list}\hlstd{(}
      \hlkwc{coefficients} \hlstd{=} \hlkwd{coef}\hlstd{(opt),}
      \hlkwc{call} \hlstd{= callT,}
      \hlkwc{X} \hlstd{= X,}
      \hlkwc{H} \hlstd{= H,}
      \hlkwc{y} \hlstd{= y,}
      \hlkwc{Psi} \hlstd{= Psi,}
      \hlkwc{W} \hlstd{= W,}
      \hlkwc{estimator} \hlstd{= estimator}
    \hlstd{),}
    \hlkwc{class} \hlstd{=} \hlstr{"gmm.slm"}
  \hlstd{)}
  \hlkwd{return}\hlstd{(results)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

The following function returns the matrix $\mD_n$ evaluated at $\widehat{\vtheta}_n$, given in Equation \eqref{eq:D-gmm-slm}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Create D matrix for asymptotic distribution}
\hlstd{make.D} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{rho}\hlstd{,} \hlkwc{beta}\hlstd{,} \hlkwc{y}\hlstd{,} \hlkwc{X}\hlstd{,} \hlkwc{H}\hlstd{,} \hlkwc{W}\hlstd{)\{}
  \hlstd{n}      \hlkwb{<-} \hlkwd{nrow}\hlstd{(X)}
  \hlstd{k}      \hlkwb{<-} \hlkwd{ncol}\hlstd{(X)}
  \hlstd{k_x}    \hlkwb{<-} \hlkwd{ncol}\hlstd{(H)}
  \hlstd{I}      \hlkwb{<-} \hlkwd{diag}\hlstd{(n)}
  \hlstd{S}      \hlkwb{<-} \hlstd{I} \hlopt{-}  \hlstd{rho} \hlopt{*} \hlstd{W}
  \hlstd{epsi}   \hlkwb{<-} \hlkwd{crossprod}\hlstd{(}\hlkwd{t}\hlstd{(S), y)} \hlopt{-} \hlkwd{crossprod}\hlstd{(}\hlkwd{t}\hlstd{(X), beta)}
  \hlstd{sigma2} \hlkwb{<-} \hlkwd{as.numeric}\hlstd{(}\hlkwd{crossprod}\hlstd{(epsi)} \hlopt{/} \hlstd{n)}
  \hlstd{P1}     \hlkwb{<-} \hlstd{W}
  \hlcom{#W2     <- crossprod(t(W), W)}
  \hlstd{W2}     \hlkwb{<-} \hlkwd{tcrossprod}\hlstd{(W, W)}
  \hlstd{P2}     \hlkwb{<-} \hlstd{W2} \hlopt{-} \hlstd{(}\hlkwd{tr}\hlstd{(W2)} \hlopt{/} \hlstd{n)} \hlopt{*} \hlkwd{diag}\hlstd{(n)}
  \hlstd{P1s}    \hlkwb{<-} \hlstd{P1} \hlopt{+} \hlkwd{t}\hlstd{(P1)}
  \hlstd{P2s}    \hlkwb{<-} \hlstd{P2} \hlopt{+} \hlkwd{t}\hlstd{(P2)}
  \hlstd{G}      \hlkwb{<-} \hlstd{W} \hlopt{%*%} \hlkwd{solve}\hlstd{(S)}

  \hlcom{# Gen D}
  \hlstd{D} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{0}\hlstd{,} \hlkwc{nrow} \hlstd{= (k_x} \hlopt{+} \hlnum{2}\hlstd{) ,} \hlkwc{ncol} \hlstd{= k} \hlopt{+} \hlnum{1}\hlstd{)}
  \hlkwd{rownames}\hlstd{(D)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"q1"}\hlstd{,} \hlstr{"q2"}\hlstd{,} \hlkwd{colnames}\hlstd{(H))}
  \hlkwd{colnames}\hlstd{(D)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"Wy"}\hlstd{,} \hlkwd{colnames}\hlstd{(X))}
  \hlstd{D[}\hlnum{1}\hlstd{,} \hlnum{1}\hlstd{]} \hlkwb{<-} \hlstd{sigma2} \hlopt{*} \hlkwd{tr}\hlstd{(P1s} \hlopt{%*%} \hlstd{G)}
  \hlstd{D[}\hlnum{2}\hlstd{,} \hlnum{1}\hlstd{]} \hlkwb{<-} \hlstd{sigma2} \hlopt{*} \hlkwd{tr}\hlstd{(P2s} \hlopt{%*%} \hlstd{G)}
  \hlstd{D[}\hlnum{3}\hlopt{:}\hlstd{(k_x} \hlopt{+} \hlnum{2}\hlstd{),} \hlnum{1}\hlstd{]} \hlkwb{<-} \hlkwd{t}\hlstd{(H)} \hlopt{%*%} \hlstd{(G} \hlopt{%*%} \hlstd{X} \hlopt{%*%} \hlstd{beta)}
  \hlstd{D[}\hlnum{3}\hlopt{:}\hlstd{(k_x} \hlopt{+} \hlnum{2}\hlstd{),} \hlnum{2}\hlopt{:}\hlstd{(k} \hlopt{+} \hlnum{1}\hlstd{)]}  \hlkwb{<-} \hlkwd{t}\hlstd{(H)} \hlopt{%*%} \hlstd{X}
  \hlkwd{return}\hlstd{(D)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

The following functions provides the S3 methods \texttt{vcov}, \texttt{summary}, and \texttt{print.summary}. The \texttt{vcov} function for \texttt{gmm.slm} class allows to compute the VC matrix using either the asymptotic matrix $\mD_n$ evaluated at $\widehat{\vtheta}$ or the gradient evaluated at $\widehat{\vtheta}$.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Variance-covariance matrix}
\hlstd{vcov.gmm.slm} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{object}\hlstd{,} \hlkwc{D} \hlstd{=} \hlkwd{c}\hlstd{(}\hlstr{"population"}\hlstd{,} \hlstr{"gradient"}\hlstd{),} \hlkwc{...}\hlstd{)\{}
  \hlstd{estimator} \hlkwb{<-} \hlstd{object}\hlopt{$}\hlstd{estimator}
  \hlstd{D.type} \hlkwb{<-} \hlkwd{match.arg}\hlstd{(D)}
  \hlstd{X}      \hlkwb{<-} \hlstd{object}\hlopt{$}\hlstd{X}
  \hlstd{H}      \hlkwb{<-} \hlstd{object}\hlopt{$}\hlstd{H}
  \hlstd{y}      \hlkwb{<-} \hlstd{object}\hlopt{$}\hlstd{y}
  \hlstd{W}      \hlkwb{<-} \hlstd{object}\hlopt{$}\hlstd{W}
  \hlstd{k}      \hlkwb{<-} \hlkwd{ncol}\hlstd{(X)}
  \hlstd{n}      \hlkwb{<-} \hlkwd{nrow}\hlstd{(X)}
  \hlstd{b.hat}  \hlkwb{<-} \hlstd{object}\hlopt{$}\hlstd{coefficients}
  \hlstd{rho}    \hlkwb{<-} \hlstd{b.hat[}\hlnum{1L}\hlstd{]}
  \hlstd{beta}   \hlkwb{<-} \hlstd{b.hat[(}\hlnum{2L}\hlopt{:}\hlstd{(k} \hlopt{+} \hlnum{1}\hlstd{))]}
  \hlcom{# Matrix D is (k_x + 2) * (k + 1)}
  \hlkwa{if} \hlstd{(estimator} \hlopt{==} \hlstr{"gmm"}\hlstd{)\{}
    \hlkwa{if} \hlstd{(D.type} \hlopt{==} \hlstr{"population"}\hlstd{)\{}
      \hlstd{D}     \hlkwb{<-} \hlkwd{make.D}\hlstd{(}\hlkwc{rho} \hlstd{= rho,} \hlkwc{beta} \hlstd{= beta,} \hlkwc{y} \hlstd{= y,} \hlkwc{X} \hlstd{= X,} \hlkwc{H} \hlstd{= H,} \hlkwc{W} \hlstd{= W)} \hlopt{/} \hlstd{n}
    \hlstd{\}}
    \hlkwa{if} \hlstd{(D.type} \hlopt{==} \hlstr{"gradient"}\hlstd{)\{}
      \hlstd{D}     \hlkwb{<-} \hlkwd{moments.lee2007}\hlstd{(b.hat,} \hlkwc{y} \hlstd{= y,} \hlkwc{X} \hlstd{= X,} \hlkwc{H} \hlstd{= H,} \hlkwc{W} \hlstd{= W)}\hlopt{$}\hlstd{D}
    \hlstd{\}}
    \hlstd{Omega} \hlkwb{<-} \hlkwd{make.vmom}\hlstd{(}\hlkwc{b.hat} \hlstd{= b.hat,} \hlkwc{y} \hlstd{= y,} \hlkwc{X} \hlstd{= X,} \hlkwc{H} \hlstd{= H,} \hlkwc{W} \hlstd{= W)}
    \hlstd{var}   \hlkwb{<-} \hlkwd{solve}\hlstd{(}\hlkwd{crossprod}\hlstd{(D))} \hlopt{%*%} \hlkwd{t}\hlstd{(D)} \hlopt{%*%} \hlstd{Omega} \hlopt{%*%} \hlstd{D} \hlopt{%*%} \hlkwd{solve}\hlstd{(}\hlkwd{crossprod}\hlstd{(D))} \hlopt{/} \hlstd{n}
  \hlstd{\}}
  \hlkwa{if} \hlstd{(estimator} \hlopt{==} \hlstr{"ogmm"}\hlstd{)\{}
    \hlkwa{if} \hlstd{(D.type} \hlopt{==} \hlstr{"population"}\hlstd{)\{}
      \hlstd{D}     \hlkwb{<-} \hlkwd{make.D}\hlstd{(}\hlkwc{rho} \hlstd{= rho,} \hlkwc{beta} \hlstd{= beta,} \hlkwc{y} \hlstd{= y,} \hlkwc{X} \hlstd{= X,} \hlkwc{H} \hlstd{= H,} \hlkwc{W} \hlstd{= W)} \hlopt{/}\hlstd{n}
    \hlstd{\}}
    \hlkwa{if} \hlstd{(D.type} \hlopt{==} \hlstr{"gradient"}\hlstd{)\{}
      \hlstd{D}     \hlkwb{<-} \hlkwd{moments.lee2007}\hlstd{(b.hat,} \hlkwc{y} \hlstd{= y,} \hlkwc{X} \hlstd{= X,} \hlkwc{H} \hlstd{= H,} \hlkwc{W} \hlstd{= W)}\hlopt{$}\hlstd{D}
    \hlstd{\}}
    \hlstd{Psi} \hlkwb{<-} \hlstd{object}\hlopt{$}\hlstd{Psi}
    \hlstd{var} \hlkwb{<-} \hlkwd{solve}\hlstd{(}\hlkwd{t}\hlstd{(D)} \hlopt{%*%} \hlstd{Psi} \hlopt{%*%} \hlstd{D)} \hlopt{/} \hlstd{n}
  \hlstd{\}}
  \hlkwd{return}\hlstd{(var)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{summary.gmm.slm} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{object}\hlstd{,}
                            \hlkwc{D} \hlstd{=} \hlkwd{c}\hlstd{(}\hlstr{"population"}\hlstd{,} \hlstr{"gradient"}\hlstd{),}
                             \hlkwc{table} \hlstd{=} \hlnum{TRUE}\hlstd{,}
                             \hlkwc{digits} \hlstd{=} \hlkwd{max}\hlstd{(}\hlnum{3}\hlstd{, .Options}\hlopt{$}\hlstd{digits} \hlopt{-} \hlnum{3}\hlstd{),}
                             \hlkwc{...}\hlstd{)\{}
  \hlstd{D.type} \hlkwb{<-} \hlkwd{match.arg}\hlstd{(D)}
  \hlstd{n}       \hlkwb{<-} \hlkwd{nrow}\hlstd{(object}\hlopt{$}\hlstd{X)}
  \hlstd{df}      \hlkwb{<-} \hlstd{n} \hlopt{-} \hlkwd{length}\hlstd{(object}\hlopt{$}\hlstd{coefficients)}
  \hlstd{b}       \hlkwb{<-} \hlstd{object}\hlopt{$}\hlstd{coefficients}
  \hlstd{std.err} \hlkwb{<-} \hlkwd{sqrt}\hlstd{(}\hlkwd{diag}\hlstd{(}\hlkwd{vcov}\hlstd{(object,} \hlkwc{D} \hlstd{= D.type)))}
  \hlstd{z}       \hlkwb{<-} \hlstd{b} \hlopt{/} \hlstd{std.err}
  \hlstd{p}       \hlkwb{<-} \hlnum{2} \hlopt{*} \hlkwd{pt}\hlstd{(}\hlopt{-}\hlkwd{abs}\hlstd{(z),} \hlkwc{df} \hlstd{= df)}
  \hlstd{CoefTable} \hlkwb{<-} \hlkwd{cbind}\hlstd{(b, std.err, z, p)}
  \hlkwd{colnames}\hlstd{(CoefTable)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"Estimate"}\hlstd{,} \hlstr{"Std.Error"}\hlstd{,} \hlstr{"t-value"}\hlstd{,} \hlstr{"Pr(>|t|)"}\hlstd{)}
  \hlstd{result} \hlkwb{<-} \hlkwd{structure}\hlstd{(}
    \hlkwd{list}\hlstd{(}
      \hlkwc{CoefTable} \hlstd{= CoefTable,}
      \hlkwc{digits}    \hlstd{= digits,}
      \hlkwc{call}      \hlstd{= object}\hlopt{$}\hlstd{call),}
    \hlkwc{class} \hlstd{=} \hlstr{'summary.gmm.slm'}
  \hlstd{)}
  \hlkwd{return}\hlstd{(result)}
\hlstd{\}}

\hlstd{print.summary.gmm.slm} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{,}
                                   \hlkwc{digits} \hlstd{= x}\hlopt{$}\hlstd{digits,}
                                   \hlkwc{na.print} \hlstd{=} \hlstr{""}\hlstd{,}
                                   \hlkwc{symbolic.cor} \hlstd{= p} \hlopt{>} \hlnum{4}\hlstd{,}
                                   \hlkwc{signif.stars} \hlstd{=} \hlkwd{getOption}\hlstd{(}\hlstr{"show.signif.stars"}\hlstd{),}
                                   \hlkwc{...}\hlstd{)}
\hlstd{\{}
  \hlkwd{cat}\hlstd{(}\hlstr{"\textbackslash{}nCall:\textbackslash{}n"}\hlstd{)}
  \hlkwd{cat}\hlstd{(}\hlkwd{paste}\hlstd{(}\hlkwd{deparse}\hlstd{(x}\hlopt{$}\hlstd{call),} \hlkwc{sep} \hlstd{=} \hlstr{"\textbackslash{}n"}\hlstd{,} \hlkwc{collapse} \hlstd{=} \hlstr{"\textbackslash{}n"}\hlstd{),} \hlstr{"\textbackslash{}n\textbackslash{}n"}\hlstd{,} \hlkwc{sep} \hlstd{=} \hlstr{""}\hlstd{)}

  \hlkwd{cat}\hlstd{(}\hlstr{"\textbackslash{}nCoefficients:\textbackslash{}n"}\hlstd{)}
  \hlkwd{printCoefmat}\hlstd{(x}\hlopt{$}\hlstd{CoefTable,} \hlkwc{digit} \hlstd{= digits,} \hlkwc{P.value} \hlstd{=} \hlnum{TRUE}\hlstd{,} \hlkwc{has.Pvalue} \hlstd{=} \hlnum{TRUE}\hlstd{)}
  \hlkwd{invisible}\hlstd{(}\hlkwa{NULL}\hlstd{)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

Now, we test the function \code{slm.gmm} estimating the GMM and OGMM procedure, along with standard errors estimated using the matrix $\mD_n$ evaluated at $\widehat{\vtheta}$, \code{D = "population"}, and the gradient evaluated at $\widehat{\vtheta}$, \code{D = "gradient"}. 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Test }
\hlstd{gmm} \hlkwb{<-} \hlkwd{slm.gmm}\hlstd{(y} \hlopt{~} \hlstd{x1} \hlopt{+} \hlstd{x2} \hlopt{+} \hlstd{x3,} \hlkwc{data} \hlstd{= data,} \hlkwc{instruments} \hlstd{=} \hlnum{2}\hlstd{,} \hlkwc{W} \hlstd{= W)}
\end{alltt}
\begin{verbatim}
## Initial function value: -0.1048576 
## Initial gradient value:
##          Wy (Intercept)          x1          x2          x3 
## -0.36908069  0.39381179  0.27513366 -0.07774734 -0.31510188 
## initial  value 0.104858 
## iter   2 value 0.035790
## iter   3 value 0.012847
## iter   4 value 0.012371
## iter   5 value 0.012121
## iter   6 value 0.012097
## iter   7 value 0.012084
## iter   8 value 0.012083
## iter   9 value 0.012083
## iter  10 value 0.012083
## iter  11 value 0.012083
## iter  11 value 0.012083
## iter  11 value 0.012083
## final  value 0.012083 
## converged
\end{verbatim}
\begin{alltt}
\hlkwd{summary}\hlstd{(gmm,} \hlkwc{D} \hlstd{=} \hlstr{"population"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## slm.gmm(formula = y ~ x1 + x2 + x3, data = data, W = W, instruments = 2)
## 
## 
## Coefficients:
##             Estimate Std.Error t-value Pr(>|t|)    
## Wy           0.53258   0.04350  12.242   <2e-16 ***
## (Intercept) -0.10708   0.06530  -1.640    0.102    
## x1          -1.02807   0.06365 -16.152   <2e-16 ***
## x2          -0.04150   0.06355  -0.653    0.514    
## x3           1.05343   0.06585  15.997   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}
\begin{alltt}
\hlkwd{summary}\hlstd{(gmm,} \hlkwc{D} \hlstd{=} \hlstr{"gradient"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## slm.gmm(formula = y ~ x1 + x2 + x3, data = data, W = W, instruments = 2)
## 
## 
## Coefficients:
##             Estimate Std.Error t-value Pr(>|t|)    
## Wy           0.53258   0.04988  10.676   <2e-16 ***
## (Intercept) -0.10708   0.06621  -1.617    0.106    
## x1          -1.02807   0.06401 -16.061   <2e-16 ***
## x2          -0.04150   0.06360  -0.652    0.514    
## x3           1.05343   0.06914  15.237   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}
\begin{alltt}
\hlstd{ogmm} \hlkwb{<-} \hlkwd{slm.gmm}\hlstd{(y} \hlopt{~} \hlstd{x1} \hlopt{+} \hlstd{x2} \hlopt{+} \hlstd{x3,} \hlkwc{data} \hlstd{= data,} \hlkwc{instruments} \hlstd{=} \hlnum{2}\hlstd{,} \hlkwc{W} \hlstd{= W,}
                \hlkwc{estimator} \hlstd{=} \hlstr{"ogmm"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Initial function value: -0.1048576 
## Initial gradient value:
##          Wy (Intercept)          x1          x2          x3 
## -0.36908069  0.39381179  0.27513366 -0.07774734 -0.31510188 
## initial  value 0.104858 
## iter   2 value 0.035790
## iter   3 value 0.012847
## iter   4 value 0.012371
## iter   5 value 0.012121
## iter   6 value 0.012097
## iter   7 value 0.012084
## iter   8 value 0.012083
## iter   9 value 0.012083
## iter  10 value 0.012083
## iter  11 value 0.012083
## iter  11 value 0.012083
## iter  11 value 0.012083
## final  value 0.012083 
## converged
## Initial function value: -0.02203645 
## Initial gradient value:
##           Wy  (Intercept)           x1           x2           x3 
##  0.056818912 -0.002763490 -0.010072487  0.008073935 -0.009049050 
## initial  value 0.022036 
## iter   2 value 0.021897
## iter   3 value 0.021041
## iter   4 value 0.021027
## iter   5 value 0.021025
## iter   6 value 0.021025
## iter   7 value 0.021025
## iter   7 value 0.021025
## iter   7 value 0.021025
## final  value 0.021025 
## converged
\end{verbatim}
\begin{alltt}
\hlkwd{summary}\hlstd{(ogmm,} \hlkwc{D} \hlstd{=} \hlstr{"population"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## slm.gmm(formula = y ~ x1 + x2 + x3, data = data, W = W, instruments = 2, 
##     estimator = "ogmm")
## 
## 
## Coefficients:
##             Estimate Std.Error t-value Pr(>|t|)    
## Wy           0.56420   0.03736  15.101   <2e-16 ***
## (Intercept) -0.09892   0.06470  -1.529    0.127    
## x1          -1.03113   0.06328 -16.295   <2e-16 ***
## x2          -0.03512   0.06335  -0.554    0.580    
## x3           1.03201   0.06530  15.804   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}
\begin{alltt}
\hlkwd{summary}\hlstd{(ogmm,} \hlkwc{D} \hlstd{=} \hlstr{"gradient"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## slm.gmm(formula = y ~ x1 + x2 + x3, data = data, W = W, instruments = 2, 
##     estimator = "ogmm")
## 
## 
## Coefficients:
##             Estimate Std.Error t-value Pr(>|t|)    
## Wy           0.56420   0.04327  13.039   <2e-16 ***
## (Intercept) -0.09892   0.06529  -1.515    0.130    
## x1          -1.03113   0.06325 -16.302   <2e-16 ***
## x2          -0.03512   0.06325  -0.555    0.579    
## x3           1.03201   0.06710  15.379   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}
\end{kframe}
\end{knitrout}

Table \ref{tab:gmm-slms-b} compares different estimates for the SLM using all previous estimators: MLE, S2SLSE, BS2SLSE, GMME and OGMME. Similarly, Table \ref{tab:gmm-slms-se} compares the SEs. 

\begin{table}[ht]
\caption{Comparing coefficients for SLM.}\label{tab:gmm-slms-b}
\centering
\begin{kframe}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in sar.mle.con(y \textasciitilde{} x1 + x2 + x3, data = data, W = W): could not find function "{}sar.mle.con"{}}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in cbind(c(mle\$beta\_hat, mle\$rho\_hat), s2sls.e\$coefficients, b2sls\$coefficients, : object 'mle' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in cbind(coefs): object 'coefs' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in rownames(Tab) <- c("{}b0"{}, "{}b1"{}, "{}b2"{}, "{}b3"{}, "{}rho"{}): object 'Tab' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in colnames(Tab) <- c("{}mle"{}, "{}s2sls"{}, "{}bs2sls"{}, "{}gmm"{}, "{}ogmm"{}): object 'Tab' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in toLatex(Tab, compact = TRUE, useBooktabs = TRUE, digits = 4): object 'Tab' not found}}\end{kframe}
\end{table}


\begin{table}[ht]
\caption{Comparing SE for SLM.}\label{tab:gmm-slms-se}
\centering
\begin{kframe}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'diag': object 'mle' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in cbind(ses): object 'ses' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in rownames(Tab) <- c("{}b0"{}, "{}b1"{}, "{}b2"{}, "{}b3"{}, "{}rho"{}): object 'Tab' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in colnames(Tab) <- c("{}mle"{}, "{}s2sls-ho"{}, "{}s2sls-he"{}, "{}bs2sls-i"{}, : object 'Tab' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in toLatex(Tab, compact = TRUE, useBooktabs = TRUE, digits = 4): object 'Tab' not found}}\end{kframe}
\end{table}

%*************************************************
\subsection{Best GMM estimator}
%*************************************************

\cite{lee2007gmm} proposes the optimal instrument for the OGMM estimator. The best linear instruments are:
\begin{equation*}
  \mH_n^* = \E(\mZ_n|\mX_n) = \left(\mG_0\mX_n\vbeta_0, \mX_n\right).
\end{equation*}

There are two cases which can make the selection of best quadratic matrices easier.

%-------------------
\subsubsection{Selection from $\calP_{2n}$}
%--------------------

For the best selection of $\mP_{jn}$, consider $\mP_{jn}\in \calP_{2n}$. In this case $\vomega_n$ is a $n\times 1$ matrix of zeros and $\mOmega_n$ becomes:
\begin{equation*}
\mOmega_n = \mV_n = \sigma_0^4
          \begin{pmatrix}
          \tr(\mP_{1n}\mP_{1n}^s) & \tr(\mP_{1n}\mP_{2n}^s) & \vzeros \\
          \tr(\mP_{2n}\mP_{1n}^s) & \tr(\mP_{2n}\mP_{2n}^s) & \vzeros \\
          \vzeros & \vzeros & \frac{1}{\sigma_0^2}\mH_n^{\top}\mH_n
          \end{pmatrix} = 
          \sigma_0^4\begin{pmatrix}
          \mDelta_{2n} & \vzeros \\
          \vzeros & \frac{1}{\sigma_0^2}\mH_n^{\top}\mH_n
          \end{pmatrix},
\end{equation*}
%
with $\mDelta_{2n} = \left[\vec(\mP_{1n})^\top, \vec(\mP_{2n}^\top)\right]^\top\left[\vec(\mP_{1n}^s), \vec(\mP_{2n}^s)\right]$, because, for any conformable matrices $\mA$ and $\mB$, $\tr(\mA\mB) = \vec(\mA^\top)^\top\vec(\mB)$ and
\footnotesize
\begin{equation*}
\begin{aligned}
 \mD_n^\top\mOmega_n^{-1}\mD_n = &   \frac{1}{\sigma_0^4}
 \begin{pmatrix}
  \sigma_0^2\tr(\mP^s_{1n}\mG_0) &  \sigma_0^2\tr(\mP^s_{2n}\mG_0) & (\mG_0\mX_n\vbeta_0)^\top\mH_n\\
 \vzeros & \vzeros & \mX_n^\top\mH_n
 \end{pmatrix}\\
 & 
           \begin{pmatrix}
          \tr(\mP_{1n}\mP_{1n}^s)^{-1} & \tr(\mP_{1n}\mP_{2n}^s)^{-1} & \vzeros \\
          \tr(\mP_{2n}\mP_{1n}^s)^{-1} & \tr(\mP_{2n}\mP_{2n}^s)^{-1} & \vzeros \\
          \vzeros & \vzeros & \sigma_0^2\left(\mH_n^{\top}\mH_n\right)^{-1}
          \end{pmatrix} 
         \begin{pmatrix}
           \sigma_0^2\tr(\mP^s_{1n}\mG_0) & \vzeros \\
           \sigma_0^2\tr(\mP^s_{2n}\mG_0) & \vzeros \\
           \mH_n^\top\mG_0\mX_n\vbeta_0 & \mH^\top_n\mX_n
         \end{pmatrix}, \\
         & = \frac{1}{\sigma_0^4}
         \begin{pmatrix}
         \sigma_0^2\mC_{2n} & (\mG_0\mX_n\vbeta_0)^\top\mH_n \\
         \vzeros & \mX_n^\top\mH_n
         \end{pmatrix}
         \begin{pmatrix}
          \mDelta_{2n}^{-1} & \vzeros \\
          \vzeros & \sigma_0^2\left(\mH_n^\top\mH_n\right)^{-1}
         \end{pmatrix}
         \begin{pmatrix}
          \sigma_0^2\mC_{2n}^\top & \vzeros \\
          \mH_n^\top\mG_0\mX_n\vbeta_0 & \mH^\top_n\mX_n
         \end{pmatrix}, \\
        &  = \frac{1}{\sigma_0^4}
         \begin{pmatrix}
          \sigma_0^2\mC_{2n}\mDelta_{2n}^{-1} & \sigma_0^2(\mG_0\mX_n\vbeta_0)^\top\mH_n(\mH_n^\top\mH_n) \\
          \vzeros & \sigma_0^2\mX_n^\top\mH_n(\mH_n^\top\mH_n)^{-1}
         \end{pmatrix}
                  \begin{pmatrix}
          \sigma_0^2\mC_{2n}^\top & \vzeros \\
          \mH_n^\top\mG_0\mX_n\vbeta_0 & \mH^\top_n\mX_n
         \end{pmatrix}, \\
         & = 
         \begin{pmatrix}
         \mC_{2n}\mDelta_{2n}^{-1}\mC_{2n}^\top + \frac{1}{\sigma_0^2}(\mG_0\mX_n\vbeta_0)^\top\mH_n(\mH_n^\top\mH_n)\mH_n^\top\mG_0\mX_n\vbeta_0 & \frac{1}{\sigma_0^2}(\mG_0\mX_n\vbeta_0)^\top\mH_n(\mH_n^\top\mH_n)\mH_n^\top\mX_n \\
        \frac{1}{\sigma_0^2}\mX_n^\top\mH_n(\mH_n^\top\mH_n)^{-1} \mH_n^\top\mG_0\mX_n\vbeta_0  &  \frac{1}{\sigma_0^2}\mX_n^\top\mH_n(\mH_n^\top\mH_n)^{-1} \mH_n^\top\mX_n
         \end{pmatrix}, \\
         & = \begin{pmatrix}
          \mC_{2n}\mDelta_{2n}^{-1}\mC_{2n}^\top & \vzeros \\
          \vzeros & \vzeros 
         \end{pmatrix}
         +
         \frac{1}{\sigma_0^2}
         \begin{pmatrix}
           (\mG_0\mX_n\vbeta_0)^\top \\
           \mX_n^\top
         \end{pmatrix}
         \mH_n(\mH_n^\top\mH_n)^{-1} \mH_n^\top
         \begin{pmatrix}
          \mG_0\mX_n\vbeta_0 & \mX_n 
         \end{pmatrix}, 
      \end{aligned}
\end{equation*}
\normalsize
%
where $\mC_{2n} = \left[\tr\left(\mP_{1n}^s\mG_0\right), \tr\left(\mP_{2n}^s\mG_0\right)\right]$. With the best instruments $\mH^*_n$ for $\mH_n$, the optimal variance-covariance matrix is
\begin{equation*}
 \mD_n^\top\mOmega_n^{-1}\mD_n = \begin{pmatrix}
          \mC_{2n}\mDelta_{2n}^{-1}\mC_{2n}^\top & \vzeros \\
          \vzeros & \vzeros 
         \end{pmatrix}
         +
         \frac{1}{\sigma_0^2}
         \begin{pmatrix}
           (\mG_0\mX_n\vbeta_0) & \mX_n
         \end{pmatrix}^\top
         \begin{pmatrix}
          \mG_0\mX_n\vbeta_0 & \mX_n 
         \end{pmatrix}.
\end{equation*}

Since $\tr(\mP_{jn}\mP_{ln}^s) = \frac{1}{2}\tr(\mP_{jn}^s\mP_{ln}^s)$, we can also write
\begin{equation*}
\begin{aligned}
\mDelta_{2n} & = \frac{1}{2}
\begin{pmatrix}
\tr(\mP_{1n}^s\mP_{1n}^s) & \tr(\mP_{1n}^s\mP_{2n}^s) \\
\tr(\mP_{2n}^s\mP_{1n}^s) & \tr(\mP_{2n}^s\mP_{2n}^s) 
\end{pmatrix},\\
& = \frac{1}{2}\left[\vec(\mP_{1n})^s, \vec(\mP_{2n}^s)\right]^\top\left[\vec(\mP_{1n}^s), \vec(\mP_{2n}^s)\right].
\end{aligned}
\end{equation*}

If $\mP_{jn}$s belong to $\calP_{2n}$ such that $\diag(\mP_{jn}) = \vzeros$, then 
\begin{equation*}
\begin{aligned}
\tr(\mP_{jn}^s\mG_0) & = \frac{1}{2}\tr\left[\mP_{jn}^s\left(\mG_0 -\Diag(\mG_0)\right)^s\right], \\
                     & = \frac{1}{2}\vec\left(\left[\mG_0 - \Diag(\mG_0)\right]^s\right)^\top\vec(\mP_{jn}^s).
\end{aligned}
\end{equation*}

Schwartz inequality implies that
\begin{equation*}
\begin{aligned}
\mC_{2n}\mDelta_{2n}^{-1}\mC_{2n}^\top & \leq \frac{1}{2}\vec\left(\left[\mG_0 - \Diag(\mG_0)\right]^s\right)^\top \frac{1}{2}\vec\left(\left[\mG_0 - \Diag(\mG_0)\right]^s\right) \\
& = \tr\left(\left[\mG_0 - \Diag(\mG_0)\right]^s \mG_0\right).
\end{aligned}
\end{equation*}

Therefore, in the subclass $\calP_{2n}$, $\left[\mG_0 - \Diag(\mG_0)\right]$ and $\left[\mG_0\mX_n\vbeta_0, \mX_n\right]$ provide the set of best IV functions, and the vector of moment functions is
\begin{equation*}
\vg_{b, n} = \begin{pmatrix}
               \widehat{\vepsi}_n(\vtheta)^\top\left(\widehat{\mG}_n - \Diag(\widehat{\mG}_n)\right)\widehat{\vepsi}_n(\vtheta) \\
               \widehat{\vbeta}_n^\top\mX_n ^\top\widehat{\mG}_n^\top\widehat{\vepsi}_n(\vtheta)\\
               \mX_n^\top\widehat{\vepsi}_n(\vtheta)
              \end{pmatrix}, 
\end{equation*}
%
and the corresponding estimated $\mV_n$ is
\begin{equation}
\widehat{\mV}_n = \widehat{\sigma}^4_n
\begin{pmatrix}
\tr\left(\left[\widehat{\mG}_n - \Diag(\widehat{\mG}_n)\right]^s\widehat{\mG}_n\right) & \vzeros \\
\vzeros & \frac{1}{\widehat{\sigma}_n^2}\left(\widehat{\mG}_n\mX_n\widehat{\vbeta}, \mX_n\right)^\top \left(\widehat{\mG}_n\mX_n\widehat{\vbeta}, \mX_n\right)
\end{pmatrix},
\end{equation}
%
and the Feasible Best GMME (FBGMM) with $\calP_{2n}$ will be from minimizing:
\begin{equation*}
Q_n(\vtheta) = \vg_{b,n}^\top(\vtheta)\widehat{\mV}_n^{-1}\vg_{b,n}(\vtheta).
\end{equation*}




%-------------------
\subsubsection{Assuming Normality}
%--------------------

If $\vepsi_n \sim \rN(\vzeros, \sigma_0^2\mI_n)$, then $\mOmega_n = \mV_n$ even though $\mP_{jn}$'s are selected from the class $\calP_{1n}$. 

Because, for any $\mP_{jn}\in\calP_{1n}$, 
\begin{equation*}
  \tr\left(\mP_{jn}^s\mG_0\right)= \frac{1}{2}\vec\left(\left[\mG_0 - \frac{\tr\left(\mG_0\right)}{n}\mI_n\right]^s\mG_0\right)^\top\vec\left(\mP_{jn}^s\right), 
\end{equation*}
%
the generalized Schwartz inequality implies that
\begin{equation*}
\mC_{2n}\mDelta_{2n}^{-1}\mC_{2n}^\top\leq \tr\left(\left[\mG_0 - \frac{\tr\left(\mG_0\right)}{n}\mI_n\right]^s\mG_0\right).
\end{equation*}

Therefore, in the broader class $\calP_{1n}$, $\left[\mG_0 - \frac{\tr\left(\mG_0\right)}{n}\mI_n\right]$ and $\left[\mG_0\mX_n\vbeta_0, \mX_n\right]$ provide the best set of IV functions. Thus, the vector of moment functions is
\begin{equation*}
\vg_{b, n} = \begin{pmatrix}
               \widehat{\vepsi}_n(\vtheta)^\top\left(\widehat{\mG}_n - \frac{\tr(\widehat{\mG}_n)}{n}\mI_n \right)\widehat{\vepsi}_n(\vtheta) \\
               \widehat{\vbeta}_n^\top\mX_n ^\top\widehat{\mG}_n^\top\widehat{\vepsi}_n(\vtheta)\\
               \mX_n^\top\widehat{\vepsi}_n(\vtheta)
              \end{pmatrix}, 
\end{equation*}
%
and the corresponding estimated $\mV_n$ is
\begin{equation}
\widehat{\mV}_n = \widehat{\sigma}^4_n
\begin{pmatrix}
\tr\left(\left[\widehat{\mG}_n - \frac{\tr(\widehat{\mG}_n)}{n}\mI_n \right]^s\widehat{\mG}_n\right) & \vzeros \\
\vzeros & \frac{1}{\widehat{\sigma}_n^2}\left(\widehat{\mG}_n\mX_n\widehat{\vbeta}, \mX_n\right)^\top \left(\widehat{\mG}_n\mX_n\widehat{\vbeta}, \mX_n\right)
\end{pmatrix}.
\end{equation}



%--------------------------------------------------------
\begin{theorem}[BGMME for SLM \citep{lee2007gmm}]
Under Assumptions (a)-(c) of \ref{assumption:lee2007-1}, suppose that $\widehat{\rho}_n$ is a $\sqrt{n}$-consistent estimate of $\rho_0$, $\widehat{\vbeta}_n$ is a consistent estimate of $\vbeta_0$, and $\widehat{\sigma}_n^2$ is a consistent estimate of $\sigma_0^2$. Within the class of GMMEs derived with $\calP_{2n}$, the Best GMM estimator (BGMME) $\widehat{\vtheta}_n$ has the limiting distribution that
\begin{equation*}
  \sqrt{n}\left(\widehat{\vtheta}_n - \vtheta_0\right)\dto \rN\left(\vzeros, \mSigma^{b2}\right), 
\end{equation*}
%
where
\begin{equation*}
  \mSigma^{b2} = \lim_{n\to\infty}\frac{1}{n}
  \begin{pmatrix}
    \tr\left(\left[\mG_0 - \Diag(\mG_0)\right]^s \mG_0\right) + \frac{1}{\sigma_0^2}\left(\mG_0\mX_n\vbeta_0\right)^\top\left(\mG_0\mX_n\vbeta_0\right) & \frac{1}{\sigma_0^2}\left(\mG_0\mX_n\vbeta_0\right)^\top\mX_n \\
    \frac{1}{\sigma_0^2}\mX_n^\top\left(\mG_0\mX_n\vbeta_0\right) & \frac{1}{\sigma_0^2}\mX_n^\top\mX_n
  \end{pmatrix},
\end{equation*}
%
with $\mG_0 = \mW_n\left(\mI_n - \rho_0\mW_n\right)^{-1}$, which is assumed to exist.

When $\vepsi_n\sim\rN\left(\vzeros, \sigma_0^2\mI_n\right)$, within the broader class of GMMEs derived with $\calP_{1n}$, the BGMME $\widehat{\vtheta}_{n}$ has the limiting distribution that
\begin{equation*}
  \sqrt{n}\left(\widehat{\vtheta}_n - \vtheta_0\right)\dto \rN\left(\vzeros, \mSigma^{b1}\right), 
\end{equation*}
%
where
\begin{equation*}
  \mSigma^{b1} = \lim_{n\to\infty}\frac{1}{n}
  \begin{pmatrix}
    \tr\left(\left[\mG_0 - \frac{\tr(\mG_0)}{n}\mI_n\right]^s \mG_0\right) + \frac{1}{\sigma_0^2}\left(\mG_0\mX_n\vbeta_0\right)^\top\left(\mG_0\mX_n\vbeta_0\right) & \frac{1}{\sigma_0^2}\left(\mG_0\mX_n\vbeta_0\right)^\top\mX_n \\
    \frac{1}{\sigma_0^2}\mX_n^\top\left(\mG_0\mX_n\vbeta_0\right) & \frac{1}{\sigma_0^2}\mX_n^\top\mX_n
  \end{pmatrix},
\end{equation*}
%
which is assumed to exist.
\end{theorem}
%--------------------------------------------------------



%*************************************************
\subsection{S2SLS Estimator as GMM Estimator}
%*************************************************

The moment conditions for the S2SLS estimators are
\begin{equation*}
\vg_n = \frac{1}{n}\mH^\top_n\vepsi_n= \frac{1}{n}\mH^\top_n\left(\vy_n - \mZ_n\vdelta\right),
\end{equation*}
%
whose variance-covariance matrix is:
\begin{equation*}
 \frac{1}{n}\mOmega_n = \frac{1}{n}\sigma_0^2\mH_n^\top\mH_n.
\end{equation*}

The matrix $\mUpsilon_n^{-1}$ is the optimal weight matrix, which correspond to the inverse of the covariance matrix of the sample moments: 
\begin{equation*}
	\mUpsilon_n^{-1} = \left(\frac{1}{n}\sigma_0^2\mH_n^\top\mH_n\right)^{-1}.
\end{equation*}

Then, the function to minimize is:
\begin{equation*}
		Q  = \frac{1}{n\widehat{\sigma}^2}\left\lbrace \left[\mH^\top_n\vy_n - \mH^\top_n\mZ_n\vdelta\right]^\top\left(\mH^\top_n\mH_n\right)^{-1}\left[\mH^\top_n\vy_n - \mH^\top_n\mZ_n\vdelta\right]\right\rbrace 
\end{equation*}

Obtaining the first order conditions and solving for $\vdelta$, we obtain:
\begin{equation}
  \widehat{\vdelta}_{n} = \left(\mZ^\top_n\mP_{H,n}\mZ_n\right)^{-1}\mZ^\top_n\mP_{H, n}\vy_n. 
\end{equation}

Its asymptotic distribution is
\begin{equation*}
  \sqrt{n}\left(\widehat{\vdelta}_{n} - \vdelta_0\right)\dto \rN\left(\vzeros, \sigma_0^2\lim_{n\to\infty}\left\lbrace \frac{1}{n}\left(\mG_n\mX_n\vbeta_0\right)^\top \mH_n(\mH_n^\top\mH_n)^{-1}\mH_n^\top\left(\mG_n\mX_n\vbeta_0\right)\right\rbrace^{-1}\right).
\end{equation*}

%*************************************************
\section{Feasible Generalized Least Squares Estimator for SEM Model}\label{sec:sfgls-sem}
%*************************************************

\cite{kelejian1999generalized} derive a Method of Moments (MOM) estimator for $\lambda$, which is later used in the Feasible Generalized Least Squares (FGLS) estimator. The motivation behind this new estimator, according to \cite{kelejian1999generalized}, is that the (quasi) maximum likelihood estimator may not be computationally feasible, especially in cases with moderate or large-sized samples. As they note, the MOM estimator is computationally simple, regardless of sample size, making it highly attractive when dealing with large spatial datasets. Additionally, the IV/GMM estimators avoid the Jacobian term, alleviating many problems associated with matrix inversion, the computation of characteristic roots, and/or Cholesky decomposition. Another motivation for proposing this estimator was that, at the time, there were no formal results regarding the consistency and asymptotic normality of the ML estimator  \citep[pag. 1608]{pruchaHB}. Recall that Lee formally derived the asymptotic properties of the ML estimator in 2004 for the Spatial Lag Model (SLM).   

Recall that the SEM model is given by: 
\begin{equation}\label{eq:sem_gmm}
	\begin{aligned}
	\vy_n  & = \mX_n\vbeta_0 + \vu_n, \\ 
	\vu_n  & = \lambda_0\mM_n \vu_n + \vepsi_n.
	\end{aligned}
\end{equation}

In brief, \cite{kelejian1999generalized} propose the use of \textbf{nonlinear least square} to obtain a consistent generalized moment estimate for $\lambda_0$, which can be used to obtain consistent estimates for $\vbeta_0$ in a FGLS framework. The key distinction between the MOM estimation discussed here and the Generalized Method of Moment (GMM) estimator presented later is that in the MOM approach, there is no inference for the spatial autoregressive coefficient $\lambda$. In other words, $\lambda$ is treated purely as a nuisance parameter, whose only purpose is to aid in obtaining consistent estimates for $\vbeta_0$.

\begin{remark}
The MOM procedure proposed by \cite{kelejian1999generalized} was originally motivated by the computational difficulties of the ML.
\end{remark}

\begin{remark}
\cite{kelejian1999generalized} does not provide an asymptotic variance for $\lambda$., which is why some software packages only provide the estimate $\widehat{\lambda}$ without is standard error. 
\end{remark}


One advantage of the MOM estimator is that it does not rely on the assumption of normality of the disturbances $\vepsi_n$. Nonetheless, it does assume that $\epsilon_i$ are independently and identically distributed for all $i$ with zero mean and constant variance $\sigma^2$. We adopt the following assumption about the error terms, as stated by \cite{kelejian1999generalized}.

%------------------------------------------------------------------------------------
\begin{assumption}[Homoskedastic Errors \citep{kelejian1999generalized}]\label{assu:errors_triang_homokedastic}
The innovations $\left\lbrace \epsilon_{i,n}, 1 \leq i \leq n, n\geq 1\right\rbrace$ are independently and identically distributed for all $n$ with zero mean and variance $\sigma^2$, where $0 < \sigma^2 < b$, with $b < \infty$. Additionally, the innovations are assumed to possess finite fourth moments. 
\end{assumption}
%------------------------------------------------------------------------------------

Next, we present the following assumptions regarding the weight matrix:

%-------------------------------------------------------------------------------
\begin{assumption}[Weight Matrix $\mM_n$ \citep{kelejian1999generalized}]\label{assu:KP1999_M} Assume the following:
\begin{enumerate}
  \item All diagonal elements of the spatial weighting matrix $\mM_n$ are zero.
  \item The matrix $(\mI_n - \lambda_0\mM_n)$ is nonsingular with $\left|\lambda_0 \right|<1$.
\end{enumerate}
\end{assumption}
%-------------------------------------------------------------------------------

Given Equation \eqref{eq:sem_gmm}  and Assumption \ref{assu:KP1999_M} on the weight matrix, we can express the error term as $\vu_n = (\mI_n - \lambda_0 \mM_n)^{-1}\vepsi_n = \mR_0^{-1}\vepsi_n$. Therefore, the expectation and variance of $\vu_n$ are $\E(\vu_n) = 0$ and $\E(\vu_n\vu^\top_n) = \mOmega_{n0}$, respectively, where:
\begin{equation*}
  \mOmega_{n0} = \sigma^2_{0}(\mI_n - \lambda_0\mM_n)^{-1}(\mI_n - \lambda_0\mM_n^\top)^{-1} = \sigma_{0}^2\mR_0^{-1}\mR_0^{-1\top}.
\end{equation*}

Note that a row-standardized spatial weight matrix is typically not symmetric, such that $\mM_n\neq \mM_n^\top$ and thus $(\mI_n - \lambda_0\mM_n)^{-1}\neq (\mI_n - \lambda_0\mM_n^\top)^{-1}$.

%----------------------
\subsection{Generalized Least Squares Estimator}\label{sec:swls}
%------------------------

The primary objective in \cite{kelejian1999generalized} is to derive a \textbf{consistent estimator} for  $\lambda_0$ to ensure the consistency of the resulting spatially weighted estimator. In this context, \cite{kelejian1999generalized} were not primarily concerned with inference on $\lambda$, but rather viewed it as a tool to obtain consistent estimates for $\vbeta$. This implies that $\lambda$ is considered a \textbf{nuisance parameter}. 

The Spatially Weighted Least Squares (SWLS) boils down to:
\begin{equation}\label{eq:beta_swls}
  \widehat{\vbeta}_{SWLS} = \left(\mX_{s, n}^\top\mX_{s, n}\right)^{-1}\mX_{s, n}^\top\vy_{s, n},
\end{equation}
%
were $\mX_{s, n} = \mX_n - \lambda_0\mM_n\mX_n$ and $\vy_{s, n} = \vy_n - \lambda_0\mM_n\vy_n$, using a consistent estimate $\widehat{\lambda}_n$ for the autoregressive parameter. Note that this model is essentially an OLS applied to spatially filtered variables. Moreover, it is important to note that SWLS is a special case of the Feasible Generalized Least Squares (FGLS) estimator. 

Under the assumptions made, the Generalized Least Squares (GLS) estimator---assuming we know $\lambda_0$---for $\vbeta$ is:
\begin{equation*}
\begin{aligned}
\widehat{\vbeta}_{GLS}  & = \left[\mX^\top_n\mOmega^{-1}_{n0}\mX_n\right]^{-1}\mX^\top_n\mOmega^{-1}_{n0}\vy_n, \\
& = \left[\mX^\top_n\frac{1}{\sigma^2}\left(\mI_n - \lambda_0\mM_n\right)^\top\left(\mI_n - \lambda_0\mM_n\right)\mX_n\right]^{-1}\mX^\top_n\frac{1}{\sigma^2}\left(\mI_n -\lambda_0\mM_n\right)^\top\left(\mI_n - \lambda_0\mM_n\right)\vy_n,  \\
   & =\left[\mX^\top_n\left(\mI_n - \lambda_0\mM_n\right)^\top\left(\mI_n - \lambda_0\mM_n\right)\mX_n\right]^{-1}\mX^\top_n\left(\mI_n -\lambda_0\mM_n\right)^\top\left(\mI_n - \lambda_0\mM_n\right)\vy_n, \\
  & =\left(\mX_n^\top \mR_{n0}^\top\mR_{n0}\mX_n\right)^{-1} \mX_n^\top \mR_{n0}^\top\mR_{n0}\vy_n. 
\end{aligned}
\end{equation*}


The FGLS estimator substitutes a consistent estimate for $\lambda$ into this expression, resulting in the following:
\begin{equation*}
\begin{aligned}
  \widehat{\vbeta}_{FGLS} & =\left[\mX^\top_n\left(\mI_n - \widehat{\lambda}_n\mM_n\right)^\top\left(\mI_n - \widehat{\lambda}_n\mM_n\right)\mX_n\right]^{-1}\mX^\top_n\left(\mI_n - \widehat{\lambda}_n\mM_n\right)^\top\left(\mI_n - \widehat{\lambda}_n\mM_n\right)\vy_n, \\
  & = \left(\mX_n^\top \mR_{n}^\top\mR_{n}\mX_n\right)^{-1} \mX_n^\top \mR_{n}^\top\mR_{n}\vy_n,
  \end{aligned}
\end{equation*}
%
which is precisely the same as Equation \eqref{eq:beta_swls}.

%--------------------------------------------------------------------
\subsection{Moment Conditions}\label{section:Moment_Condtions}\index{Moment conditions}\index{Generalized method of moments!Moment conditions}
%--------------------------------------------------------------------

The core idea behind a MOM estimator is to find a set of population moments equations that relate population moments to the parameters of interest. These population moments are then replaced by sample moments to derive consistent estimates for the parameters, such as $\lambda$, which can be used in Equation \eqref{eq:beta_swls}.

Given the DGP in Equation \eqref{eq:sem_gmm}, we express the relationship as: 
\begin{equation*}
  \vepsi_n = \vu_n - \lambda_0\mM_n\vu_n,
\end{equation*}
%
where $\vepsi_n$ denotes the idiosyncratic error and $\vu_n$ is the regression error. The \cite{kelejian1999generalized}'s MOM estimator of $\lambda_0$ is based on these three moment conditions \index{quadratic moment conditions} (see Section \ref{sec:gmm-slm-homo}):

%-----------------------------------------------------------------------
\begin{definition}[Moment Conditions]\label{def:moment-conditions-homo}
Under \textbf{homoskedasticity} \citep{kelejian1999generalized} the moment conditions are:
\begin{equation*}
  \begin{aligned}
  \E\left[n^{-1}\vepsi^\top_n \vepsi_n\right] &= \sigma^2_0, \\
   \E\left[n^{-1}\vepsi^\top_n\mM_n^\top\mM_n\vepsi_n\right] &= \frac{\sigma^2_0}{n}\tr\left(\mM^\top_n\mM_n\right),\\
   \E\left[n^{-1}\vepsi^\top_n \mM_n \vepsi_n \right] & =  0.
  \end{aligned}
\end{equation*}
\end{definition}
%-----------------------------------------------------------------------

To implement the moment conditions, we need to express the conditions on $\vepsi_n$ in terms of $\vu_n$, since $\vepsi_n$ is not observed. Given that $\vu_n = \lambda \mM_n\vu_n + \vepsi_n$, it follows that $\vepsi_n = \vu_n - \lambda_0\mM_n\vu_n$, i.e., the spatially filtered regression error terms. This leads to the following relationships:
\begin{eqnarray}
\vepsi^\top_n \vepsi_n & = & (\vu_n - \lambda_0\mM_n\vu_n)^\top(\vu_n - \lambda_0\mM_n\vu_n), \nonumber \\ 
                   & = & \vu^\top_n\vu_n - 2\lambda_0\vu^\top_n\mM_n\vu_n + \lambda^2_0\vu^\top_n\mM^\top_n\mM_n\vu_n \label{eq:mom_1}.\\
\vepsi^\top_n \mM^\top_n\mM_n \vepsi& = &(\vu_n - \lambda_0\mM_n\vu_n)^\top\mM^\top_n\mM_n(\vu_n - \lambda_0\mM_n\vu_n), \nonumber \\
                   &= & \vu^\top_n\mM^\top_n\mM_n\vu_n - 2\lambda_0\vu^\top_n\mM^\top_n\mM_n\mM_n\vu_n + \lambda^2_0\vu^\top_n\mM^\top_n\mM_n\mM^\top_n\mM_n\vu_n. \label{eq:mom_2} \\
\vepsi^\top_n\mM_n\vepsi_n& = &(\vu_n - \lambda_0\mM_n\vu_n)^\top\mM_n(\vu_n - \lambda_0\mM_n\vu_n), \nonumber \\ 
                   &=& \vu^\top_n\mM_n\vu_n - 2\lambda_0\vu^\top_n\mM_n\mM_n\vu_n + \lambda^2_0\vu^\top_n\mM^\top_n\mM_n\mM_n\vu_n. \label{eq:mom_3}
\end{eqnarray}

Defining $\vu_{L} = \mM_n\vu_n$, $\vu_{LL} = \mM_n\mM_n\vu_n$, \footnote{Spatially lagged variables are denoted by bar superscripts in the articles. For clarity, we use the $L$ subscript throughout. That is, a first order spatial lag of $\vy_n$, $\mW_n\vy_n$, is denoted by $\vy_L$. Higher order spatial lags are symbolized by adding additional $L$ subscripts.} we take expectations in \eqref{eq:mom_1} under the assumption of homoskedasticity:
\begin{equation}\label{eq:Pequation_1_gmm}
  \begin{aligned}
      \E\left[\vepsi^\top_n \vepsi_n\right] & = \E\left[\vu^\top_n\vu_n\right] - 2\lambda_0\E\left[\vu^\top_n\mM_n\vu_n\right] + \lambda^2_0\E\left[\vu^\top_n\mM^\top_n\mM_n\vu_n\right], \\
       \sigma^2_0& = \frac{1}{n}\E\left[\vu^\top_n\vu_n\right] - \lambda_0 \frac{2}{n}\E\left[\vu^\top_n\vu_L\right] + \lambda^2_0\frac{1}{n}\E\left[\vu_L^\top\vu_L\right] \quad \mbox{since $\E\left[n^{-1}\vepsi^\top_n \vepsi_n\right] = \sigma^2_0$},\\
       0 &= \sigma^2_0  - \frac{1}{n}\E\left[\vu^\top_n\vu_n\right] + \lambda_0 \frac{2}{n}\E\left[\vu^\top_n\vu_L\right] - \lambda^2_0\frac{1}{n}\E\left[\vu_L^\top\vu_L\right],\\
       0 &= \lambda_0 \frac{2}{n}\E\left[\vu^\top_n\vu_L\right] - \lambda^2_0\frac{1}{n}\E\left[\vu_L^\top\vu_L\right] + \frac{1}{n}\sigma^2_0 - \frac{1}{n}\E\left[\vu^\top_n\vu_n\right], \\
       0 & =  \begin{pmatrix}
         \frac{2}{n}\E\left[\vu^\top_n\vu_L\right] & - \frac{1}{n}\E\left[\vu_L^\top\vu_L\right] &  1
            \end{pmatrix}
            \begin{pmatrix}
              \lambda_0 \\
              \lambda^2_0 \\
              \sigma^2_0
            \end{pmatrix} - \frac{1}{n}\E\left[\vu^\top_n\vu_n\right].
  \end{aligned}
\end{equation}

Similarly, we derive two additional moment conditions:
\begin{eqnarray}
 0 & = & \begin{pmatrix}
            \frac{2}{n}\E\left[\vu^\top_{LL}\vu_L\right] & -\frac{1}{n}\E\left[\vu_{LL}^\top\vu_{LL}\right] & \frac{1}{n} \tr(\mM^\top_n\mM_n)
          \end{pmatrix}
                      \begin{pmatrix}
              \lambda_0 \\
              \lambda^2_0 \\
              \sigma^2_0
            \end{pmatrix} - \frac{1}{n}\E\left[\vu^\top_L\vu_L\right]\label{eq:Pequation_2_gmm}, \\
 0 & = & \begin{pmatrix}
            \frac{1}{n}\E\left[\vu^\top\vu_{LL} + \vu_L^\top\vu_L\right] & - \frac{1}{n}\E\left[\vu_{L}^\top\vu_{LL}\right] & 0
          \end{pmatrix}
                      \begin{pmatrix}
              \lambda_0 \\
              \lambda^2_0 \\
              \sigma^2_0
            \end{pmatrix} - \frac{1}{n}\E\left[\vu^\top_n\vu_L\right].\label{eq:Pequation_3_gmm}
\end{eqnarray}

At this point it is important to realized that we have three equations an three unknowns: $\lambda_0$, $\lambda^2_0$ and $\sigma^2_0$. Consider the following three-equations system implied by Equations \eqref{eq:Pequation_1_gmm}, \eqref{eq:Pequation_2_gmm} and \eqref{eq:Pequation_3_gmm}:
\begin{equation}\label{eq:system_gm}
  \mGamma_n\valpha_0  = \vgamma_n,
\end{equation}
%
where $\mGamma_n$ is given in Equation \eqref{eq:Gamma_gm}, and $\valpha_0 = (\lambda_0, \lambda^2_0, \sigma^2_0)$.\footnote{Note that we are assuming that $\lambda^2$ is a new parameter.} If $\mGamma_n$ where known, Assumption \ref{assu:gm_identification} (Identification) implies that Equation \eqref{eq:system_gm} determines $\valpha_0$ as:
\begin{equation*}
\valpha_0 = \mGamma_n^{-1}\vgamma_n,
\end{equation*}
%
where:
\begin{equation}\label{eq:Gamma_gm}
  \mGamma_n = \begin{pmatrix}
   \frac{2}{n}\E\left[\vu^\top_n\vu_L\right] & -\frac{1}{n}\E\left[\vu_L^\top\vu_L\right] & 1 \\
   \frac{2}{n}\E\left[\vu^\top_{LL}\vu_L\right] & -\frac{1}{n}\E\left[\vu_{LL}^\top\vu_{LL}\right] & \frac{1}{n} \tr(\mM^\top_n\mM_n)\\
   \frac{1}{n}\E\left[\vu^\top_n\vu_{LL} + \vu_L^\top\vu_L\right] & -\frac{1}{n}\E\left[\vu_{L}^\top\vu_{LL}\right] & 0
        \end{pmatrix},
\end{equation}
%
and
\begin{equation}
  \vgamma_n = \begin{pmatrix}
  \frac{1}{n}\E\left[\vu^\top_n\vu_n\right] \\
  \frac{1}{n}\E\left[\vu^\top_L\vu_L\right] \\
  \frac{1}{n}\E\left[\vu^\top_n\vu_L\right]
        \end{pmatrix}..
\end{equation}

Now, we express the moment conditions $\vgamma_n = \mGamma_n\valpha_n$ as sample averages in observables spatial lags of OLS residuals:
\begin{equation}\label{eq:sample_moments_sem}
  \vg_n = \mG_n\valpha_n +  \vupsilon_n(\lambda_n, \sigma^2_n),
\end{equation}
%
where $\vupsilon_n(\lambda_n, \sigma^2_n)$ can be viewed as a vector of residuals and $\mG_n$ is an $3\times 3$ matrix given by
\begin{equation}\label{eq:matrix-G-sem}
  \mG_n = \begin{pmatrix}
   \frac{2}{n}\widehat{\vu}^\top_n\widehat{\vu}_L & -\frac{1}{n}\widehat{\vu}_L^\top\widehat{\vu}_L & 1\\
   \frac{2}{n}\widehat{\vu}^\top_{LL}\widehat{\vu}_L & -\frac{1}{n}\widehat{\vu}_{LL}^\top\widehat{\vu}_{LL} & \frac{1}{n} \tr(\mM^\top_n\mM_n)\\
   \frac{1}{n}\left[\widehat{\vu}^\top_n\widehat{\vu}_{LL} + \widehat{\vu}_L^\top\widehat{\vu}_L\right] & -\frac{1}{n}\widehat{\vu}_{L}^\top\widehat{\vu}_{LL} & 0
        \end{pmatrix},
\end{equation}
%
and
\begin{equation}\label{eq:matrix-g-sem}
  \vg_n = \begin{pmatrix}
  \frac{1}{n}\widehat{\vu}^\top_n\widehat{\vu}_n \\
  \frac{1}{n}\widehat{\vu}^\top_L\widehat{\vu}_L \\
  \frac{1}{n}\widehat{\vu}^\top_n\widehat{\vu}_L
        \end{pmatrix}.
\end{equation}

Equation \eqref{eq:sample_moments_sem} can be thought as an OLS regression where \citep{kelejian1998generalized}:
\begin{equation}\label{eq:moments_as_ols}
\widetilde{\valpha}_n = \mG_n^{-1}\vg_n.
\end{equation}

However, the estimator in \eqref{eq:moments_as_ols} is based on an overparameterization in the sense that it does not use the information that the second element of $\valpha_n$, $\lambda^2_n$, is the squared of the first element. To address this issue, \cite{kelejian1998generalized} and \cite{kelejian1999generalized} define the MOM estimator for $\lambda$ and $\sigma^2$ as the nonlinear least square estimator corresponding to Equation \eqref{eq:sample_moments_sem}:\footnote{They argue that this estimator is more efficient than the OLS estimator. However, both estimator are consistent. See Theorem 2 in \citep{kelejian1998generalized}.}
\begin{equation}\label{eq:nls_estimator}
  (\widehat{\lambda}_{NLS, n}, \widehat{\sigma}^2_{NLS, n}) = \argmin \left\lbrace \vupsilon_n(\lambda_n, \sigma^2_n)^\top\vupsilon_n(\lambda_n, \sigma^2_n): \lambda_n \in [-a, a], \sigma^2_n\in [0, b]\right\rbrace 
\end{equation}

Note that $(\widehat{\lambda}_{NLS, n}, \widehat{\sigma}^2_{NLS, n})$ are defined as the minimizers of
\begin{equation}\label{eq:Qn-sem-mom}
Q_n(\lambda_n, \lambda^2_n, \sigma^2_n) = \left[\vg_n - \mG_n \begin{pmatrix}\lambda_n \\ \lambda ^2_n \\ \sigma^2_n\end{pmatrix}\right]^\top \left[\vg_n - \mG_n \begin{pmatrix}\lambda _n\\ \lambda ^2_n \\ \sigma^2_n\end{pmatrix}\right].
\end{equation}

We provide the following assumptions. 
%--------------------------------------------
\begin{assumption}[Bounded Matrices \citep{kelejian1999generalized}]\label{assu:bounded_matrix_M}
The row and column sums of the matrices $\mM_n$ and $(\mI_n - \lambda_0\mM_n)$ are bounded uniformly in absolute value.
\end{assumption}
%--------------------------------------------

%--------------------------------------------
\begin{assumption}[Residuals \citep{kelejian2010specification}]\label{assu:residuals_gm}
Let $\widetilde{u}_{i, n}$ denote the $i$-th element of $\widetilde{\vu}_n$. We then assume that
\begin{equation*}
\widetilde{u}_{i, n} - u_{i,n} = \vd_{i,n}\mDelta_n, 
\end{equation*}
%
where $\vd_{i, n}$ and $\mDelta_n$ are $1\times p$ and $p\times 1$ dimensional random vectors. Let $d_{ij,n}$ be the $j$th element of $\vd_{i, n}$. Then, we assume that for some $\delta>0$, $\E\left|d_{ij,n}\right|^{2 + \delta}\leq c_{d} < \infty$, where $c_d$ does not depend on $n$, and that
\begin{equation}
\sqrt{n}\left\lVert \mDelta_n\right\rVert = O_p(1).
\end{equation}
\end{assumption}
%--------------------------------

The vector $\vd_{i, n}$ denotes the $i$-th row of the regressor matrix, and $\mDelta_n$ denotes de difference between the parameter estimator and the true parameter values. This assumption should be satisfied for most cases in which $\widetilde{\vu}_n$ is based on $\sqrt{n}$-consistent estimators of the regression coefficients (non-linear OLS, linear OLS, S2SLS). Assumption \ref{assu:residuals_gm} comes from \cite{kelejian2010specification} and is a bit stronger than the corresponding assumption in \cite{kelejian1999generalized}.



%----------------------------------------------------------------------------------
\begin{assumption}[Identification \citep{kelejian1999generalized}]\label{assu:gm_identification} Let $\mGamma_n$ be the matrix in Equation (\ref{eq:Gamma_gm}). The smallest eigenvalues of $\mGamma^\top_n\mGamma_n$ is bounded away from zero, that is, $\omega_{min} (\mGamma^\top_n\mGamma_n) \geq \omega_* > 0$, where $\omega_*$ may depend on $\lambda$ and $\sigma^2$
\end{assumption}
%----------------------------------------------------------------------------------

The following Theorem establishes the consistency of the NLS estimator. 

%-------------------------------------------------------------------
\begin{theorem}[Consistency]\label{teo:consistency_gm_lambda}
Let $(\widehat{\lambda}_{NLS, n}, \widehat{\sigma}^2_{NLS, N})$ given by:
\begin{equation*}
  (\widehat{\lambda}_{NLS, n}, \widehat{\sigma}^2_{NLS, N}) = \argmin \left\lbrace \vupsilon_n(\lambda, \sigma^2)^\top\vupsilon_n(\lambda, \sigma^2): \rho \in [-a, a], \sigma^2\in [0, b]\right\rbrace 
\end{equation*}

Then, given Assumptions \ref{assu:errors_triang} (Heterokedastic errors), \ref{assu:KP1999_M} (Weight Matrix $\mM_n$), \ref{assu:bounded_matrix_M} (Bounded Matrices), \ref{assu:residuals_gm} (Residuals), and \ref{assu:gm_identification} (Identification),
\begin{equation*}
(\widehat{\lambda}_{NLS, n}, \widehat{\sigma}^2_{NLS, n})  \pto (\lambda_0, \sigma^2_0)\quad \mbox{as $n\to \infty$}
\end{equation*}
\end{theorem}
%---------------------------------------------------

An important remark is that Theorem \ref{teo:consistency_gm_lambda} establishes the consistency of the NLS estimates but does not provide information about the asymptotic distribution of  $\widehat{\lambda}_{NLS, n}$.

%--------------------------------------------------------------
\begin{proof}[Sketch or proof for NLS estimator of $\widehat{\lambda}_n$]
The proof is based on \cite{kelejian2017spatial} and consist into two steps. First, we prove consistency of $\widehat{\lambda}_n$ for the OLS estimate of $\lambda_0$---which is more simple---and assuming that the vector $\vu_n$ is observed. We then show that $\vu_n$ can be replaced in the GM estimator for $\lambda$ by $\widehat{\vu}_n$. For a more general proof see \citet{kelejian1998generalized, kelejian1999generalized}.
\begin{enumerate}
  \item \emph{Assuming that $\vu_n$ is observed.} Recall that in Equation \eqref{eq:sample_moments_sem} the sample moments are based on the estimated $\widehat{\vu}_n$. But, if $\vu_n$ were observed, then we would use the following sample moments:
\begin{equation*}
  \vg_n^* = \mG_n^*\valpha_0
\end{equation*}
%
where 
\begin{equation*}
  \mG_n^* = \begin{pmatrix}
   \frac{2}{n}\vu^\top_n\vu_L & -\frac{1}{n}\vu_L^\top\vu_L & 1\\
   \frac{2}{n}\vu^\top_{LL}\vu_L & -\frac{1}{n}\vu_{LL}^\top\vu_{LL} & \frac{1}{n} \tr(\mM^\top_n\mM_n)\\
   \frac{1}{n}\left[\vu^\top\vu_{LL} + \vu_L^\top\vu_L\right] & -\frac{1}{n}\vu_{L}^\top\vu_{LL} & 0
        \end{pmatrix},\; 
  \vg_n^* = \begin{pmatrix}
  \frac{1}{n}\vu^\top_n\vu_n \\
  \frac{1}{n}\vu^\top_L\vu_L \\
  \frac{1}{n}\vu^\top_n\vu_L
        \end{pmatrix}
\end{equation*}

Recall that:
\begin{equation*}
  \begin{aligned}
    \vu_n    & = \left(\mI_n - \lambda_0\mM_n\right)^{-1}\vepsi_n \\
    \vu_L    & = \mM_n\left(\mI_n - \lambda_0\mM_n\right)^{-1}\vepsi_n \\
    \vu_{LL} & = \mM_n\mM_n\left(\mI_n - \lambda_0\mM_n\right)^{-1}\vepsi_n
  \end{aligned}
\end{equation*}
%
and first and second column of $\mG^*_n$ are quadratic forms of $\vepsi_n$. Since $\mM_n$ is uniformly bounded then, using Lemma of consistency of quadratic forms \ref{lemma:O-lemma-lee}, we can state that:
\begin{equation*}
\mG^*_n\pto \mGamma_n.
\end{equation*}

Also:
\begin{equation*}
\begin{aligned}
\plim \vg_n^* &= \plim \mG^*_n\valpha_0, \\
 & = \mGamma_n\valpha_0.
 \end{aligned}
\end{equation*}

If $\vu_n$ would be observed, a linear GMM estimator for $\lambda_0$, say $\widetilde{\lambda}$,  would be the first element of the least squared estimator $\widetilde{\valpha}_n$, namely:
\begin{equation*}
  \widetilde{\valpha}_n= \mG_n^{-1*}\vg_n^*, 
\end{equation*}
%
since $\mG_n^{*}$ is a $3\times 3$ matrix which is nonsingular. Thus, using our previous results:
\begin{equation}\label{eq:consisten_error_gm}
  \plim \widetilde{\valpha}_n=\plim \mG_n^{-1*} \plim \vg_n^* = \mGamma_n^{-1}\vgamma_n = \valpha_0.
\end{equation}

\item \emph{Replacing $\vu_n$ by $\widehat{\vu}_n$}. Now consider the estimator of $\valpha_0$ based on $\widehat{\vu}_n$. The OLS estimator is consistent and can be expressed as:
\begin{equation*}
\widetilde{\vbeta}_n = \vbeta_0 + \Delta_n, \quad \Delta_n\pto \vzeros.
\end{equation*}

Then, the OLS estimator $\widehat{\vu}_n$ is:\footnote{For a more formal proof with any consistent estimate see Lemma C.1 in \cite{kelejian2010specification}.}
\begin{equation*}
\begin{aligned}
\widehat{\vu}_n & = \vy_n - \mX_n\widehat{\vbeta}_n,  \\
              & = \vy_n - \mX_n\left(\vbeta_0 + \Delta_n\right), \\
              & = \vy_n - \mX_n\vbeta_0 - \mX\Delta_n, \\
              & = \vu_n - \mX_n\Delta_n. 
\end{aligned}
\end{equation*}

Note that, with the exception of the constants in the third column of $\mG^*_n$, every element of $\mG^*_n$ and $\vg_n^*$ can be expressed as a quadratic terms of the form $\vepsi^\top_n\mS_n\vepsi_n/n$, where $\mS_n$ is an $n\times n$ matrix whose row and columns are uniformly bounded in absolute value given our assumption \ref{assu:bounded_matrix_M}. For example:
\begin{equation*}
      \frac{1}{n}\vu^\top_n\vu_L  = \frac{1}{n} \vepsi^{\top}_n\left(\mI_n - \lambda_0\mM_n\right)^{-1\top}\mM_n\left(\mI_n - \lambda_0\mM_n\right)^{-1}\vepsi_n = \frac{1}{n}\vepsi^\top_n\mS_n\vepsi_n.
\end{equation*}

Then:
\begin{equation*}
\begin{aligned}
\frac{\widehat{\vu}^\top_n\mS_n\widehat{\vu}_n}{n} & = \frac{\left(\vu_n - \mX_n\Delta_n \right)^\top\mS_n\left(\vu_n - \mX_n\Delta_n \right)}{n}, \\
& = \frac{\vu^\top_n\mS_n\vu_n}{n}- \frac{2\Delta_n^\top\mX^\top_n\mS_n\vu_n}{n}+\frac{\Delta_n^\top\mX^\top_n\mS_n\mX_n\Delta_n}{n}.
\end{aligned}
\end{equation*}

We need to show that:
\begin{equation*}
\begin{aligned}
  \frac{2\Delta_n^\top\mX^\top_n\mS_n\vu_n}{n} & \pto \vzeros, \\
  \frac{\Delta_n^\top\mX^\top_n\mS_n\mX_n\Delta_n}{n} & \pto \vzeros,
\end{aligned}
\end{equation*}
%
so that we can conclude that:
\begin{equation*}
\frac{\widehat{\vu}^\top_n\mS_n\widehat{\vu}_n}{n}  \pto \frac{1}{n}\vu^\top_n\mS_n\vu_n,
\end{equation*}
%
and finally say that:
\begin{equation*}
\vg_n\pto \vg_n^*\pto \vgamma_n, \quad \mG_n\pto \mG^*_n\pto \mGamma_n.
\end{equation*}

Given Equation \eqref{eq:consisten_error_gm}, consistency is proved.
\end{enumerate}
\end{proof}

We can also derive the MOM estimation using a GMM approach without weighting the moments:
\begin{equation*}
\min_{\vtheta \in \mTheta}\; Q_n(\vtheta_n) = \vg_n(\vtheta)^\top \vg_n(\vtheta). 
\end{equation*}
%---------------------------------------------------------

Using the moment conditions under homoskedasticity results in the following empirical moments:
\begin{equation*}
  \vg_n(\vtheta)  = 
  \begin{pmatrix}
  \vepsi_n^\top\vepsi_n  - n\widehat{\sigma}^2 \\
  \vepsi_n^\top\mM_n^\top\mM_n\vepsi_n - \widehat{\sigma}^2\tr(\mM_n^\top\mM_n) \\
  \vepsi_n^\top\mM_n\vepsi_n 
  \end{pmatrix} =
  \begin{pmatrix}
   \mR_n^\top\vu_n^\top\mR_n\vu_n  - n\widehat{\sigma}^2 \\
   \mR_n^\top\vu_n^\top\mM_n^\top\mM_n\mR_n\vu_n - \widehat{\sigma}^2\tr(\mM_n^\top\mM_n) \\
   \mR_n^\top\vu_n^\top\mM_n\mR_n\vu_n
  \end{pmatrix}
\end{equation*}
%
where $\vu_n$ are replaced by least squares residuals. 

%--------------------------------------------------------
\subsection{Feasible Generalized Least Squares Estimator}
%--------------------------------------------------------

In Section \ref{sec:swls} we established that the GLS estimator is given by:
\begin{equation}\label{eq:beta_gls_gm}
\vbeta_{GLS}(\lambda_0) =\left[\mX^\top_n\mOmega(\lambda_0)^{-1}\mX_n\right]^{-1}\mX^\top_n\mOmega(\lambda_0)^{-1}\vy_n,
\end{equation}
%
where $\mOmega(\lambda_0) = (\mI_n - \lambda_0\mM_n)^{-1}(\mI_n - \lambda_0\mM^\top_n)^{-1}$. The MOM procedure provides a consistent estimate for $\lambda_0$, which can be use to obtain the FGLS estimator:
\begin{equation}\label{eq:beta_fgls}
\vbeta_{FGLS}(\widehat{\lambda}_n) =\left[\mX^\top_n\mOmega(\widehat{\lambda}_n)^{-1}\mX_n\right]^{-1}\mX^\top_n\mOmega(\widehat{\lambda}_n)^{-1}\vy_n.
\end{equation}

The following theorem provides the identification conditions for the Spatial FGLS estimator.
%--------------------------------------------------------------------------
\begin{assumption}[Limiting Behavior]\label{assu:X_bounded_fgls}
  The elements of $\mX_n$ are non-stochastic and bounded in absolute value by $c_X, 0 < c_X < \infty$. Also, $\mX_n$ has full rank, and the matrix $\mQ_X = \lim_{n\to\infty}n^{-1}\mX^\top_n\mX_n$ is finite and nonsingular. Furthermore, the matrices
\begin{equation*}
\mQ_X(\lambda_0) = \lim_{n\to \infty} n^{-1}\mX^\top_n\mOmega(\lambda_0)^{-1}\mX_n 
\end{equation*}
%
are finite and nonsingular for all $\left|\lambda_0\right| < 1$
\end{assumption}
%--------------------------------------------------------------------------

The following Theorem proposes the asymptotic distribution for the SFGLS Estimator:

%-----------------------------------------------------------------------------
\begin{theorem}[Asymptotic Properties of FGLS Estimator]\label{teo:Asymptotic-FGLS}
  If assumptions \ref{assu:errors_triang} (Homoskedastic errors), \ref{assu:KP1999_M} (Weight Matrix $\mM_n$), \ref{assu:bounded_matrix_M} (Bounded Matrices), and \ref{assu:X_bounded_fgls} (Limiting Behavior) hold: 
  \begin{enumerate}
    \item The true GLS estimator $\widehat{\vbeta}_{GLS}$ is a consistent estimator for $\vbeta_0$, and
    \begin{equation*}
      \sqrt{n}\left(\widehat{\vbeta}_{GLS} - \vbeta_0\right)\dto \rN\left(\vzeros, \sigma^2 \mQ_X(\lambda_0)^{-1}\right)
    \end{equation*}
    
    \item Let $\widehat{\lambda}_n$ be a consistent estimator for $\lambda$. Then the true GLS estimator $\widehat{\vbeta}_{GLS}$ and the feasible GLS estimator $\widehat{\vbeta}_{FGLS}$ have the same asymptotic distribution.
    \item Suppose further than $\widehat{\sigma}^2_n$ is a consistent estimator for $\sigma^2$. Then $\widehat{\sigma}^2_n\left[n^{-1}\mX^\top_n\mOmega(\widehat{\lambda}_n)^{-1}\mX_n\right]$ is a consistent estimator for $\sigma^2 \mQ_X(\lambda_0)^{-1}$.
  \end{enumerate}
\end{theorem}

Note that Theorem \ref{teo:Asymptotic-FGLS} assumes the existence of a consistent estimator of $\lambda_0$ and $\sigma^2_0$. It can be shown that the OLS estimator:
\begin{equation*}
\widehat{\vbeta}_n = \left(\mX^\top_n\mX_n\right)^{-1}\mX^\top_n\vy_n, 
\end{equation*}
%
is $\sqrt{n}$-consistent. Thus, the OLS residuals $\widetilde{u}_i = y_i - \vx_i^\top\widehat{\vbeta}_n$ satisfy Assumption \ref{assu:residuals_gm} with $d_{i, n} = \left|\vx_i\right|$ and $\Delta_n = \widehat{\vbeta}_n -\vbeta_0$. Thus, OLS residuals can be used to obtain consistent estimators of $\lambda_0$ and $\sigma^2_0$.

Then, the SFGLS estimator is given by
\begin{equation*}
  \widehat{\vbeta}_{FGLS} = \left[\mX^\top(\widetilde{\lambda})\mX(\widetilde{\lambda})\right]^{-1}\mX^\top(\widetilde{\lambda})\vy(\widetilde{\lambda}),
\end{equation*}
%
where:
\begin{equation*}
  \mX(\widetilde{\lambda})  = (\mI_n-\widetilde{\lambda}_n\mM_n)\mX_n, \quad \vy(\widetilde{\lambda})  = (\mI_n-\widetilde{\lambda}_n\mM_n)\vy_n.
\end{equation*}

The variance covariance matrix of $\widehat{\vbeta}_{FGLS}$ is estimated as:
\begin{equation*}
  \widehat{\var}\left(\widehat{\vbeta}_{FGLS}\right) = \widehat{\sigma}^2_n\left[\mX^\top(\widetilde{\lambda})\mX(\widetilde{\lambda})\right]^{-1},
\end{equation*}
%
where:
\begin{equation*}
  \begin{aligned}
    \widehat{\sigma}^2_n & = \widehat{\vepsi}^\top(\widetilde{\lambda})\widehat{\vepsi}(\widetilde{\lambda}), \\
    \widehat{\vepsi}(\widetilde{\lambda}) & = \vy(\widetilde{\lambda}) - \mX(\widetilde{\lambda})\widehat{\vbeta}_{FGLS} = (\mI-\widetilde{\lambda}\mM)\widehat{\vu}_n, \\
    \widehat{\vu}_n & = \vy_n-\mX_n\widehat{\vbeta}_{FGLS}. 
  \end{aligned}
\end{equation*}


%---------------- Proof ---------------
\begin{proof}[Sketch of Proof of Theorem \ref{teo:Asymptotic-FGLS}]
We first prove part (a). Recall that the GLS and FGSL estimator are given by:
\begin{equation*}
\begin{aligned}
\widehat{\vbeta}_{GLS} & = \left[\mX^\top_n\mOmega(\lambda_0)^{-1}\mX_n\right]^{-1}\mX^\top_n\mOmega(\lambda_0)^{-1}\vy_n,  \\
\widehat{\vbeta}_{FGLS} & = \left[\mX^\top_n\widehat{\mOmega}(\lambda)^{-1}\mX\right]^{-1}\mX^\top\widehat{\mOmega}(\lambda)^{-1}\vy_n.
\end{aligned}
\end{equation*}

Since $\vy_n = \mX_n\vbeta_0 + \vu_n = \mX_n\vbeta_0 + \left(\mI_n-\lambda_0\mM_n\right)^{-1}\vepsi_n$, the sampling error of $\widehat{\vbeta}_{GLS}$ is,
\begin{equation*}
  \begin{aligned}
  \widehat{\vbeta}_n & = \vbeta_0 + \left[\mX^\top_n\mOmega(\lambda_0)^{-1}\mX_n\right]^{-1}\mX^\top_n\mOmega(\lambda_0)^{-1}\vu_n,  \\
  \widehat{\vbeta}_n - \vbeta_0 & =  \left[\mX^\top_n\mOmega(\lambda_0)^{-1}\mX_n\right]^{-1}\mX^\top\left(\mI_n - \lambda_0\mM_n\right)^\top\left(\mI_n - \lambda_0\mM_0\right)\left(\mI_n - \lambda_0\mM_n\right)^{-1}\vepsi_n, \\
  \widehat{\vbeta}_n - \vbeta_0 & =  \left[\mX^\top_n\mOmega(\lambda_0)^{-1}\mX_n\right]^{-1}\mX^\top_n\left(\mI_n - \lambda_0\mM_n\right)^\top\vepsi_n,  \\
  \sqrt{n}(\widehat{\vbeta}_n - \vbeta_0) & =  \left[\frac{1}{n}\mX^\top_n\mOmega(\lambda_0)^{-1}\mX_n\right]^{-1}\frac{1}{\sqrt{n}}\mA^\top_0\vepsi_n, 
  \end{aligned}
\end{equation*}
%
where $\mA_0 = \left(\mI_n - \lambda_0\mM_n\right)\mX_n$. By Assumption \ref{assu:X_bounded_fgls} (Limiting Behavior): 
\begin{equation*}
  \frac{1}{n}\mX^\top_n\mOmega(\lambda_0)^{-1}\mX_n \to \mQ_X(\lambda_0).
\end{equation*}

Since $\mQ_X$ is not singular, by continuous mapping theorem:
\begin{equation*}
  \left[\frac{1}{n}\mX^\top_n\mOmega(\lambda_0)^{-1}\mX_n\right]^{-1} \to \mQ_X^{-1}(\lambda_0).
\end{equation*}

Because $\mA_0$ is bounded in absolute value (why?), by Theorem \ref{teo:CLT_tri_arr} it follows that:
\begin{equation*}
\frac{1}{\sqrt{n}}\mA^\top_0\vepsi_n  \dto \rN\left(\vzeros, \lim_{n\to\infty}n^{-1}\sigma^2\mA^\top_0\mA_0\right), 
\end{equation*}
%
where  $\lim_{n\to\infty}n^{-1}\sigma^2\mA^\top_0\mA_0 = \sigma^2\lim_{n\to\infty}n^{-1}\mX^\top_n\left(\mI_n - \lambda_0\mM_n\right)^\top\left(\mI_n - \lambda_0\mM_n\right)\mX_n=\sigma^2\mQ_X(\lambda_0)$. Consequently:
\begin{equation*}
  \begin{aligned}
  \sqrt{n}(\widehat{\vbeta}_n - \vbeta_0) & =  \underbrace{\left[\frac{1}{n}\mX^\top_n\mOmega(\lambda_0)^{-1}\mX_0\right]^{-1}}_{\to \mQ_X^{-1}(\lambda_0)}\underbrace{\frac{1}{\sqrt{n}}\mA^\top\vepsi}_{\dto \rN(\vzeros, \sigma^2\mQ_X(\lambda_0))} \\
   & \dto \rN\left[\vzeros, \mQ_X^{-1}(\lambda_0)\sigma^2\mQ_X(\lambda_0)\mQ_X^{-1}(\lambda_0)^\top)\right], \\
   & \dto \rN\left[\vzeros, \sigma^2\mQ_X^{-1}(\lambda_0)\right].
  \end{aligned}
\end{equation*}

This also implies that $\widehat{\vbeta}_{GLS}$ is consistent (why?). To show part (b), we can show that:
\begin{equation*}
  \sqrt{n}(\widehat{\vbeta}_{GLS} - \widehat{\vbeta}_{FGLS}) \pto \vzeros.
\end{equation*}

Recall that $\plim(X_n- Y_n) = 0$ implies that the random variables $X_n$ and $Y_n$ have the same asymptotic distribution. Following \cite{kelejian1999generalized}, if suffices to show that 
\begin{equation}\label{eq:omega-omega}
\frac{1}{n}\mX^\top_n\left[\mOmega(\widehat{\lambda}_n)^{-1}- \mOmega(\lambda_0)^{-1}\right]\mX_n\pto \vzeros
\end{equation}
%
and
\begin{equation*}
\frac{1}{\sqrt{n}}\mX^\top_n\left[\mOmega(\widehat{\lambda}_n)^{-1}- \mOmega(\lambda_0)^{-1}\right]\vu_n\pto \vzeros.
\end{equation*}


To derive the expression for \(\mOmega(\widehat{\lambda}_n)^{-1} - \mOmega(\lambda_0)^{-1}\), we start by expanding both matrices in terms of \( \lambda_0\) and \(\widehat{\lambda}_n\).

Given that:
\[
\mOmega(\lambda_0)^{-1} = (\mI_n - \lambda_0\mM_n^\top)(\mI_n - \lambda_0 \mM_n),
\]
%
and
\[
\mOmega(\widehat{\lambda}_n)^{-1} = (\mI_n - \widehat{\lambda}_n \mM_n^\top)(\mI_n - \widehat{\lambda}_n \mM_n),
\]
%
we subtract these two expressions to obtain:
\[
\mOmega(\widehat{\lambda}_n)^{-1} - \mOmega(\lambda_0)^{-1} = \left[ (\mI_n - \widehat{\lambda}_n \mM_n^\top)(\mI_n - \widehat{\lambda}_n \mM_n) - (\mI_n - \lambda_0\mM_n^\top)(\mI_n - \lambda_0\mM_n) \right].
\]

Expanding yields:
\[
(\mI_n - \lambda \mM_n^\top)(\mI_n - \lambda \mM_n) = \mI_n - \lambda \mM_n^\top - \lambda \mM_n +-\lambda^2\mM_n^\top\mM_n,
\]

By subtracting these expansions, we obtain the first-order difference:
\begin{equation*}
\begin{aligned}
\mOmega(\widehat{\lambda}_n)^{-1} - \mOmega(\lambda)^{-1} & =  \mI_n - \lambda \mM_n^\top - \lambda \mM_n-\lambda^2\mM_n^\top\mM_n  \\
& - \left(\mI_n - \lambda_0 \mM_n^\top - \lambda_0 \mM_n -\lambda^2_0\mM_n^\top\mM_n\right), \\
& = (\lambda_0 - \widehat{\lambda}_n)(\mM + \mM^\top) + (\lambda^2_0 - \widehat{\lambda}_n^2)\mM^\top\mM.
\end{aligned}
\end{equation*}

Then using the fact the we have summable matrices, 
\begin{equation*}
  \frac{1}{n}\mX^\top_n\left[\mOmega(\widehat{\lambda}_n)^{-1}- \mOmega(\lambda_0)^{-1}\right]\mX_n = \underbrace{(\lambda_0 - \widehat{\lambda}_n)}_{\pto 0}\underbrace{n^{-1}\mX^\top_n(\mM_n + \mM^\top_n)\mX_n}_{O(1)} + \underbrace{(\lambda^2_0 - \widehat{\lambda}_n^2)}_{\pto 0}\underbrace{n^{-1}\mX^\top_n\mM^\top_n\mM_n\mX_n}_{O(1)},
\end{equation*}
%
where $(\lambda - \widehat{\lambda}_n)=o_p(1)$ since $\widehat{\lambda}_n$ is a consistent estimate of $\lambda$, and:
\begin{equation}
  \begin{aligned}
  \frac{1}{\sqrt{n}}\mX^\top_n\left[\mOmega(\widehat{\lambda}_n)^{-1}- \mOmega(\lambda_0)^{-1}\right]\vu_n & = \underbrace{(\lambda_0 - \widehat{\lambda}_n)}_{\pto 0}\underbrace{n^{-1/2}\mX^\top_n(\mM_n + \mM^\top_n)\vu_n}_{O_p(1)} + \underbrace{(\lambda^2_0 - \widehat{\lambda}_n^2)}_{\pto 0}\underbrace{n^{-1/2}\mX^\top\mM^\top\mM\vu}_{O_p(1)}, \\
  & =  o_p(1)*O_p(1) + o_p(1)*O_p(1), \\
  & = o_p(1) + o_p(1), \\
  & = o_p(1), \\
  & \pto 0.
  \end{aligned}
\end{equation}

To see that $n^{-1/2}\mX^\top_n(\mM_n + \mM^\top_n)\vu_n = O_p(1)$ note that
\begin{eqnarray*}
\E\left[n^{-1/2}\mX^\top_m(\mM_n + \mM^\top_n)\vu_n\right] &=& 0, \\
\var[n^{-1/2}\mX^\top_n(\mM_n + \mM^\top_n)\vu_n] &=& n^{-1}\underbrace{\mX^\top\underbrace{(\mM_n + \mM^\top_n)\mOmega_0(\mM^\top_n + \mM_n)}_{\mbox{absolutely summable}}\mX}_{O(n)} = O(1).
\end{eqnarray*}

A similar result holds for $n^{-1/2}\mX^\top_n\mM^\top_n\mM_n\vu_n$.

Part 3 of the theorem follows from \eqref{eq:omega-omega} and the fact that $\widehat{\sigma}^2$ is a consistent estimator for $\sigma^2$.
\end{proof}
%-----------------------------


A Feasible GLS (FGLS) can be obtained along with the following steps:

\begin{algorithm}[GLS (FGLS) Algorithm of SEM]\label{algorithm:FGSL}
The steps are the following:

\begin{enumerate}
  \item First of all obtain a consistent estimate of $\vbeta$, say $\widetilde{\vbeta}$ using either OLS or NLS.
  \item Use this estimate to obtain an estimate of $\vu$, say $\widehat{\vu}$,
  \item Use $\widehat{\vu}$, to estimate $\lambda$, say $\widehat{\lambda}$, using (\ref{eq:nls_estimator}),
  \item Estimate $\vbeta$ using Equation (\ref{eq:beta_fgls})
\end{enumerate}
\end{algorithm}


%==================================
\subsection{Coding the FSGLS estimator in R}
%==================================

In this Section, we show how to create a function for the FSGLS estimator of the SEM outlined in Algorithm \ref{algorithm:FGSL}. First, we generate a function that create the matrix $\mG_n$ and the moments vector $\vg_n$ given in Equation \eqref{eq:matrix-G-sem} and \eqref{eq:matrix-g-sem}, respectively. 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Function that generates g and G}
\hlstd{mom.sem} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{u}\hlstd{,} \hlkwc{M}\hlstd{)\{}
  \hlcom{# This function generates the moment conditions}
  \hlcom{# inputs: Consistent residuals and W matrix}
  \hlstd{n}       \hlkwb{<-} \hlkwd{length}\hlstd{(u)}
  \hlstd{u_l}     \hlkwb{<-} \hlstd{W} \hlopt{%*%} \hlstd{u}
  \hlstd{u_ll}    \hlkwb{<-} \hlstd{W} \hlopt{%*%} \hlstd{u_l}
  \hlstd{trMM}    \hlkwb{<-} \hlkwd{sum}\hlstd{(}\hlkwd{diag}\hlstd{(}\hlkwd{crossprod}\hlstd{(M)))}
  \hlstd{uu}      \hlkwb{<-} \hlkwd{crossprod}\hlstd{(u)}
  \hlstd{uul}     \hlkwb{<-} \hlkwd{crossprod}\hlstd{(u, u_l)}
  \hlstd{uull}    \hlkwb{<-} \hlkwd{crossprod}\hlstd{(u, u_ll)}
  \hlstd{ullul}   \hlkwb{<-} \hlkwd{crossprod}\hlstd{(u_ll, u_l)}
  \hlstd{ullull}  \hlkwb{<-} \hlkwd{crossprod}\hlstd{(u_ll, u_ll)}
  \hlstd{ulul}    \hlkwb{<-} \hlkwd{crossprod}\hlstd{(u_l, u_l)}
  \hlstd{ulull}   \hlkwb{<-} \hlkwd{crossprod}\hlstd{(u_l, u_ll)}
  \hlstd{G}       \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{3}\hlstd{,} \hlnum{3}\hlstd{)}
  \hlstd{G[}\hlnum{1}\hlstd{,} \hlnum{1}\hlstd{]} \hlkwb{<-} \hlnum{2} \hlopt{*} \hlstd{uul}
  \hlstd{G[}\hlnum{2}\hlstd{,} \hlnum{1}\hlstd{]} \hlkwb{<-} \hlnum{2} \hlopt{*} \hlstd{ullul}
  \hlstd{G[}\hlnum{3}\hlstd{,} \hlnum{1}\hlstd{]} \hlkwb{<-} \hlstd{uull} \hlopt{+} \hlstd{ulul}
  \hlstd{G[}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{]} \hlkwb{<-} \hlopt{-}\hlstd{ulul}
  \hlstd{G[}\hlnum{2}\hlstd{,} \hlnum{2}\hlstd{]} \hlkwb{<-} \hlopt{-}\hlstd{ullull}
  \hlstd{G[}\hlnum{3}\hlstd{,} \hlnum{2}\hlstd{]} \hlkwb{<-} \hlopt{-}\hlstd{ulull}
  \hlstd{G[}\hlnum{1}\hlstd{,} \hlnum{3}\hlstd{]} \hlkwb{<-} \hlnum{1} \hlopt{*} \hlstd{n}
  \hlstd{G[}\hlnum{2}\hlstd{,} \hlnum{3}\hlstd{]} \hlkwb{<-} \hlstd{trMM}
  \hlstd{G} \hlkwb{<-} \hlstd{G} \hlopt{/} \hlstd{n}
  \hlstd{g} \hlkwb{<-} \hlkwd{c}\hlstd{(uu, ulul, uul)} \hlopt{/} \hlstd{n}
  \hlkwd{list}\hlstd{(}\hlkwc{G} \hlstd{= G,} \hlkwc{g} \hlstd{= g)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

In the following lines, we provide a function that returns the $Q_n = \vupsilon_n^\top\vupsilon_n$ defined in Equation \eqref{eq:Qn-sem-mom}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Function to be optimized}
\hlstd{Qn} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{par}\hlstd{,} \hlkwc{mom}\hlstd{,} \hlkwc{verbose} \hlstd{= verbose)\{}
    \hlcom{# par has lambda and sigma}
    \hlstd{upsilon} \hlkwb{<-} \hlstd{mom}\hlopt{$}\hlstd{g} \hlopt{-} \hlstd{mom}\hlopt{$}\hlstd{G} \hlopt{%*%} \hlkwd{c}\hlstd{(par[}\hlnum{1}\hlstd{], par[}\hlnum{1}\hlstd{]}\hlopt{^}\hlnum{2}\hlstd{, par[}\hlnum{2}\hlstd{])}
    \hlstd{upup} \hlkwb{<-} \hlkwd{crossprod}\hlstd{(upsilon)}
    \hlkwa{if} \hlstd{(verbose)}
      \hlkwd{cat}\hlstd{(}\hlstr{"function:"}\hlstd{, upup,} \hlstr{"lambda:"}\hlstd{, par[}\hlnum{1}\hlstd{],} \hlstr{"sig2:"}\hlstd{,}
          \hlstd{par[}\hlnum{2}\hlstd{],} \hlstr{"\textbackslash{}n"}\hlstd{)}
   \hlkwd{return}\hlstd{(upup)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}


Next, we create the main function that generates the FSGLS estimator of the SEM model:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Main function for the FSGLSE}
\hlstd{sem.sfgls} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{formula}\hlstd{,} \hlkwc{data}\hlstd{,} \hlkwc{M}\hlstd{,} \hlkwc{verbose} \hlstd{=} \hlnum{TRUE}\hlstd{)\{}
  \hlcom{# Model Frame}
  \hlstd{callT}    \hlkwb{<-} \hlkwd{match.call}\hlstd{(}\hlkwc{expand.dots} \hlstd{=} \hlnum{TRUE}\hlstd{)}
  \hlstd{mf}       \hlkwb{<-} \hlstd{callT}
  \hlstd{m}        \hlkwb{<-} \hlkwd{match}\hlstd{(}\hlkwd{c}\hlstd{(}\hlstr{"formula"}\hlstd{,} \hlstr{"data"}\hlstd{),} \hlkwd{names}\hlstd{(mf),} \hlnum{0L}\hlstd{)}
  \hlstd{mf}       \hlkwb{<-} \hlstd{mf[}\hlkwd{c}\hlstd{(}\hlnum{1L}\hlstd{, m)]}
  \hlstd{mf[[}\hlnum{1L}\hlstd{]]} \hlkwb{<-} \hlkwd{as.name}\hlstd{(}\hlstr{"model.frame"}\hlstd{)}
  \hlstd{mf} \hlkwb{<-} \hlkwd{eval}\hlstd{(mf,} \hlkwd{parent.frame}\hlstd{())}

  \hlcom{# Get variables and globals}
  \hlstd{y}  \hlkwb{<-} \hlkwd{model.response}\hlstd{(mf)}
  \hlstd{X}  \hlkwb{<-} \hlkwd{model.matrix}\hlstd{(formula, mf)}
  \hlstd{n}  \hlkwb{<-} \hlkwd{nrow}\hlstd{(X)}
  \hlstd{k}  \hlkwb{<-} \hlkwd{ncol}\hlstd{(X)}
  \hlstd{sn} \hlkwb{<-} \hlkwd{nrow}\hlstd{(M)}
  \hlkwa{if} \hlstd{(n} \hlopt{!=} \hlstd{sn)} \hlkwd{stop}\hlstd{(}\hlstr{"number of spatial units in W is different to the number of data"}\hlstd{)}

  \hlcom{# First Step: Obtain consistent residuals from OLS}
  \hlstd{ols}   \hlkwb{<-} \hlkwd{lm}\hlstd{(y} \hlopt{~} \hlstd{X} \hlopt{-} \hlnum{1}\hlstd{)}
  \hlstd{u.hat} \hlkwb{<-} \hlkwd{residuals}\hlstd{(ols)}

  \hlcom{# Generate Moments}
  \hlstd{mom.hat} \hlkwb{<-} \hlkwd{mom.sem}\hlstd{(}\hlkwc{u} \hlstd{= u.hat,} \hlkwc{M} \hlstd{= M)}

  \hlcom{# Initial values for lambda and sigma}
  \hlstd{scorr} \hlkwb{<-} \hlkwd{crossprod}\hlstd{(W} \hlopt{%*%} \hlstd{u.hat, u.hat)} \hlopt{/} \hlkwd{crossprod}\hlstd{(u.hat)}
  \hlstd{par}  \hlkwb{<-} \hlkwd{c}\hlstd{(scorr,} \hlkwd{var}\hlstd{(u.hat))}

  \hlcom{#Optimization}
  \hlstd{opt} \hlkwb{<-} \hlkwd{nlminb}\hlstd{(}\hlkwc{start} \hlstd{=  par, Qn,} \hlkwc{mom} \hlstd{= mom.hat,} \hlkwc{verbose} \hlstd{= verbose)}

  \hlcom{# B FSGLS}
  \hlstd{lambda.hat} \hlkwb{<-} \hlstd{opt}\hlopt{$}\hlstd{par[}\hlnum{1L}\hlstd{]}
  \hlstd{ys}      \hlkwb{<-} \hlstd{y} \hlopt{-} \hlkwd{drop}\hlstd{(lambda.hat)} \hlopt{*} \hlstd{W} \hlopt{%*%} \hlstd{y}
  \hlstd{Xs}      \hlkwb{<-} \hlstd{X} \hlopt{-} \hlkwd{drop}\hlstd{(lambda.hat)} \hlopt{*} \hlstd{W} \hlopt{%*%} \hlstd{X}
  \hlstd{b.hat}   \hlkwb{<-} \hlkwd{solve}\hlstd{(}\hlkwd{crossprod}\hlstd{(Xs))} \hlopt{%*%} \hlkwd{crossprod}\hlstd{(Xs, ys)}

  \hlcom{# Residuals}
  \hlstd{e.hat} \hlkwb{<-} \hlstd{ys} \hlopt{-} \hlstd{Xs} \hlopt{%*%} \hlstd{b.hat}

  \hlcom{# Save results}
  \hlstd{results} \hlkwb{<-} \hlkwd{structure}\hlstd{(}
    \hlkwd{list}\hlstd{(}
      \hlkwc{coefficients} \hlstd{=} \hlkwd{c}\hlstd{(b.hat, lambda.hat),}
      \hlkwc{call}         \hlstd{= callT,}
      \hlkwc{X}            \hlstd{= X,}
      \hlkwc{y}            \hlstd{= y,}
      \hlkwc{Xs}           \hlstd{= Xs,}
      \hlkwc{e.hat}        \hlstd{= e.hat}
    \hlstd{),}
    \hlkwc{class} \hlstd{=} \hlstr{'myfs2sls'}
  \hlstd{)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

We generate the following DGP to test our function:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Generate DGP}
\hlkwd{library}\hlstd{(}\hlstr{"spatialreg"}\hlstd{)}
\hlkwd{library}\hlstd{(}\hlstr{"spdep"}\hlstd{)}
\hlkwd{set.seed}\hlstd{(}\hlnum{1}\hlstd{)}
\hlstd{n}      \hlkwb{<-} \hlnum{529}
\hlstd{lambda} \hlkwb{<-} \hlnum{0.6}
\hlstd{M.nb2}  \hlkwb{<-} \hlkwd{cell2nb}\hlstd{(}\hlkwd{sqrt}\hlstd{(n),} \hlkwd{sqrt}\hlstd{(n))}
\hlstd{M}      \hlkwb{<-} \hlkwd{nb2mat}\hlstd{(M.nb2)}

\hlcom{# Exogenous variables}
\hlstd{x1}     \hlkwb{<-} \hlkwd{rnorm}\hlstd{(n)}
\hlstd{x2}     \hlkwb{<-} \hlkwd{rnorm}\hlstd{(n)}
\hlstd{x3}     \hlkwb{<-} \hlkwd{rnorm}\hlstd{(n)}

\hlcom{# DGP parameters}
\hlstd{b0} \hlkwb{<-} \hlnum{0} \hlstd{; b1} \hlkwb{<-} \hlopt{-}\hlnum{1}\hlstd{; b2} \hlkwb{<-} \hlnum{0}\hlstd{; b3} \hlkwb{<-} \hlnum{1}
\hlstd{sigma2} \hlkwb{<-} \hlnum{2}
\hlstd{epsilon} \hlkwb{<-} \hlkwd{rnorm}\hlstd{(n,} \hlkwc{mean} \hlstd{=} \hlnum{0}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlkwd{sqrt}\hlstd{(sigma2))}

\hlcom{# Simulate the dependent variable}
\hlstd{y} \hlkwb{<-} \hlstd{b0} \hlopt{+} \hlstd{b1}\hlopt{*}\hlstd{x1} \hlopt{+} \hlstd{b2}\hlopt{*}\hlstd{x2} \hlopt{+} \hlstd{b3}\hlopt{*}\hlstd{x3} \hlopt{+} \hlkwd{solve}\hlstd{(}\hlkwd{diag}\hlstd{(n)} \hlopt{-}  \hlstd{lambda} \hlopt{*} \hlstd{M)} \hlopt{%*%} \hlstd{epsilon}

\hlstd{data} \hlkwb{<-} \hlkwd{as.data.frame}\hlstd{(}\hlkwd{cbind}\hlstd{(y, x1, x2, x3))}
\hlkwd{names}\hlstd{(data)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"y"}\hlstd{,} \hlstr{"x1"}\hlstd{,} \hlstr{"x2"}\hlstd{,} \hlstr{"x3"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

Generate S3 methods:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{vcov.myfs2sls} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{object}\hlstd{,} \hlkwc{...}\hlstd{)\{}
  \hlstd{sigma2} \hlkwb{<-} \hlkwd{crossprod}\hlstd{(object}\hlopt{$}\hlstd{e.hat)} \hlopt{/} \hlstd{n}
  \hlstd{var}    \hlkwb{<-} \hlkwd{drop}\hlstd{(sigma2)} \hlopt{*} \hlkwd{solve}\hlstd{(}\hlkwd{crossprod}\hlstd{(object}\hlopt{$}\hlstd{Xs))}
  \hlkwd{return}\hlstd{(var)}
\hlstd{\}}
\hlstd{summary.myfs2sls} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{object}\hlstd{,}
                              \hlkwc{table} \hlstd{=} \hlnum{TRUE}\hlstd{,}
                              \hlkwc{digits} \hlstd{=} \hlkwd{max}\hlstd{(}\hlnum{3}\hlstd{, .Options}\hlopt{$}\hlstd{digits} \hlopt{-} \hlnum{3}\hlstd{),}
                              \hlkwc{...}\hlstd{)\{}
    \hlstd{n}       \hlkwb{<-} \hlkwd{nrow}\hlstd{(object}\hlopt{$}\hlstd{X)}
    \hlstd{K}       \hlkwb{<-} \hlkwd{ncol}\hlstd{(object}\hlopt{$}\hlstd{X)}
    \hlstd{df}      \hlkwb{<-} \hlstd{n} \hlopt{-} \hlstd{K}
    \hlstd{b}       \hlkwb{<-} \hlstd{object}\hlopt{$}\hlstd{coefficients[}\hlnum{1}\hlopt{:}\hlstd{K]}
    \hlstd{std.err} \hlkwb{<-} \hlkwd{sqrt}\hlstd{(}\hlkwd{diag}\hlstd{(}\hlkwd{vcov}\hlstd{(object)))}
    \hlstd{z}       \hlkwb{<-} \hlstd{b} \hlopt{/} \hlstd{std.err}
    \hlstd{p}       \hlkwb{<-} \hlnum{2} \hlopt{*} \hlkwd{pt}\hlstd{(}\hlopt{-}\hlkwd{abs}\hlstd{(z),} \hlkwc{df} \hlstd{= df)}
    \hlstd{CoefTable} \hlkwb{<-} \hlkwd{cbind}\hlstd{(b, std.err, z, p)}
    \hlkwd{colnames}\hlstd{(CoefTable)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"Estimate"}\hlstd{,} \hlstr{"Std.Error"}\hlstd{,} \hlstr{"t-value"}\hlstd{,} \hlstr{"Pr(>|t|)"}\hlstd{)}
    \hlstd{result} \hlkwb{<-} \hlkwd{structure}\hlstd{(}
      \hlkwd{list}\hlstd{(}
        \hlkwc{CoefTable} \hlstd{= CoefTable,}
        \hlkwc{digits}    \hlstd{= digits,}
        \hlkwc{call}      \hlstd{= object}\hlopt{$}\hlstd{call),}
      \hlkwc{class} \hlstd{=} \hlstr{'summary.myfs2sls'}
    \hlstd{)}
    \hlkwd{return}\hlstd{(result)}
\hlstd{\}}

\hlstd{print.summary.myfs2sls} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{,}
                                   \hlkwc{digits} \hlstd{= x}\hlopt{$}\hlstd{digits,}
                                   \hlkwc{na.print} \hlstd{=} \hlstr{""}\hlstd{,}
                                   \hlkwc{symbolic.cor} \hlstd{= p} \hlopt{>} \hlnum{4}\hlstd{,}
                                   \hlkwc{signif.stars} \hlstd{=} \hlkwd{getOption}\hlstd{(}\hlstr{"show.signif.stars"}\hlstd{),}
                                   \hlkwc{...}\hlstd{)}
\hlstd{\{}
  \hlkwd{cat}\hlstd{(}\hlstr{"\textbackslash{}nCall:\textbackslash{}n"}\hlstd{)}
  \hlkwd{cat}\hlstd{(}\hlkwd{paste}\hlstd{(}\hlkwd{deparse}\hlstd{(x}\hlopt{$}\hlstd{call),} \hlkwc{sep} \hlstd{=} \hlstr{"\textbackslash{}n"}\hlstd{,} \hlkwc{collapse} \hlstd{=} \hlstr{"\textbackslash{}n"}\hlstd{),} \hlstr{"\textbackslash{}n\textbackslash{}n"}\hlstd{,} \hlkwc{sep} \hlstd{=} \hlstr{""}\hlstd{)}

  \hlkwd{cat}\hlstd{(}\hlstr{"\textbackslash{}nCoefficients:\textbackslash{}n"}\hlstd{)}
  \hlkwd{printCoefmat}\hlstd{(x}\hlopt{$}\hlstd{CoefTable,} \hlkwc{digit} \hlstd{= digits,} \hlkwc{P.value} \hlstd{=} \hlnum{TRUE}\hlstd{,} \hlkwc{has.Pvalue} \hlstd{=} \hlnum{TRUE}\hlstd{)}
  \hlkwd{invisible}\hlstd{(}\hlkwa{NULL}\hlstd{)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

We test our function and compare it with \code{GMerrorsar}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{b.fs2sls} \hlkwb{<-} \hlkwd{sem.sfgls}\hlstd{(y} \hlopt{~} \hlstd{x1} \hlopt{+} \hlstd{x2} \hlopt{+} \hlstd{x3,} \hlkwc{data} \hlstd{= data,} \hlkwc{M} \hlstd{= M,} \hlkwc{verbose} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\hlkwd{summary}\hlstd{(b.fs2sls)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## sem.sfgls(formula = y ~ x1 + x2 + x3, data = data, M = M, verbose = FALSE)
## 
## 
## Coefficients:
##              Estimate Std.Error t-value Pr(>|t|)    
## (Intercept) -0.192429  0.149854  -1.284    0.200    
## x1          -1.079338  0.063535 -16.988   <2e-16 ***
## x2           0.006391  0.060054   0.106    0.915    
## x3           0.975246  0.064009  15.236   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}
\end{kframe}
\end{knitrout}

Check
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{sem_mm}    \hlkwb{<-} \hlkwd{GMerrorsar}\hlstd{(y} \hlopt{~} \hlstd{x1} \hlopt{+} \hlstd{x2} \hlopt{+} \hlstd{x3,}
                        \hlkwc{data} \hlstd{= data,}
                        \hlkwc{listw} \hlstd{=} \hlkwd{mat2listw}\hlstd{(M,} \hlkwc{style} \hlstd{=} \hlstr{"W"}\hlstd{),}
                        \hlkwc{verbose} \hlstd{=} \hlnum{FALSE}\hlstd{,}
                        \hlkwc{legacy} \hlstd{=}  \hlnum{TRUE}\hlstd{)}
\hlkwd{summary}\hlstd{(sem_mm)}
\end{alltt}
\begin{verbatim}
## 
## Call:GMerrorsar(formula = y ~ x1 + x2 + x3, data = data, listw = mat2listw(M, 
##     style = "W"), verbose = FALSE, legacy = TRUE)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -3.748647 -0.946972 -0.050107  1.076211  4.413975 
## 
## Type: GM SAR estimator
## Coefficients: (GM standard errors) 
##              Estimate Std. Error  z value Pr(>|z|)
## (Intercept) -0.192429   0.149854  -1.2841   0.1991
## x1          -1.079338   0.063535 -16.9880   <2e-16
## x2           0.006391   0.060054   0.1064   0.9152
## x3           0.975246   0.064009  15.2362   <2e-16
## 
## Lambda: 0.55602 (standard error): 0.10098 (z-value): 5.5061
## Residual variance (sigma squared): 2.3414, (sigma: 1.5302)
## GM argmin sigma squared: 2.3535
## Number of observations: 529 
## Number of parameters estimated: 6
\end{verbatim}
\end{kframe}
\end{knitrout}


% %===================================
% \subsection{FGLS in R}
% %===================================
% 
% 
% 
% The estimation procedure by GM is carried out by the \code{GMerrorsar} function from \pkg{spatialreg} package. In order to show its functionalities we first load the required packages and dataset:
% 
% <<load-sem-gm>>=
% # Load data and packages
% library("memisc") 
% library("spdep")
% library("spatialreg")
% data("columbus")
% listw <- nb2listw(col.gal.nb)
% source("getSummary.sarlm.R")
% @
% 
% Now we estimate the SEM model by ML using Ord's eigen approximation of the determinant and the \cite{kelejian1999generalized}'s GM procedure:
% 
% <<sem-gm-estimation>>=
% # Estimate the SEM model by ML and GM
% sem_ml <- errorsarlm(CRIME ~ INC + HOVAL, 
%                      data = columbus,
%                      listw, 
%                      method = "eigen")
% sem_mm    <- GMerrorsar(CRIME ~ HOVAL + INC, 
%                   data =  columbus,
%                   listw = listw,
%                   returnHcov =  TRUE)
% @
% 
% A Hausman test comparing an OLS and SEM model can be obtained using
% <<huasman-gm>>=
% # Hausman test
% summary(sem_mm, Hausman = TRUE)
% @
% 
% The default model specification shown above. The output follows the familiar R format. Note that even though the estimation procedure is the GM, the output presents inference for $\lambda$. In this case, the inference is based on the analytical method described in \url{http://econweb.umd.edu/~prucha/STATPROG/OLS/desols.pdf}. The output also shows the Hausman test. Recall that this test can be used whenever there are two estimators, one of which is inefficient but consistent (OLS in this case under the maintained hypothesis of the SEM), while the other is efficient (SEM in this case).  The null hypothesis is that the SEM and OLS estimates are not significantly different \citep[see][pag. 62]{lesage2010introduction}. We reject the null hypothesis, thus the SEM model is more appropriate. Table \ref{tab:columbus-models3} compares the estimates. 
% 
% \begin{table}[ht]
% \caption{Spatial Models for Crime in Columbus: ML vs GM}\label{tab:columbus-models3}
% \centering
% <<echo = FALSE, results = 'asis', warning = FALSE>>=
% table_2 <- mtable("ML"   = sem,
%                   "GM" =  sem_mm,
%        summary.stats = c("N"),
%        coef.style = "default")
% table_2 <- relabel(table_2,
%                    "(Intercept)" = "\\emph{Constant}",
%                    "lambda" = "$\\lambda$") 
% toLatex(table_2, compact = TRUE, useBooktabs =  TRUE)
% @
% \end{table}

%===================================
\section{GMM Estimator for SEM}\label{sec:gmm-sem}
%===================================

%-----------------------------------------------------
\subsection{GMM Estimator Under Homoskedasticity}
%-----------------------------------------------------

In this section, we provide the GMM estimator for the SEM model under homoskedasticity following \cite{lee2010efficient} and \cite{liu2010efficient}. It is important to note that both articles derive the OGMME for the SAC model, but \cite{lee2010efficient} focuses on SAC model with higher orders. 

Recall that the SEM model is given by:
\begin{equation*}
 \begin{aligned}
  \vy_n  & = \mX\vbeta_0 +  \vu_n  \\
  \vu_n & =\lambda_0 \mM_n\vu_n + \vepsi_n, 
\end{aligned}
\end{equation*}
%
where $\mP_{jn}, j = 1,2$ are from $\calP_{1n}$, and $\epsilon_{ni}, i = 1, \ldots, n$, of $\vepsi_n$  are i.i.d. $(0, \sigma^2_0)$ with zero mean and variance $\sigma_0^2$. For this model, denote $\vepsi_n = \left(\mI_n - \lambda_0\mM_n\right)\vu_n = \mR_0\vu_n$, where $\mR_0 = \left(\mI_n - \lambda_0\mM_n\right)$, and $\vu_n = \vy_n - \mX_n\vbeta_0$. The reduced form equation of this model is
\begin{equation}\label{eq:reduced-sem-gmm}
\vy_n = \mX_n\vbeta_0 + \mR_0^{-1}\vepsi_n. 
\end{equation}

Let $\vtheta = (\vbeta^\top,\lambda)^\top$. The GMM estimator of this model can be based on the following linear and quadratic empirical moments
\begin{equation*}
 \vg_n(\vtheta) = \begin{pmatrix}
                    \mX_n^\top
                    \vepsi_n \\
                    \vepsi_n(\vtheta)^\top\mP_{1n}^\top\vepsi_n(\vtheta) \\
                    \vepsi_n(\vtheta)^\top\mP_{2n}^\top\vepsi_n(\vtheta)
                  \end{pmatrix}
                  =
                  \begin{pmatrix}
                   \mX_n^\top\mR_n\vu_n  \\
                    \vu_n^\top\mR_n^\top \mP_{1n}^\top\mR_n\vu_n \\
                    \vu_n^\top\mR_n^\top \mP_{2n}^\top\mR_n\vu_n
                  \end{pmatrix}
                  = \begin{pmatrix}
    \mX_n^\top \\
    \vepsi_n^\top\mP_{1n}^\top \\
    \vepsi_n^\top\mP_{2n}^\top
\end{pmatrix} 
\mR_n\vu_n, 
\end{equation*}
%
where $\mR_n = (\mI_n - \lambda \mM_n)$. At $\vtheta = \vtheta_0$, the population moments are $\E\left[\vg_n(\vtheta_0)\right] = \E\left[\left(\mX_n, \mP_{1n}\vepsi_n, \mP_{2n}\vepsi_n\right)^\top\vepsi_n\right] = \vzeros$, because under exogeneity $\E(\mX_n^\top\vepsi_n) = \mX_n^\top\E(\vepsi_n) = \vzeros$ and $\E(\vepsi_n^\top\mP_{jn}\vepsi_n) = \sigma_0^2\tr(\mP_{jn}) = 0$ for $j = 1, 2$. 

The derivatives of $\vg_n(\vtheta)$ with respect to $\vbeta$ and $\lambda$ are
\begin{equation*}
  \begin{aligned}
    \frac{\partial \vg_n(\vtheta)}{\partial \vtheta ^\top} & = \begin{pmatrix}
    \mX_n^\top\frac{\partial \vepsi_n}{\partial \vtheta^\top} \\
    \vepsi_n^\top\mP_{1n}^s\frac{\partial \vepsi_n}{\partial \vtheta^\top} \\
     \vepsi_n^\top\mP_{2n}^s\frac{\partial \vepsi_n}{\partial \vtheta^\top}
\end{pmatrix} = \begin{pmatrix}
    \mX_n^\top \\
    \vepsi_n^\top \mP_{1n}^s \\
     \vepsi_n^\top \mP_{2n}^s
\end{pmatrix}\frac{\partial \vepsi_n}{\partial \vtheta^\top}, 
  \end{aligned}
\end{equation*}
%
where $\mP_{jn}^s = \mP_{jn} + \mP_{jn}^\top$, $\vepsi_n = \mR_n\vy_n - \mR_n\mX\vbeta$, and 
\begin{equation*}
  \frac{\partial \vepsi_n}{\partial \vtheta^\top}  = \begin{pmatrix}
    \frac{\partial \vepsi_n}{\partial \vbeta^\top} & \frac{\partial \vepsi_n}{\partial \lambda} 
    \end{pmatrix}
= \begin{pmatrix}
      -\mR_n\mX_n & - \mM_n\vu_n
  \end{pmatrix}.
\end{equation*}

Thus, 
\begin{equation*}
\frac{\partial \vg_n(\vtheta)}{\partial \vtheta ^\top} = 
- \begin{pmatrix}
 \mX_n^\top\mR_n\mX_n & \mX_n^\top\mM_n\vu_n \\
 \vepsi_n^\top\mP_{1n}^s\mR_n\mX_n & \vepsi_n^\top\mP_{1n}^s\mM_n\vu_n \\
 \vepsi_n^\top\mP_{2n}^s\mR_n\mX_n & \vepsi_n^\top\mP_{2n}^s\mM_n\vu_n 
\end{pmatrix}.
\end{equation*}

The variance-covariance matrix of the moments is:
\begin{equation*}
\underset{(k + 2) \times (k+2)}{\mOmega_n} = \begin{pmatrix}
\underset{(k \times k)}{\mZeros} & \underset{(k \times 2)}{\mu_3 \mX_n^\top\vomega_n} \\
              \underset{(2 \times k)}{\mu_3\vomega^\top_n\mX_n}  & \underset{(2 \times 2)}{(\mu_4 - 3\sigma_0^4)\vomega_n^\top\vomega_n} & \\
               & 
            \end{pmatrix}
             + 
             \mV_n,
\end{equation*}
%
with $\vomega_n = \left[\diag(\mP_{1n}), \diag(\mP_{2n})\right]$ is $n\times 2$ and 
\begin{equation*}
  \mV_n = \sigma_0^4
          \begin{pmatrix}
          \underset{(k\times k)}{\frac{1}{\sigma_0^2}\mX_n^\top\mX_n} & \underset{(k\times 1)}{\vzeros} & \underset{(k\times 1)}{\vzeros} \\
          \underset{(1\times k)}{\vzeros}
          & \underset{(1\times 1)}{\tr(\mP_{1n}^s\mP_{1n})} & \underset{(1\times 1)}{\tr(\mP_{1n}^s\mP_{2n})}  \\
          \underset{(1\times k)}{\vzeros} &
          \tr(\mP_{2n}^s\mP_{1n}) & \tr(\mP_{2n}^s\mP_{2n})
          \end{pmatrix}.
\end{equation*}

When $\vepsi_n$ is normally distributed, $\mOmega_n$ is simplified to $\mV_n$ because $\mu_3$ and $\mu_4 = 3\sigma_0^3$.

If $\mUpsilon_n = \left(\frac{1}{n}\widehat{\mOmega}_n\right)^{-1}$, then the OGMM estimator has the following asymptotic distribution
\begin{equation*}
\sqrt{n}\left(\widehat{\vtheta}_n - \vtheta_0\right)\dto \rN(\vzeros, \mSigma^0),
\end{equation*}
%
where
\begin{equation*}
\mSigma^0 = \left[\lim_{n\to\infty}\mD_n^\top\mOmega_n\mD_n\right]^{-1}.
\end{equation*}

At $\vtheta_0$, $\frac{\partial \E\left[g(\vtheta_0)\right]}{\partial \vtheta^\top} = - \mD_n$, where
\begin{equation}\label{eq:D-gmm-sem}
\underset{(k + 2)\times (k + 1)}{\mD_n} = 
\begin{pmatrix}
  \underset{k\times k}{\mX_n^\top\mR_0\mX_n} & \underset{k\times 1}{\vzeros} \\
  \underset{1 \times k}{\vzeros} & \underset{1\times 1}{\sigma_0^2 \tr\left(\mP_{1n}^{s}\mQ_0\right)}\\
  \underset{1\times k}{\vzeros} & \underset{1\times 1}{\sigma_0^2 \tr\left(\mP_{2n}^{s}\mQ_0\right)}
\end{pmatrix}
\end{equation}
%
where $\mQ_0 = \mM_n\mR_0^{-1}$. 

%-----------------------------------------------------
\subsection{Coding GMM Estimator for SEM}
%-----------------------------------------------------

First, we create the function \code{moments.lee2010} which creates the empirical moments

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{moments.lee2010} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{theta}\hlstd{,} \hlkwc{y}\hlstd{,} \hlkwc{X}\hlstd{,} \hlkwc{M}\hlstd{)\{}
  \hlcom{# Theta: beta and lambda}
  \hlstd{k}        \hlkwb{<-} \hlkwd{ncol}\hlstd{(X)}
  \hlstd{n}        \hlkwb{<-} \hlkwd{nrow}\hlstd{(X)}
  \hlstd{beta}     \hlkwb{<-} \hlstd{theta[}\hlnum{1}\hlopt{:}\hlstd{k]}
  \hlstd{lambda}   \hlkwb{<-} \hlkwd{tail}\hlstd{(theta,} \hlkwc{n} \hlstd{=} \hlnum{1L}\hlstd{)}
  \hlstd{I}        \hlkwb{<-} \hlkwd{diag}\hlstd{(n)}
  \hlstd{R}        \hlkwb{<-} \hlstd{I} \hlopt{-}  \hlstd{lambda} \hlopt{*} \hlstd{M}
  \hlstd{u.n}      \hlkwb{<-} \hlstd{y} \hlopt{-} \hlkwd{crossprod}\hlstd{(}\hlkwd{t}\hlstd{(X), beta)}
  \hlstd{epsi}     \hlkwb{<-} \hlstd{R} \hlopt{%*%} \hlstd{u.n}
  \hlstd{P1}       \hlkwb{<-} \hlstd{M}
  \hlstd{M2}       \hlkwb{<-} \hlkwd{crossprod}\hlstd{(}\hlkwd{t}\hlstd{(M), M)}
  \hlstd{P2}       \hlkwb{<-} \hlstd{M2} \hlopt{-} \hlstd{(}\hlkwd{tr}\hlstd{(M2)} \hlopt{/} \hlstd{n)} \hlopt{*} \hlkwd{diag}\hlstd{(n)}
  \hlstd{g.lin}    \hlkwb{<-} \hlkwd{crossprod}\hlstd{(X, epsi)}            \hlcom{# k * 1}
  \hlstd{g.q1}     \hlkwb{<-} \hlkwd{crossprod}\hlstd{(epsi, P1)} \hlopt{%*%} \hlstd{epsi}  \hlcom{# 1*1}
  \hlstd{g.q2}     \hlkwb{<-} \hlkwd{crossprod}\hlstd{(epsi, P2)} \hlopt{%*%} \hlstd{epsi}  \hlcom{# 1*1}
  \hlstd{g}        \hlkwb{<-} \hlkwd{rbind}\hlstd{(g.lin, g.q1, g.q2)}

  \hlcom{# Gradient }
  \hlstd{P1s} \hlkwb{<-} \hlstd{P1} \hlopt{+} \hlkwd{t}\hlstd{(P1)}
  \hlstd{P2s} \hlkwb{<-} \hlstd{P2} \hlopt{+} \hlkwd{t}\hlstd{(P2)}
  \hlstd{D}   \hlkwb{<-}  \hlopt{-}\hlnum{1} \hlopt{*} \hlkwd{rbind}\hlstd{(}\hlkwd{t}\hlstd{(X),}
                     \hlkwd{crossprod}\hlstd{(epsi, P1s),}
                     \hlkwd{crossprod}\hlstd{(epsi, P2s)}
                     \hlstd{)} \hlopt{%*%} \hlkwd{cbind}\hlstd{(R} \hlopt{%*%} \hlstd{X, M} \hlopt{%*%} \hlstd{u.n)}

  \hlcom{# Return results (note that they are divided by n)}
  \hlstd{out} \hlkwb{<-} \hlkwd{list}\hlstd{(}\hlkwc{g} \hlstd{= g} \hlopt{/}\hlstd{n ,} \hlkwc{D} \hlstd{= D} \hlopt{/} \hlstd{n)}
  \hlkwd{return}\hlstd{(out)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

The objective function to be minimized is the following:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{Q.sem} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{start}\hlstd{,} \hlkwc{y}\hlstd{,} \hlkwc{X}\hlstd{,} \hlkwc{M}\hlstd{,} \hlkwc{Psi}\hlstd{,} \hlkwc{gradient}\hlstd{)\{}
  \hlstd{g.hat} \hlkwb{<-} \hlkwd{moments.lee2010}\hlstd{(}\hlkwc{theta} \hlstd{= start,} \hlkwc{y} \hlstd{= y,} \hlkwc{X} \hlstd{= X,} \hlkwc{M} \hlstd{= M)}
  \hlstd{Q} \hlkwb{<-} \hlopt{-}\hlnum{1} \hlopt{*} \hlkwd{crossprod}\hlstd{(g.hat}\hlopt{$}\hlstd{g, Psi)} \hlopt{%*%} \hlstd{g.hat}\hlopt{$}\hlstd{g}
  \hlkwa{if} \hlstd{(gradient)\{}
    \hlstd{D} \hlkwb{<-} \hlstd{g.hat}\hlopt{$}\hlstd{D}
    \hlstd{Gr} \hlkwb{<-} \hlopt{-}\hlnum{2} \hlopt{*} \hlkwd{crossprod}\hlstd{(D, Psi)} \hlopt{%*%} \hlstd{g.hat}\hlopt{$}\hlstd{g}
    \hlkwd{attr}\hlstd{(Q,} \hlstr{"gradient"}\hlstd{)} \hlkwb{<-} \hlkwd{as.vector}\hlstd{(Gr)}
  \hlstd{\}}
  \hlkwd{return}\hlstd{(Q)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

The function that generates the variance-covariance of the moments is the following
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{make.vmom.sem} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{b.hat}\hlstd{,} \hlkwc{y}\hlstd{,} \hlkwc{X}\hlstd{,} \hlkwc{M}\hlstd{)\{}
  \hlstd{k}        \hlkwb{<-} \hlkwd{ncol}\hlstd{(X)}
  \hlstd{n}        \hlkwb{<-} \hlkwd{nrow}\hlstd{(X)}
  \hlstd{beta}     \hlkwb{<-} \hlstd{b.hat[}\hlnum{1}\hlopt{:}\hlstd{k]}
  \hlstd{lambda}   \hlkwb{<-} \hlkwd{tail}\hlstd{(b.hat,} \hlkwc{n} \hlstd{=} \hlnum{1L}\hlstd{)}
  \hlstd{I}        \hlkwb{<-} \hlkwd{diag}\hlstd{(n)}
  \hlstd{R}        \hlkwb{<-} \hlstd{I} \hlopt{-}  \hlstd{lambda} \hlopt{*} \hlstd{M}
  \hlstd{u.n}      \hlkwb{<-} \hlstd{y} \hlopt{-} \hlkwd{crossprod}\hlstd{(}\hlkwd{t}\hlstd{(X), beta)}
  \hlstd{epsi}     \hlkwb{<-} \hlstd{R} \hlopt{%*%} \hlstd{u.n}
  \hlstd{sigma2}   \hlkwb{<-} \hlkwd{as.numeric}\hlstd{(}\hlkwd{crossprod}\hlstd{(epsi)} \hlopt{/} \hlstd{n)}
  \hlstd{P1}       \hlkwb{<-} \hlstd{M}
  \hlstd{M2}       \hlkwb{<-} \hlkwd{crossprod}\hlstd{(}\hlkwd{t}\hlstd{(M), M)}
  \hlstd{P2}       \hlkwb{<-} \hlstd{M2} \hlopt{-} \hlstd{(}\hlkwd{tr}\hlstd{(M2)} \hlopt{/} \hlstd{n)} \hlopt{*} \hlkwd{diag}\hlstd{(n)}
  \hlstd{P1s}      \hlkwb{<-} \hlstd{P1} \hlopt{+} \hlkwd{t}\hlstd{(P1)}
  \hlstd{P2s}      \hlkwb{<-} \hlstd{P2} \hlopt{+} \hlkwd{t}\hlstd{(P2)}

  \hlcom{# Construct V: (2 + k) * (2 + k)}
  \hlstd{V11} \hlkwb{<-} \hlstd{(}\hlnum{1} \hlopt{/} \hlstd{sigma2)} \hlopt{*} \hlkwd{crossprod}\hlstd{(X)}         \hlcom{# k + k}
  \hlstd{Delta} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{0}\hlstd{,} \hlkwc{nrow} \hlstd{=} \hlnum{2}\hlstd{,} \hlkwc{ncol} \hlstd{=} \hlnum{2}\hlstd{)}
  \hlstd{Delta[}\hlnum{1}\hlstd{,} \hlnum{1}\hlstd{]} \hlkwb{<-} \hlkwd{tr}\hlstd{(P1s} \hlopt{%*%} \hlstd{P1)}
  \hlstd{Delta[}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{]} \hlkwb{<-} \hlkwd{tr}\hlstd{(P1s} \hlopt{%*%} \hlstd{P2)}
  \hlstd{Delta[}\hlnum{2}\hlstd{,} \hlnum{1}\hlstd{]} \hlkwb{<-} \hlkwd{tr}\hlstd{(P2s} \hlopt{%*%} \hlstd{P1)}
  \hlstd{Delta[}\hlnum{2}\hlstd{,} \hlnum{2}\hlstd{]} \hlkwb{<-} \hlkwd{tr}\hlstd{(P2s} \hlopt{%*%} \hlstd{P2)}
  \hlstd{V} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{0}\hlstd{,} \hlkwc{nrow} \hlstd{= (k} \hlopt{+} \hlnum{2}\hlstd{),} \hlkwc{ncol} \hlstd{= (k} \hlopt{+} \hlnum{2}\hlstd{))}
  \hlstd{V[}\hlnum{1}\hlopt{:}\hlstd{k,} \hlnum{1}\hlopt{:}\hlstd{k]} \hlkwb{<-} \hlstd{V11}
  \hlstd{V[(k} \hlopt{+} \hlnum{1}\hlstd{)}\hlopt{:}\hlstd{(k} \hlopt{+} \hlnum{2}\hlstd{), (k} \hlopt{+} \hlnum{1}\hlstd{)}\hlopt{:}\hlstd{(k} \hlopt{+} \hlnum{2}\hlstd{)]} \hlkwb{<-} \hlstd{Delta}
  \hlstd{V} \hlkwb{<-} \hlstd{sigma2}\hlopt{^}\hlnum{2} \hlopt{*} \hlstd{V}
  \hlcom{# Construct first part of Omega}
  \hlstd{omega}   \hlkwb{<-} \hlkwd{cbind}\hlstd{(}\hlkwd{diag}\hlstd{(P1),} \hlkwd{diag}\hlstd{(P2))} \hlcom{# n * 2}
  \hlstd{mu4.hat} \hlkwb{<-} \hlkwd{sum}\hlstd{(epsi}\hlopt{^}\hlnum{4}\hlstd{)} \hlopt{/} \hlstd{n}
  \hlstd{mu3.hat} \hlkwb{<-} \hlkwd{sum}\hlstd{(epsi}\hlopt{^}\hlnum{3}\hlstd{)} \hlopt{/} \hlstd{n}
  \hlstd{Vp1} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{0}\hlstd{,} \hlkwc{nrow} \hlstd{= (k} \hlopt{+} \hlnum{2}\hlstd{),} \hlkwc{ncol} \hlstd{= (k} \hlopt{+} \hlnum{2}\hlstd{))}
  \hlstd{Vp1[(k} \hlopt{+} \hlnum{1}\hlstd{)}\hlopt{:}\hlstd{(k} \hlopt{+} \hlnum{2}\hlstd{), (k} \hlopt{+} \hlnum{1}\hlstd{)}\hlopt{:}\hlstd{(k} \hlopt{+} \hlnum{2}\hlstd{)]} \hlkwb{<-} \hlstd{(mu4.hat} \hlopt{-} \hlnum{3} \hlopt{*} \hlstd{sigma2}\hlopt{^}\hlnum{2}\hlstd{)} \hlopt{*} \hlkwd{crossprod}\hlstd{(omega)}   \hlcom{# 2 * 2 }
  \hlstd{Vp1[(k} \hlopt{+} \hlnum{1}\hlstd{)}\hlopt{:}\hlstd{(k} \hlopt{+} \hlnum{2}\hlstd{),} \hlnum{1}\hlopt{:}\hlstd{k]} \hlkwb{<-} \hlstd{mu3.hat} \hlopt{*} \hlkwd{crossprod}\hlstd{(omega, X)}
  \hlstd{Vp1[}\hlnum{1}\hlopt{:}\hlstd{k, (k} \hlopt{+} \hlnum{1}\hlstd{)}\hlopt{:}\hlstd{(k} \hlopt{+} \hlnum{2}\hlstd{)]} \hlkwb{<-} \hlstd{mu3.hat} \hlopt{*} \hlkwd{crossprod}\hlstd{(X, omega)}
  \hlstd{Omega}                 \hlkwb{<-} \hlstd{Vp1} \hlopt{+} \hlstd{V}
  \hlstd{Omega}                 \hlkwb{<-} \hlstd{Omega} \hlopt{/} \hlstd{n}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}


The main function is the following
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{sem.gmm} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{formula}\hlstd{,} \hlkwc{data}\hlstd{,} \hlkwc{M}\hlstd{,}
                    \hlkwc{estimator} \hlstd{=} \hlkwd{c}\hlstd{(}\hlstr{"gmm"}\hlstd{,} \hlstr{"ogmm"}\hlstd{),}
                    \hlkwc{gradient} \hlstd{=} \hlnum{TRUE}\hlstd{)\{}
  \hlcom{# Model Frame}
  \hlstd{callT}    \hlkwb{<-} \hlkwd{match.call}\hlstd{(}\hlkwc{expand.dots} \hlstd{=} \hlnum{TRUE}\hlstd{)}
  \hlstd{mf}       \hlkwb{<-} \hlstd{callT}
  \hlstd{m}        \hlkwb{<-} \hlkwd{match}\hlstd{(}\hlkwd{c}\hlstd{(}\hlstr{"formula"}\hlstd{,} \hlstr{"data"}\hlstd{),} \hlkwd{names}\hlstd{(mf),} \hlnum{0L}\hlstd{)}
  \hlstd{mf}       \hlkwb{<-} \hlstd{mf[}\hlkwd{c}\hlstd{(}\hlnum{1L}\hlstd{, m)]}
  \hlstd{mf[[}\hlnum{1L}\hlstd{]]} \hlkwb{<-} \hlkwd{as.name}\hlstd{(}\hlstr{"model.frame"}\hlstd{)}
  \hlstd{mf}       \hlkwb{<-} \hlkwd{eval}\hlstd{(mf,} \hlkwd{parent.frame}\hlstd{())}

  \hlcom{# Estimator}
  \hlstd{estimator} \hlkwb{<-} \hlkwd{match.arg}\hlstd{(estimator)}

  \hlcom{# Get variables and globals}
  \hlstd{y}  \hlkwb{<-} \hlkwd{model.response}\hlstd{(mf)}
  \hlstd{X}  \hlkwb{<-} \hlkwd{model.matrix}\hlstd{(formula, mf)}
  \hlstd{n}  \hlkwb{<-} \hlkwd{nrow}\hlstd{(X)}
  \hlstd{sn} \hlkwb{<-} \hlkwd{nrow}\hlstd{(M)}
  \hlkwa{if} \hlstd{(n} \hlopt{!=} \hlstd{sn)} \hlkwd{stop}\hlstd{(}\hlstr{"number of spatial units in W is different to the number of data"}\hlstd{)}

  \hlcom{# Starting values for optimization}
  \hlstd{ols.e} \hlkwb{<-} \hlkwd{lm}\hlstd{(y} \hlopt{~} \hlstd{X} \hlopt{-} \hlnum{1}\hlstd{)}
  \hlstd{ols.r} \hlkwb{<-} \hlkwd{residuals}\hlstd{(ols.e)}
  \hlstd{start} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlkwd{coef}\hlstd{(ols.e),} \hlkwd{cor}\hlstd{(M} \hlopt{%*%} \hlstd{ols.r, ols.r))}
  \hlkwd{names}\hlstd{(start)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlkwd{colnames}\hlstd{(X),} \hlstr{"Mu"}\hlstd{)}

  \hlcom{# GMM estimator with weighting matrix using a identity matrix  }
  \hlstd{k}   \hlkwb{<-} \hlkwd{ncol}\hlstd{(X)}
  \hlstd{Psi} \hlkwb{<-} \hlkwd{diag}\hlstd{(k} \hlopt{+} \hlnum{2}\hlstd{)}
  \hlkwd{require}\hlstd{(}\hlstr{"maxLik"}\hlstd{)}
  \hlstd{opt} \hlkwb{<-} \hlkwd{maxLik}\hlstd{(}\hlkwc{logLik} \hlstd{= Q.sem,}
                \hlkwc{start}  \hlstd{= start,}
                \hlkwc{method} \hlstd{=} \hlstr{"bfgs"}\hlstd{,}
                \hlkwc{y}      \hlstd{= y,}
                \hlkwc{X}      \hlstd{= X,}
                \hlkwc{M}      \hlstd{= M,}
                \hlkwc{Psi}    \hlstd{= Psi,}
                \hlkwc{gradient} \hlstd{= gradient,}
                \hlkwc{print.level} \hlstd{=} \hlnum{3}\hlstd{,}
                \hlkwc{finalHessian} \hlstd{=} \hlnum{FALSE}\hlstd{)}

  \hlcom{# OGMM: GMM estimator with weighting matrix using the inverse of the var-cov of moment functions}
  \hlkwa{if} \hlstd{(estimator} \hlopt{==} \hlstr{"ogmm"}\hlstd{)\{}
    \hlstd{Omega.hat} \hlkwb{<-} \hlkwd{make.vmom.sem}\hlstd{(}\hlkwd{coef}\hlstd{(opt),} \hlkwc{y} \hlstd{= y,} \hlkwc{X} \hlstd{= X,} \hlkwc{M} \hlstd{= M)}
    \hlstd{Psi}       \hlkwb{<-} \hlkwd{solve}\hlstd{(Omega.hat)}
    \hlstd{opt}       \hlkwb{<-} \hlkwd{maxLik}\hlstd{(}\hlkwc{logLik} \hlstd{= Q.sem,}
                        \hlkwc{start} \hlstd{=} \hlkwd{coef}\hlstd{(opt),}
                        \hlkwc{method} \hlstd{=} \hlstr{"bfgs"}\hlstd{,}
                        \hlkwc{y} \hlstd{= y,}
                        \hlkwc{X} \hlstd{= X,}
                        \hlkwc{M} \hlstd{= M,}
                        \hlkwc{Psi} \hlstd{= Psi,}
                        \hlkwc{gradient} \hlstd{= gradient,}
                        \hlkwc{print.level} \hlstd{=} \hlnum{3}\hlstd{,}
                        \hlkwc{finalHessian} \hlstd{=} \hlnum{FALSE}\hlstd{)}
  \hlstd{\}}

  \hlstd{results} \hlkwb{<-} \hlkwd{structure}\hlstd{(}
    \hlkwd{list}\hlstd{(}
      \hlkwc{coefficients} \hlstd{=} \hlkwd{coef}\hlstd{(opt),}
      \hlkwc{call}         \hlstd{= callT,}
      \hlkwc{X}            \hlstd{= X,}
      \hlkwc{y}            \hlstd{= y,}
      \hlkwc{Psi}          \hlstd{= Psi,}
      \hlkwc{M}            \hlstd{= M,}
      \hlkwc{estimator}    \hlstd{= estimator}
    \hlstd{),}
    \hlkwc{class} \hlstd{=} \hlstr{"gmm.sem"}
  \hlstd{)}
  \hlkwd{return}\hlstd{(results)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

The function that creates $\mD$ is the following
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{make.D.sem} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{b.hat}\hlstd{,} \hlkwc{y}\hlstd{,} \hlkwc{X}\hlstd{,} \hlkwc{M}\hlstd{)\{}
  \hlstd{k}        \hlkwb{<-} \hlkwd{ncol}\hlstd{(X)}
  \hlstd{n}        \hlkwb{<-} \hlkwd{nrow}\hlstd{(X)}
  \hlstd{beta}     \hlkwb{<-} \hlstd{b.hat[}\hlnum{1}\hlopt{:}\hlstd{k]}
  \hlstd{lambda}   \hlkwb{<-} \hlkwd{tail}\hlstd{(b.hat,} \hlkwc{n} \hlstd{=} \hlnum{1L}\hlstd{)}
  \hlstd{I}        \hlkwb{<-} \hlkwd{diag}\hlstd{(n)}
  \hlstd{R}        \hlkwb{<-} \hlstd{I} \hlopt{-}  \hlstd{lambda} \hlopt{*} \hlstd{M}
  \hlstd{u.n}      \hlkwb{<-} \hlstd{y} \hlopt{-} \hlkwd{crossprod}\hlstd{(}\hlkwd{t}\hlstd{(X), beta)}
  \hlstd{epsi}     \hlkwb{<-} \hlstd{R} \hlopt{%*%} \hlstd{u.n}
  \hlstd{sigma2}   \hlkwb{<-} \hlkwd{as.numeric}\hlstd{(}\hlkwd{crossprod}\hlstd{(epsi)} \hlopt{/} \hlstd{n)}
  \hlstd{P1}       \hlkwb{<-} \hlstd{M}
  \hlstd{M2}       \hlkwb{<-} \hlkwd{crossprod}\hlstd{(}\hlkwd{t}\hlstd{(M), M)}
  \hlstd{P2}       \hlkwb{<-} \hlstd{M2} \hlopt{-} \hlstd{(}\hlkwd{tr}\hlstd{(M2)} \hlopt{/} \hlstd{n)} \hlopt{*} \hlkwd{diag}\hlstd{(n)}
  \hlstd{P1s}      \hlkwb{<-} \hlstd{P1} \hlopt{+} \hlkwd{t}\hlstd{(P1)}
  \hlstd{P2s}      \hlkwb{<-} \hlstd{P2} \hlopt{+} \hlkwd{t}\hlstd{(P2)}
  \hlstd{Q}        \hlkwb{<-} \hlstd{M} \hlopt{%*%} \hlkwd{solve}\hlstd{(R)}

  \hlcom{# Generate D}
  \hlstd{D} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{0}\hlstd{,} \hlkwc{nrow} \hlstd{= (k} \hlopt{+} \hlnum{2}\hlstd{) ,} \hlkwc{ncol} \hlstd{= k} \hlopt{+} \hlnum{1}\hlstd{)}
  \hlkwd{rownames}\hlstd{(D)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlkwd{colnames}\hlstd{(X),} \hlstr{"q1"}\hlstd{,} \hlstr{"q2"}\hlstd{)}
  \hlkwd{colnames}\hlstd{(D)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlkwd{colnames}\hlstd{(X),} \hlstr{"Mu"}\hlstd{)}
  \hlstd{D[k} \hlopt{+} \hlnum{1}\hlstd{, k} \hlopt{+} \hlnum{1}\hlstd{]} \hlkwb{<-} \hlstd{sigma2} \hlopt{*} \hlkwd{tr}\hlstd{(P1s} \hlopt{%*%} \hlstd{Q)}
  \hlstd{D[k} \hlopt{+} \hlnum{2}\hlstd{, k} \hlopt{+} \hlnum{1}\hlstd{]} \hlkwb{<-} \hlstd{sigma2} \hlopt{*} \hlkwd{tr}\hlstd{(P2s} \hlopt{%*%} \hlstd{Q)}
  \hlstd{D[}\hlnum{1}\hlopt{:}\hlstd{k,} \hlnum{1}\hlopt{:}\hlstd{k]}     \hlkwb{<-} \hlkwd{t}\hlstd{(X)} \hlopt{%*%} \hlstd{R} \hlopt{%*%} \hlstd{X}
  \hlkwd{return}\hlstd{(D)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}


The function to construct the variance-covariance matrix is given by
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{vcov.gmm.sem} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{object}\hlstd{,} \hlkwc{D} \hlstd{=} \hlkwd{c}\hlstd{(}\hlstr{"population"}\hlstd{,} \hlstr{"gradient"}\hlstd{),} \hlkwc{...}\hlstd{)\{}
  \hlstd{estimator} \hlkwb{<-} \hlstd{object}\hlopt{$}\hlstd{estimator}
  \hlstd{D.type} \hlkwb{<-} \hlkwd{match.arg}\hlstd{(D)}
  \hlstd{X}      \hlkwb{<-} \hlstd{object}\hlopt{$}\hlstd{X}
  \hlstd{y}      \hlkwb{<-} \hlstd{object}\hlopt{$}\hlstd{y}
  \hlstd{M}      \hlkwb{<-} \hlstd{object}\hlopt{$}\hlstd{M}
  \hlstd{k}      \hlkwb{<-} \hlkwd{ncol}\hlstd{(X)}
  \hlstd{n}      \hlkwb{<-} \hlkwd{nrow}\hlstd{(X)}
  \hlstd{b.hat}  \hlkwb{<-} \hlstd{object}\hlopt{$}\hlstd{coefficients}

  \hlkwa{if} \hlstd{(estimator} \hlopt{==} \hlstr{"gmm"}\hlstd{)\{}
    \hlkwa{if} \hlstd{(D.type} \hlopt{==} \hlstr{"population"}\hlstd{)\{}
      \hlstd{D} \hlkwb{<-} \hlkwd{make.D.sem}\hlstd{(}\hlkwc{b.hat} \hlstd{= b.hat,} \hlkwc{y} \hlstd{= y,} \hlkwc{X} \hlstd{= X,} \hlkwc{M} \hlstd{= M)}
      \hlstd{D} \hlkwb{<-} \hlstd{D} \hlopt{/} \hlstd{n}
    \hlstd{\}}
    \hlkwa{if} \hlstd{(D.type} \hlopt{==} \hlstr{"gradient"}\hlstd{)\{}
      \hlstd{D} \hlkwb{<-} \hlkwd{moments.lee2010}\hlstd{(b.hat,} \hlkwc{y} \hlstd{= y,} \hlkwc{X} \hlstd{= X,} \hlkwc{M} \hlstd{= M)}\hlopt{$}\hlstd{D}
    \hlstd{\}}
    \hlstd{Omega} \hlkwb{<-} \hlkwd{make.vmom.sem}\hlstd{(}\hlkwc{b.hat} \hlstd{= b.hat,} \hlkwc{y} \hlstd{= y,} \hlkwc{X} \hlstd{= X,} \hlkwc{M} \hlstd{= M)}
    \hlstd{var}   \hlkwb{<-} \hlkwd{solve}\hlstd{(}\hlkwd{crossprod}\hlstd{(D))} \hlopt{%*%} \hlkwd{t}\hlstd{(D)} \hlopt{%*%} \hlstd{Omega} \hlopt{%*%} \hlstd{D} \hlopt{%*%} \hlkwd{solve}\hlstd{(}\hlkwd{crossprod}\hlstd{(D))} \hlopt{/} \hlstd{n}
  \hlstd{\}}
  \hlkwa{if} \hlstd{(estimator} \hlopt{==} \hlstr{"ogmm"}\hlstd{)\{}
    \hlkwa{if} \hlstd{(D.type} \hlopt{==} \hlstr{"population"}\hlstd{)\{}
      \hlstd{D} \hlkwb{<-} \hlkwd{make.D.sem}\hlstd{(}\hlkwc{b.hat} \hlstd{= b.hat,} \hlkwc{y} \hlstd{= y,} \hlkwc{X} \hlstd{= X,} \hlkwc{M} \hlstd{= M)}
      \hlstd{D} \hlkwb{<-} \hlstd{D} \hlopt{/} \hlstd{n}
    \hlstd{\}}
    \hlkwa{if} \hlstd{(D.type} \hlopt{==} \hlstr{"gradient"}\hlstd{)\{}
      \hlstd{D} \hlkwb{<-} \hlkwd{moments.lee2010}\hlstd{(b.hat,} \hlkwc{y} \hlstd{= y,} \hlkwc{X} \hlstd{= X,} \hlkwc{M} \hlstd{= M)}\hlopt{$}\hlstd{D}
    \hlstd{\}}
    \hlstd{Psi} \hlkwb{<-} \hlstd{object}\hlopt{$}\hlstd{Psi}
    \hlstd{var} \hlkwb{<-} \hlkwd{solve}\hlstd{(}\hlkwd{t}\hlstd{(D)} \hlopt{%*%} \hlstd{Psi} \hlopt{%*%} \hlstd{D)} \hlopt{/} \hlstd{n}
  \hlstd{\}}
  \hlkwd{return}\hlstd{(var)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

Summary S3 functions
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{summary.gmm.sem} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{object}\hlstd{,}
                            \hlkwc{D} \hlstd{=} \hlkwd{c}\hlstd{(}\hlstr{"population"}\hlstd{,} \hlstr{"gradient"}\hlstd{),}
                             \hlkwc{table} \hlstd{=} \hlnum{TRUE}\hlstd{,}
                             \hlkwc{digits} \hlstd{=} \hlkwd{max}\hlstd{(}\hlnum{3}\hlstd{, .Options}\hlopt{$}\hlstd{digits} \hlopt{-} \hlnum{3}\hlstd{),}
                             \hlkwc{...}\hlstd{)\{}
  \hlstd{D.type} \hlkwb{<-} \hlkwd{match.arg}\hlstd{(D)}
  \hlstd{n}       \hlkwb{<-} \hlkwd{nrow}\hlstd{(object}\hlopt{$}\hlstd{X)}
  \hlstd{df}      \hlkwb{<-} \hlstd{n} \hlopt{-} \hlkwd{length}\hlstd{(object}\hlopt{$}\hlstd{coefficients)}
  \hlstd{b}       \hlkwb{<-} \hlstd{object}\hlopt{$}\hlstd{coefficients}
  \hlstd{std.err} \hlkwb{<-} \hlkwd{sqrt}\hlstd{(}\hlkwd{diag}\hlstd{(}\hlkwd{vcov}\hlstd{(object,} \hlkwc{D} \hlstd{= D.type)))}
  \hlstd{z}       \hlkwb{<-} \hlstd{b} \hlopt{/} \hlstd{std.err}
  \hlstd{p}       \hlkwb{<-} \hlnum{2} \hlopt{*} \hlkwd{pt}\hlstd{(}\hlopt{-}\hlkwd{abs}\hlstd{(z),} \hlkwc{df} \hlstd{= df)}
  \hlstd{CoefTable} \hlkwb{<-} \hlkwd{cbind}\hlstd{(b, std.err, z, p)}
  \hlkwd{colnames}\hlstd{(CoefTable)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"Estimate"}\hlstd{,} \hlstr{"Std.Error"}\hlstd{,} \hlstr{"t-value"}\hlstd{,} \hlstr{"Pr(>|t|)"}\hlstd{)}
  \hlstd{result} \hlkwb{<-} \hlkwd{structure}\hlstd{(}
    \hlkwd{list}\hlstd{(}
      \hlkwc{CoefTable} \hlstd{= CoefTable,}
      \hlkwc{digits}    \hlstd{= digits,}
      \hlkwc{call}      \hlstd{= object}\hlopt{$}\hlstd{call),}
    \hlkwc{class} \hlstd{=} \hlstr{'summary.gmm.sem'}
  \hlstd{)}
  \hlkwd{return}\hlstd{(result)}
\hlstd{\}}

\hlstd{print.summary.gmm.sem} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{,}
                                   \hlkwc{digits} \hlstd{= x}\hlopt{$}\hlstd{digits,}
                                   \hlkwc{na.print} \hlstd{=} \hlstr{""}\hlstd{,}
                                   \hlkwc{symbolic.cor} \hlstd{= p} \hlopt{>} \hlnum{4}\hlstd{,}
                                   \hlkwc{signif.stars} \hlstd{=} \hlkwd{getOption}\hlstd{(}\hlstr{"show.signif.stars"}\hlstd{),}
                                   \hlkwc{...}\hlstd{)}
\hlstd{\{}
  \hlkwd{cat}\hlstd{(}\hlstr{"\textbackslash{}nCall:\textbackslash{}n"}\hlstd{)}
  \hlkwd{cat}\hlstd{(}\hlkwd{paste}\hlstd{(}\hlkwd{deparse}\hlstd{(x}\hlopt{$}\hlstd{call),} \hlkwc{sep} \hlstd{=} \hlstr{"\textbackslash{}n"}\hlstd{,} \hlkwc{collapse} \hlstd{=} \hlstr{"\textbackslash{}n"}\hlstd{),} \hlstr{"\textbackslash{}n\textbackslash{}n"}\hlstd{,} \hlkwc{sep} \hlstd{=} \hlstr{""}\hlstd{)}

  \hlkwd{cat}\hlstd{(}\hlstr{"\textbackslash{}nCoefficients:\textbackslash{}n"}\hlstd{)}
  \hlkwd{printCoefmat}\hlstd{(x}\hlopt{$}\hlstd{CoefTable,} \hlkwc{digit} \hlstd{= digits,} \hlkwc{P.value} \hlstd{=} \hlnum{TRUE}\hlstd{,} \hlkwc{has.Pvalue} \hlstd{=} \hlnum{TRUE}\hlstd{)}
  \hlkwd{invisible}\hlstd{(}\hlkwa{NULL}\hlstd{)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

Now, we check our function
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{semgmm} \hlkwb{<-} \hlkwd{sem.gmm}\hlstd{(y} \hlopt{~} \hlstd{x1} \hlopt{+} \hlstd{x2} \hlopt{+} \hlstd{x3,} \hlkwc{data} \hlstd{= data,}
                  \hlkwc{M} \hlstd{= M,} \hlkwc{estimator} \hlstd{=} \hlstr{"gmm"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Initial function value: -0.002361787 
## Initial gradient value:
##   (Intercept)            x1            x2            x3            Mu 
##  0.0003839962 -0.0511733218 -0.0289716000 -0.0427860038  0.0226625302 
## initial  value 0.002362 
## iter   2 value 0.001449
## iter   3 value 0.001204
## iter   4 value 0.001070
## iter   5 value 0.001006
## iter   6 value 0.000970
## iter   7 value 0.000964
## iter   8 value 0.000963
## iter   9 value 0.000963
## iter  10 value 0.000963
## iter  11 value 0.000963
## iter  12 value 0.000963
## iter  12 value 0.000963
## final  value 0.000963 
## converged
\end{verbatim}
\begin{alltt}
\hlkwd{summary}\hlstd{(semgmm,} \hlkwc{D} \hlstd{=} \hlstr{"population"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## sem.gmm(formula = y ~ x1 + x2 + x3, data = data, M = M, estimator = "gmm")
## 
## 
## Coefficients:
##             Estimate Std.Error t-value Pr(>|t|)    
## (Intercept) -0.19250   0.15049  -1.279    0.201    
## x1          -1.06289   0.06607 -16.087   <2e-16 ***
## x2           0.01368   0.06197   0.221    0.825    
## x3           0.98552   0.06651  14.818   <2e-16 ***
## Mu           0.55800   0.04690  11.898   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}
\begin{alltt}
\hlkwd{summary}\hlstd{(semgmm,} \hlkwc{D} \hlstd{=} \hlstr{"gradient"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## sem.gmm(formula = y ~ x1 + x2 + x3, data = data, M = M, estimator = "gmm")
## 
## 
## Coefficients:
##             Estimate Std.Error t-value Pr(>|t|)    
## (Intercept) -0.19250   0.15049  -1.279    0.201    
## x1          -1.06289   0.06622 -16.051   <2e-16 ***
## x2           0.01368   0.06186   0.221    0.825    
## x3           0.98552   0.06657  14.804   <2e-16 ***
## Mu           0.55800   0.04923  11.334   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}
\begin{alltt}
\hlstd{semOgmm} \hlkwb{<-} \hlkwd{sem.gmm}\hlstd{(y} \hlopt{~} \hlstd{x1} \hlopt{+} \hlstd{x2} \hlopt{+} \hlstd{x3,} \hlkwc{data} \hlstd{= data,}
                  \hlkwc{M} \hlstd{= M,} \hlkwc{estimator} \hlstd{=} \hlstr{"ogmm"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Initial function value: -0.002361787 
## Initial gradient value:
##   (Intercept)            x1            x2            x3            Mu 
##  0.0003839962 -0.0511733218 -0.0289716000 -0.0427860038  0.0226625302 
## initial  value 0.002362 
## iter   2 value 0.001449
## iter   3 value 0.001204
## iter   4 value 0.001070
## iter   5 value 0.001006
## iter   6 value 0.000970
## iter   7 value 0.000964
## iter   8 value 0.000963
## iter   9 value 0.000963
## iter  10 value 0.000963
## iter  11 value 0.000963
## iter  12 value 0.000963
## iter  12 value 0.000963
## final  value 0.000963 
## converged
## Initial function value: -0.001029206 
## Initial gradient value:
##   (Intercept)            x1            x2            x3            Mu 
## -6.818661e-07 -1.071751e-04 -2.275764e-03  5.728118e-04  6.167284e-03 
## initial  value 0.001029 
## iter   2 value 0.001019
## iter   3 value 0.001014
## iter   4 value 0.001014
## iter   5 value 0.001014
## iter   6 value 0.001014
## iter   7 value 0.001014
## iter   7 value 0.001014
## iter   7 value 0.001014
## final  value 0.001014 
## converged
\end{verbatim}
\begin{alltt}
\hlkwd{summary}\hlstd{(semOgmm,} \hlkwc{D} \hlstd{=} \hlstr{"population"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## sem.gmm(formula = y ~ x1 + x2 + x3, data = data, M = M, estimator = "ogmm")
## 
## 
## Coefficients:
##             Estimate Std.Error t-value Pr(>|t|)    
## (Intercept) -0.19255   0.15183  -1.268    0.205    
## x1          -1.06348   0.06607 -16.096   <2e-16 ***
## x2           0.01109   0.06196   0.179    0.858    
## x3           0.98598   0.06652  14.823   <2e-16 ***
## Mu           0.56190   0.04561  12.319   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}
\begin{alltt}
\hlkwd{summary}\hlstd{(semOgmm,} \hlkwc{D} \hlstd{=} \hlstr{"gradient"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## sem.gmm(formula = y ~ x1 + x2 + x3, data = data, M = M, estimator = "ogmm")
## 
## 
## Coefficients:
##             Estimate Std.Error t-value Pr(>|t|)    
## (Intercept) -0.19255   0.15183  -1.268    0.205    
## x1          -1.06348   0.06621 -16.061   <2e-16 ***
## x2           0.01109   0.06175   0.180    0.857    
## x3           0.98598   0.06658  14.810   <2e-16 ***
## Mu           0.56190   0.04909  11.445   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}
\begin{alltt}
\hlstd{sem_mm}    \hlkwb{<-} \hlkwd{GMerrorsar}\hlstd{(y} \hlopt{~} \hlstd{x1} \hlopt{+} \hlstd{x2} \hlopt{+} \hlstd{x3,}
                        \hlkwc{data} \hlstd{= data,}
                        \hlkwc{listw} \hlstd{=} \hlkwd{mat2listw}\hlstd{(M,} \hlkwc{style} \hlstd{=} \hlstr{"W"}\hlstd{),}
                        \hlkwc{verbose} \hlstd{=} \hlnum{FALSE}\hlstd{,}
                        \hlkwc{se.lambda} \hlstd{=} \hlnum{TRUE}\hlstd{)}
\hlkwd{summary}\hlstd{(sem_mm)}
\end{alltt}
\begin{verbatim}
## 
## Call:GMerrorsar(formula = y ~ x1 + x2 + x3, data = data, listw = mat2listw(M, 
##     style = "W"), verbose = FALSE, se.lambda = TRUE)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -4.461416 -1.275682 -0.023397  1.244488  4.935045 
## 
## Type: GM SAR estimator
## Coefficients: (GM standard errors) 
##              Estimate Std. Error  z value Pr(>|z|)
## (Intercept) -0.192429   0.149968  -1.2831   0.1994
## x1          -1.079338   0.063584 -16.9751   <2e-16
## x2           0.006391   0.060099   0.1063   0.9153
## x3           0.975246   0.064057  15.2246   <2e-16
## 
## Lambda: 0.55602 (standard error): 0.10114 (z-value): 5.4977
## Residual variance (sigma squared): 2.345, (sigma: 1.5313)
## GM argmin sigma squared: 2.3535
## Number of observations: 529 
## Number of parameters estimated: 6
\end{verbatim}
\begin{alltt}
\hlkwd{library}\hlstd{(}\hlstr{"sphet"}\hlstd{)}
\hlstd{sem.spreg}    \hlkwb{<-} \hlkwd{spreg}\hlstd{(y} \hlopt{~} \hlstd{x1} \hlopt{+} \hlstd{x2} \hlopt{+} \hlstd{x3,}
                        \hlkwc{data} \hlstd{= data,}
                        \hlkwc{listw} \hlstd{=} \hlkwd{mat2listw}\hlstd{(M,} \hlkwc{style} \hlstd{=} \hlstr{"W"}\hlstd{),}
                        \hlkwc{model} \hlstd{=} \hlstr{"error"}\hlstd{)}
\hlkwd{summary}\hlstd{(sem.spreg)}
\end{alltt}
\begin{verbatim}
## ======================================================
## ======================================================
##                GMM Spatial Error Model
## ======================================================
## ======================================================
## 
## Call:
## spreg(formula = y ~ x1 + x2 + x3, data = data, listw = mat2listw(M, 
##     style = "W"), model = "error")
## 
## Residuals:
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -4.4353 -1.2726 -0.0170 -0.0007  1.2293  4.8994 
## 
## Coefficients:
##              Estimate Std. Error  t-value Pr(>|t|)    
## (Intercept) -0.192284   0.151838  -1.2664   0.2054    
## x1          -1.062591   0.066023 -16.0942   <2e-16 ***
## x2           0.015213   0.061911   0.2457   0.8059    
## x3           0.985071   0.066467  14.8204   <2e-16 ***
## rho          0.562252   0.048895  11.4991   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}
\end{kframe}
\end{knitrout}


%*************************************************
\section{Estimation of SAC Model: The Feasible Generalized Two Stage Least Squares estimator Procedure}\label{sec:sfg2sls-sac}\index{SAC model!FGS2SLS}
%*************************************************

%-------------------------------------------------
\subsection{Intuition Behind the Procedure}
%-------------------------------------------------

Consider the following SAC model:
\begin{equation}\label{eq:model_gmm_sac}
\begin{aligned}
	\vy_n & = \mX_n\vbeta_0 + \rho_0\mW_n\vy_n + \vu_n   \\
	     & = \mZ_n\vdelta_0 + \vu_n,\\
	\vu_n  & = \lambda_0 \mM_n \vu_n + \vepsi_n,
\end{aligned}
\end{equation}
%
where $\mZ_n = \left[\mX_n, \mW_n\vy_n\right]$, $\vdelta_0 = \left[\vbeta_0^\top ,\rho_0\right]^\top$, $\vy_n$ is the $n\times 1$ vector of observations of the dependent variables, $\mX_n$ is the $n \times k$ matrix of observations on \textbf{nonstochastic (exogenous)} regressors, $\mW_n$ and $\mM_n$ are the $n \times n$ non stochastic weights matrices, $\vu_n$ is the $n \times 1$ vector of regression disturbances, $\vepsi_n$ is an $n \times 1$ vector of innovations. 

It is worth noting that, while the model allows for different spatial weight matrices for each process, in practice, there is rarely a strong justification for assuming different structures.

\begin{remark}
This model is generally referred to as the Spatial-ARAR(1, 1) model to emphasize its autoregressive structure both in the dependent variable and the error term. 
\end{remark}

The SAC model can be estimated using ML procedure \citep[see][]{anselin1988spatial}. However, ML estimation requires computing the inverses of $\left(\mI_n - \rho\mW_n\right)$ and $\left(\mI_n - \lambda \mM_n\right)$, which is computationally expensive for large samples. Additionally, ML estimation relies on the assumption of normally distributed errors.

To address these challenges, we rely on estimation techniques from the the S2SLS and GMM. To see how this works, consider applying a spatial Cochrane-Orcutt transformation to the SAC model: 
\begin{equation}\label{eq:sac_cotrans}
	\begin{aligned}
	\vy_n & = \mZ_n\vdelta_0 + \left(\mI_n - \lambda_0\mM_n\right)^{-1}\vepsi_n, \\
	\left(\mI_n - \lambda_0 \mM_n\right)\vy_n & = \left(\mI_n - \lambda_0 \mM_n\right) \mZ_n\vdelta_0 + \vepsi_n, \\
	\vy_s(\lambda_0) & = \mZ_s(\lambda_0)\vdelta_0 + \vepsi_n, 
	\end{aligned}
\end{equation}
%
where the spatially filtered variables are given by:
\begin{eqnarray*}
\vy_s(\lambda_0) & = & \vy_n - \lambda_0 \mM_n\vy_n \\
      & = & \vy_n - \lambda_0 \vy_L, \\
      & = & \left(\mI_n - \lambda_0 \mM_n\right)\vy_n, \\
\mZ_s(\lambda_0) & = & \mZ_n - \lambda_0 \mM_n \mZ_n, \\
      & = & \mZ_n - \lambda_0 \mZ_L, \\
      & = & \left(\mI_n - \lambda_0 \mM_n\right) \mZ_n. \\
\end{eqnarray*}

If we knew $\lambda_0$, we would be able to apply an \textbf{IV approach on the transformed model} \eqref{eq:sac_cotrans}. For the discussion below, assume that we know $\lambda_0$. Note that the ideal instruments in this case will be:
\begin{equation*}
\E\left(\mZ_n(\lambda_0)\right) = \E(\mZ_n) -\lambda_0\E(\mM_n\mZ_n)
\end{equation*}
%
where
\begin{equation*}
  \begin{aligned}
    \E\left(\mZ_n\right)    & = \E\left[\mX_n, \mW_n\E\left(\vy_n\right)\right], \\
    \E\left(\mM_n\mZ_n\right) & = \E\left[\mM_n\mX_n, \mM_n\mW_n\E\left(\vy_n\right)\right],
  \end{aligned}
\end{equation*}

Given that all the columns of $\E(\mZ_n)$ and $\E(\mM_n\mZ_n)$ are linear in
\begin{equation}\label{eq:linear_poten_H}
  \mX_n, \mW_n\mX_n, \mW^2_n\mX_n, \ldots, \mM_n\mX_n, \mM_n\mW_n\mX_n, \mM_n\mW^2_n\mX_n, \ldots
\end{equation}
%
the matrix of instruments $\mH_n$ is a subset of the linearly independent columns in \eqref{eq:linear_poten_H}, for example
\begin{equation*}
  \mH_n = \left[\mX_n, \mW_n\mX_n, \ldots, \mW^l_n\mX_n, \mM_n\mX_n, \mM_n\mW_n\mX_n, \ldots,\mM_n\mW^l_n\mX_n\right]_{LI},
\end{equation*}
%
where typically, $l\leq 2$. 

Since we have the instruments $\mH_n$, and we assuming that we obtain a consistent estimate of $\lambda_0$, we might apply a GMM-type procedure using the following moment conditions for the transformed model \eqref{eq:sac_cotrans}:
\begin{equation*}
  \vm(\lambda_0, \vdelta_0) = \E\left[\frac{1}{\sqrt{n}}\mH_n^\top\vepsi_n\right] = \vzeros.
\end{equation*}

Now let $\widetilde{\lambda}_n$ some consistent estimator for $\lambda_0$ which can be obtained in a previous step, then the sample moment vector is:
\begin{equation*}
  \vm^{\delta}(\widetilde{\lambda}_n, \vdelta_n) = \frac{1}{\sqrt{n}}\mH^\top_n\underbrace{\left[\vy_s(\widetilde{\lambda}_n) - \mZ_s(\widetilde{\lambda}_n)\vdelta_n\right]}_{\widetilde{\vepsi}_n},
\end{equation*}
%
where we explicitly state that the moments depends on $\vdelta_n$---which will be estimated---and a consistent estimate of $\lambda_0$. Under \textbf{homoskedasticity} the variance-covariance matrix of the moment vector $\vm(\lambda_0, \delta_0)$ is given by:
\begin{equation*}
  \var\left[\vm(\lambda_0, \vdelta_0)\right] = \E\left[\vm(\lambda_0, \delta_0)\vm(\lambda_0, \delta_0)^\top\right] = \sigma^2_0n^{-1}\mH^\top_n\mH_n,
\end{equation*}
%
which motivates the following two-step GMM estimator for $\vdelta_0$:
\begin{equation*}
  \widehat{\vdelta}_n = \underset{\vdelta_n}{\argmin}\quad \vm^{\delta}_n(\widetilde{\lambda}_n, \vdelta_m)^\top \mUpsilon^{\delta\delta}_n\vm^{\delta}_n(\widetilde{\lambda}_m, \vdelta_n)
\end{equation*}
%
with
\begin{equation*}
\mUpsilon^{\delta\delta}_n = \left[\frac{1}{n}\mH^\top_n\mH_n\right]^{-1}.
\end{equation*}

Note that:
\begin{equation*}
  \begin{aligned}
 Q_n & = \left[\frac{1}{\sqrt{n}}\mH^\top_n\left[\vy_s(\widetilde{\lambda}_n) - \mZ_s(\widetilde{\lambda}_n)\vdelta_n\right]\right]^\top \left[\frac{1}{n}\mH^\top_n\mH_n\right]^{-1}\left[\frac{1}{\sqrt{n}}\mH^\top_n\left[\vy_s(\widetilde{\lambda}_n) - \mZ_s(\widetilde{\lambda}_n)\vdelta_n\right]\right], \\
     & = \frac{1}{n}\left[\vy_s(\widetilde{\lambda}_n) - \mZ_s(\widetilde{\lambda}_n)\vdelta_n\right]^\top \mH_n\left[\frac{1}{n}\mH_n^\top\mH_n\right]^{-1}\mH_n^\top \left[\vy_s(\widetilde{\lambda}_n) - \mZ_s(\widetilde{\lambda}_n)\vdelta_n\right], \\
     & = \left[\vy_s(\widetilde{\lambda}_n) - \mZ_s(\widetilde{\lambda}_n)\vdelta_n\right]^\top \mH_n\left[\mH_n^\top\mH_n\right]^{-1}\mH_n^\top \left[\vy_s(\widetilde{\lambda}_n) - \mZ_s(\widetilde{\lambda}_n)\vdelta_n\right], \\
     & = \left[\vy_s(\widetilde{\lambda}_n) - \mZ_s(\widetilde{\lambda}_n)\vdelta_n\right]^\top \mP_{H,n} \left[\vy_s(\widetilde{\lambda}_n) - \mZ_s(\widetilde{\lambda}_n)\vdelta_n\right]
\end{aligned}
\end{equation*}

Then, the estimator of $\vdelta_0$ will be:
\begin{equation*}
  \widehat{\vdelta}_n = \left[\widehat{\mZ_s}^\top\mZ_s\right]^{-1}\widehat{\mZ_s}^\top\vy_s
\end{equation*}
%
where $\widehat{\mZ_s} = \mH_n\left(\mH_n^\top\mH_n\right)^{-1}\mH_n\mZ_s$. This estimator has been called the feasible generalized spatial two-stage least squares (FGS2SLS) estimator \citep{kelejian1998generalized}. However, this estimator is not fully efficient. 

The key question remains: How can we obtain a consistent estimator of $\lambda_0$? As one may anticipate, this consistent estimator is obtained in a preliminary step using the generalized moments (GM) approach.

%-------------------------------------------
\subsection{Moment Conditions Revised}
%--------------------------------------------

Since we require a consistent estimate of $\lambda_0$, this section focuses on alternative formulations of the moment conditions under both homoskedasticity \citep{kelejian1999generalized} and heteroskedasticity \citep{kelejian2010specification}. It is important to recall that the GM approach proposed by \cite{kelejian1999generalized}, as presented in Section \ref{section:Moment_Condtions}, does not yield a consistent estimate of $\lambda_0$ in the presence of heteroskedasticity. Specifically, Theorem \ref{teo:consistency_gm_lambda} is derived under the assumption of homoskedasticity. Extensions incorporating a generalized method of moments (GMM) framework have been developed by \cite{kelejian2010specification}, \cite{arraiz2010spatial}, and \cite{drukker2013two}.

The GMM approach provides three key improvements over the GM estimator. First, it is robust to heteroskedasticity. Second, it yields an asymptotic variance matrix for the parameter $\widehat{\lambda}_n$. Finally, it allows for joint inference on the spatial lag coefficient $\widehat{\rho}_n$ and the spatial error coefficient $\widehat{\lambda}_n$. 

For any nonstochastic matrix $\mA_{n}$, the quadratic moments are given by:
\begin{equation*}
\begin{aligned}
 \E\left(\vepsi_n^\top \mA_n\vepsi_n\right)& = \tr\left[\E\left(\vepsi_n^\top \mA_n\vepsi_n\right)\right], \\
 & = \E\left[\tr\left(\vepsi_n^\top \mA_n\vepsi_n\right)\right], \\
 & = \E\left[\tr\left(\mA_n\vepsi_n\vepsi_n^\top\right)\right], \\
  & = \tr\left[\E\left(\mA_n\vepsi_n\vepsi_n^\top\right)\right], \\
 & = \tr\left(\mA_n\mSigma_0\right), 
\end{aligned}
\end{equation*}
%
where $\mSigma_0 = \textrm{Diag}(\sigma_{i, n}^2, \ldots, \sigma_{n, n}^2)$. Note that $\E\left(\vepsi_n^\top \mA_n\vepsi_n\right)\neq 0$, unless $\tr(\mA_n) = 0$. But if $\textrm{Diag}(\mA_n) = 0$, meaning that $a_{ii, n} = 0$ for all $i = 1, \ldots, n$, then  $\tr\left(\mA_n\mSigma_0\right) = 0$ and $\mA_n\vepsi_n$ is uncorrelated with $\vepsi_n$. Since we need $\Diag(\mA_n) = 0$, we can always construct such matrix as $\mA_n = \mP_{n} - \Diag(\mP_n)$. 

Thus, we have the following two moment conditions
\begin{equation}\label{eq:moment_conditions_reduced}
	\begin{aligned}
\frac{1}{n}\E\left[\vepsi_n^\top\mA_{1n}\vepsi_n\right] & =  \vzeros \\
\frac{1}{n}\E\left[\vepsi_n^\top\mA_{2n}\vepsi_n\right] & =  \vzeros
	\end{aligned}
\end{equation}
%
with
\begin{equation*}
  \begin{aligned}
\mA_{1n} &= \mM^\top_n\mM_n - \Diag(\mM^\top_n\mM_n), \\
\mA_{2n} &= \mM_n.
\end{aligned}
\end{equation*}

%It can be notice that  each element $i$ of the diagonal matrix $\diag(\vw_i^\top\vm_i)$ results in the sum of squares of the weights in the $i$th column. Thus $\Diag(\mA_{1n}) = 0$. 

%where $\vm_i$ is the $i$th column of the weight matrix $\mM_n$.

The sample moments are obtained by replacing $\vepsi_n(\vtheta) = \left(\mI_n - \lambda\mM_n\right)\left(\vy_n - \mZ_n\vdelta\right)$, with $\vtheta = (\vdelta^\top, \lambda)^\top$, by the their counterpart expressed as a function of the regression residuals. Since $\vu_n(\vdelta) = \vy_n - \mZ_n\vdelta$, and $\vu_L(\vdelta) = \mM_n\vu_n(\vdelta)$, it follows that $\vepsi_n(\vtheta) = \vu_n(\vdelta) - \lambda \vu_L(\vdelta) = \vy_{s}(\lambda) - \mZ_s(\lambda)\vdelta$, the spatially filtered residuals. Then, the set of quadratic moment functions are
\begin{equation}\label{eq:moment_conditions_reduced_u}
\begin{aligned}
\vm_n(\vtheta) & = \frac{1}{n}\begin{pmatrix}
\vepsi_n(\vtheta)^\top\mA_{1n}\vepsi_n(\vtheta) \\
\vepsi_n(\vtheta)^\top\mA_{2n}\vepsi_n(\vtheta)
                              \end{pmatrix}, \\
              & = \frac{1}{n}\begin{pmatrix}
              \left[\vu_n(\vdelta) - \lambda \vu_L(\vdelta)\right]^\top \mA_{1n} \left[\vu_n(\vdelta) - \lambda \vu_L(\vdelta)\right] \\
              \left[\vu_n(\vdelta) - \lambda \vu_L(\vdelta)\right]^\top \mA_{2n} \left[\vu_n(\vdelta) - \lambda \vu_L(\vdelta)\right]
                 \end{pmatrix}.
\end{aligned}
\end{equation}


We need to write the estimator of $\lambda$ as a weighted non-linear LS estimator. Note that:
\begin{equation}\label{eq:mom-rev-1}
\begin{aligned}
  \frac{1}{n}\vepsi^\top_n(\vtheta_0)\mA_{qn}\vepsi_n(\vtheta_0) & = \left[\vu_n(\vdelta) - \lambda \vu_L(\vdelta)\right]^\top \mA_{qn} \left[\vu_n(\vdelta) - \lambda \vu_L(\vdelta)\right], \\
                               & = \frac{1}{n}\vu^\top_n\mA_{qn}\vu_n - \frac{1}{n}\lambda\left(\vu^\top_n\mA_{qn}\vu_L + \vu_L^\top\mA_{qn}\vu_n\right) +\frac{1}{n}\lambda^2\vu_L^\top\mA_{qn}\vu_L, \\
                               & = \frac{1}{n}\vu^\top_n\mA_{qn}\vu_n - 2\frac{1}{n}\lambda\vu_L^\top\mA_{qn}\vu_n +\frac{1}{n}\lambda^2\vu_L^\top\mA_{qn}\vu_L, \\
\end{aligned}
\end{equation}
%
for $q = 1, 2$. In the third line of Equation \eqref{eq:mom-rev-1}, we assume that $\mA_{qn}$ is symmetric such that:
\begin{equation*}
\begin{aligned}
\vu^\top_n\mA_{qn}\vu_L + \vu_L^\top\mA_{qn}\vu_n & = \vu_L^\top \mA_{qn}^\top\vu_n + \vu_L\mA_q\vu_n, \\
                                        & = \vu_L^\top\left(\mA_{qn} + \mA_{qn}^\top\right) \vu_n, \\
                                        & = 2\vu_L^\top\mA_{qn}\vu_n.
                                        \end{aligned}
\end{equation*}

Here it is important to note that in some cases $\mA_{2n} = \mM_n$ might not be symmetric. However, we can use Definition \ref{def:quad-form} and set:
\begin{equation}\label{eq:trick-A2}
\mA_{2n} = (1/2)\left(\mM_n + \mM^\top_n\right)
\end{equation}

Taking expectation over \eqref{eq:mom-rev-1}:
\begin{equation*}
  \begin{aligned}
 \frac{1}{n}\E\left(\vepsi^\top_n\mA_{qn}\vepsi_n\right) & = n^{-1}\E\left(\vu^\top_n\mA_{qn}\vu_n \right) - 2n^{-1}\lambda\E\left(\vu_L^\top\mA_{qn}\vu_n\right) + \lambda^2n^{-1}\E\left(\vu_L^\top\mA_{qn}\vu_L\right) \\
 0 & = n^{-1}\E\left(\vu^\top_n\mA_{qn}\vu_n \right) - \begin{pmatrix}
                                                           2n^{-1}\E\left(\vu_L^\top\mA_{qn}\vu_n\right) &   -n^{-1}\E\left(\vu_L^\top\mA_{qn}\vu_L\right)  
                                                       \end{pmatrix}
                                                       \begin{pmatrix}
                                                        \lambda \\
                                                        \lambda^2
                                                       \end{pmatrix}
 \end{aligned}
\end{equation*}

Then, we have the following system of equations for $q = 1, 2$ \citep[see][pag 56]{kelejian2010specification}:
\begin{equation}\label{eq:system-gmm-general}
\begin{aligned}
  \begin{pmatrix}
    n^{-1}\E\left(\vu^\top_n\mA_{1q}\vu_n \right)  \\
    n^{-1}\E\left(\vu^\top_n\mA_{2q}\vu_n \right) 
  \end{pmatrix} -
  \begin{pmatrix}
    2n^{-1}\E\left(\vu_L^\top\mA_{1q}\vu_n\right) &   -n^{-1}\E\left(\vu_L^\top\mA_{1q}\vu_L\right)   \\
    2n^{-1}\E\left(\vu_L^\top\mA_{2q}\vu_n\right) &   -n^{-1}\E\left(\vu_L^\top\mA_{2q}\vu_L\right)
  \end{pmatrix}
  \begin{pmatrix}
  \lambda \\
  \lambda^2
  \end{pmatrix}
  & = \vzeros \\
    \begin{pmatrix}
    n^{-1}\E\left(\vu^\top_n\mA_{1q}\vu_n \right)   \\
    n^{-1}\E\left(\vu^\top_m\mA_{2q}\vu_n \right) 
  \end{pmatrix}-
  \begin{pmatrix}
    2n^{-1}\E\left(\vu^\top_n\mM^\top_n\mA_{1q}\vu_n\right) &   -n^{-1}\E\left(\vu^\top_n\mM^\top_n\mA_{1q}\mM_n\vu_n\right)   \\
    n^{-1}\E\left(\vu_L^\top\left(\mM_n + \mM^\top_n\right)\vu_n\right) &   -n^{-1}\E\left(\vu^\top_n\mM^\top_n\mA_{2q}\mM_n\vu_n\right)
  \end{pmatrix} & = \vzeros \\
  \vgamma_n - \mGamma_n\valpha_n &= \vzeros. 
\end{aligned}
\end{equation}
%
where we use Equation \eqref{eq:trick-A2} for the second moment. Now, we can express the \textbf{sample moment conditions} as in Section \ref{section:Moment_Condtions}:
\begin{equation*}
	\underset{2 \times 1}{\widetilde{\vm}_n(\widetilde{\vdelta}_n, \lambda)} = \underset{2 \times 1}{\widetilde{\vg}_n} - \underset{2 \times 2}{\widetilde{\mG}_n}\begin{pmatrix}\lambda \\ \lambda^2\end{pmatrix} = \vzeros
\end{equation*}

The elements of $\widetilde{\vg}_n$ the following:
\begin{eqnarray*}
	\widetilde{\vg}_{1n}  &=& \frac{1}{n}\widetilde{\vu}^\top_n\mA_{1n}\widetilde{\vu}_n \\
\widetilde{\vg}_{2n}  &=&  \frac{1}{n}\widetilde{\vu}^\top_n\mA_{2n}\widetilde{\vu}_n   =  \frac{1}{n} \widetilde{\vu}^\top_n\widetilde{\vu}_L
\end{eqnarray*}

The $\widetilde{\mG}_n$ matrix is given by:
\begin{eqnarray*}
\widetilde{\mG}_{11,n} &=& 2n^{-1}\widetilde{\vu}^\top_n\mM^\top_n\mA_{1n}\widetilde{\vu}_n \\
\widetilde{\mG}_{12,n} &=& -n^{-1}\widetilde{\vu}^\top_n\mM^\top_n\mA_{1n}\mM_n\widetilde{\vu}_n \\
\widetilde{\mG}_{21,n} &=& -n^{-1}\widetilde{\vu}^\top_n\mM^\top_n\left(\mA_{2n} + \mA_{2n}^\top\right)\widetilde{\vu}_n \\
\widetilde{\mG}_{22,n}&=& -n^{-1}\widetilde{\vu}^\top_n\mM_n\mA_{2n}\mM_n\widetilde{\vu}_n
\end{eqnarray*}

A more compact notation is:
\begin{equation*}
\begin{aligned}
\widetilde{\mG}_n & = \frac{1}{n}
                    \begin{pmatrix}
                      \widetilde{\vu}^\top_n\left(\mA_{1n} + \mA^\top_{1n}\right)\widetilde{\vu}_s & - \widetilde{\vu}^\top_s\mA_{1n}\widetilde{\vu}^\top_s \\
                      \vdots & \vdots \\
                      \widetilde{\vu}^\top_n\left(\mA_{qn} + \mA^\top_{qn}\right)\widetilde{\vu}_s & - \widetilde{\vu}^\top_s\mA_{qn}\widetilde{\vu}^\top_s
                    \end{pmatrix}, \\
\widetilde{\vg}_n & = \frac{1}{n}\begin{pmatrix}
                                \widetilde{\vu}^\top_n\mA_{1n}\widetilde{\vu}_n \\
                                \vdots \\
                                \widetilde{\vu}^\top_n\mA_{qn}\widetilde{\vu}_n
                                \end{pmatrix},
\end{aligned}
\end{equation*}
%
for $q = 1, 2$.

% Now, let $\mPsi$ be $2\times 2$ matrix of variance-covariance matrix of the moment conditions $\frac{1}{n}\E\left[\vepsi^\top\mA_1\vepsi\right]$. Then, the using Equation \eqref{eq:var-quadratic-form} from  Lemma \ref{lemma:second-mom-lee}:
% 
% \begin{equation*}
% \psi_{s,r}=\frac{1}{2n}\tr\left[\left(\mA_s + \mA_s^\top\right)\mSigma\left(\mA_r + \mA_r^\top\right)\mSigma\right]+\frac{1}{n}\vmu^\top\left(\mA_1 + \mA_1^\top\right)\mSigma\left(\mA_2 + \mA_2^\top\right)\vmu
% \end{equation*}
% %
% where $s,r = 1,2$ correspond to the moment conditions; $\mSigma$ is a diagonal matrix in the heteroskedasticity case with elements:
% 
% \begin{equation*}
% \widehat{\epsilon}_i^2 = (\widetilde{u}_i - \lambda\widetilde{u}_{L_i})^2 = \widetilde{u}_{s_i}^2
% \end{equation*}



% The variance of the moments is the following:
% 
% \begin{equation*}
% \begin{aligned}
%   \E\left[\frac{1}{n}(\vu^\top_s\mA_r\vu_s)\frac{1}{n}(\vu^\top_s\mA_q\vu_s)^\top\right] & = \frac{1}{n^2}\E\left[\vu_s^\top\mA_r\vu_s\vu_s^\top\mA_q^\top\vu_s\right]
% \end{aligned}
% \end{equation*}



% \begin{remark}
% 	\cite{kelejian1999generalized} show consistency of the Method of Moment estimator of $\lambda$, but not asymptotic normality of the estimator.
% \end{remark}	



%----------------------------------
\subsection{Assumptions}
%----------------------------------

In this section, we outline the key assumptions for the SAC model under heteroskedasticity, following \cite{arraiz2010spatial}. These assumptions primarily concern the spatial weight matrices, error structure, regressors, and instruments.

%---------------------------------------------------------------------------------
\begin{assumption}[Spatial Weights Matrices \citep{arraiz2010spatial}]\label{assump:w_matri_gmm}
	Assume the following:
	\begin{enumerate}
		\item All diagonal elements $\mW_n$ and $\mM_n$ are zero.
		\item $\lambda_n\in (-1, 1)$, $\rho_n \in (-1, 1)$.
		\item The matrices $\mI_n - \rho_n \mW_n$ and $\mI_n - \lambda_n \mM_n$ are nonsingular for all $\lambda_n\in (-1, 1)$ and $\rho_n \in (-1, 1)$.
	\end{enumerate}
\end{assumption}	
%---------------------------------------------------------------------------------

Assumption \ref{assump:w_matri_gmm}(a) is a normalization rule, ensuring that a region cannot be its own neighbor. Assumption \ref{assump:w_matri_gmm}(b) defines the parameter space, as discussed in \citet[section 2.2]{kelejian2010specification}. Finally, Assumption \ref{assump:w_matri_gmm}(c) ensures that the spatial processes for $\vy_n$ and $\vu_n$ are uniquely defined. Under this assumption, the SAC model can be written as:
\begin{equation*}
	\begin{aligned}
	\vy_n  & = \left(\mI_n - \rho_0 \mW_n\right)^{-1}\left[\mX_n\vbeta_0 + \vu_n\right] \\
	\vu_n  & = \left(\mI_n -  \rho_0 \mM_n\right)^{-1}\vepsi_n.
	\end{aligned}
\end{equation*}

The reduced form is:
\begin{equation*}
\vy_n = (\mI_n - \rho_0\mW_n)^{-1}\mX_n\vbeta_0 +(\mI_n -\rho_0\mW)^{-1}(\mI_n - \lambda_0\mM_n)^{-1}\vepsi_n. 
\end{equation*}

% The reduced form represents a system of $n$ simultaneous equations. As in the standard spatial lag model, we can include endogenous explanatory variables on the right hand side of model specification. In this case:
% 
% \begin{equation*}
% \vy = \rho\mW\vy + \mX\vbeta + \mY\vgamma + \left(\mI - \lambda\mW\right)^{-1}\vepsi. 
% \end{equation*}


%--------------------------------------------------------------------
\begin{assumption}[Heteroskedastic Errors  \citep{arraiz2010spatial}]\label{assump:error_hete_gmm} 
	The error term  $\left\lbrace\epsilon_{i,n}: 1 \leq i \leq n, n\geq 1\right\rbrace$ satisfy $\E(\epsilon_{i,n}) = 0$, $\E(\epsilon_{i,n}^2) = \sigma^2_{i,n}$, with $0 < \underline{a}^\sigma \leq \sigma^2_{i,n}\leq \overline{a}^\sigma<\infty$. Furthermore, for each $n\geq 1$ the random variables $\epsilon_{1,n}, \ldots, \epsilon_{n,n}$ are totally independent.  
\end{assumption}
%--------------------------------------------------------------------

This assumption allows for heteroskedasticity in the innovations while ensuring uniformly bounded variances. It also accommodates triangular array structures where variances may depend on the sample size $n$. 

%--------------------------------------------------------------------
\begin{assumption}[Bounded Spatial Weight Matrices \citep{arraiz2010spatial}]\label{assump:bounded_matrices_hetgmm}
		 The row and column sums of the matrices $\mW_n$ and $\mM_n$ are bounded uniformly in absolute value, by , respectively, one and some finite constant, and the row and column sums of the matrices $(\mI_n - \rho_0\mW_n)^{-1}$ and  $(\mI_n - \lambda_0\mM_n)^{-1}$ are bounded uniformly in absolute value by some finite constant.
\end{assumption}	
%--------------------------------------------------------------------

This assumption is a technical requirement for deriving large-sample properties of the estimators. It ensures that spatial dependence in $\vy_n$ and $\vu_n$ does not accumulate indefinitely, maintaining a ``fading'' memory property. Under this assumption:
\begin{equation*}
\begin{aligned}
\E\left[\vu_n \right] & = \E\left[\left(\mI_n - \lambda_0 \mM_n\right)^{-1}\vepsi_n\right],  \\
             & = \left(\mI_n - \lambda_0 \mM_n\right)^{-1} \E\left[\vepsi_n\right], \\
             & = \vzeros \;\;\;\mbox{by Assumption \ref{assump:error_hete_gmm} (Heteroskedastic Errors)}.
\end{aligned}
\end{equation*}

Additionally, the variance of $\vu_n$ is given by:
\begin{equation*}
\begin{aligned}
	\E\left[\vu_n\vu^\top_n\right] & = \E\left[\left(\mI_n - \lambda_0 \mM_n\right)^{-1}\vepsi_n\vepsi^\top_n\left(\mI_n - \lambda_0 \mM^\top_n\right)^{-1}\right], \\
	& = \left(\mI_n - \lambda_0\mM_n\right)^{-1}\E\left[\vepsi_n\vepsi^\top_n\right]\left(\mI - \lambda_0 \mM^\top_n\right)^{-1}, \\
	& = \left(\mI_n - \lambda_0 \mM_n\right)^{-1}\mSigma_0\left(\mI_n - \lambda_0\mM^\top_n\right)^{-1}
\end{aligned}
\end{equation*}
%
where $\mSigma_0 = \Diag(\sigma^2_{i,n})$.

%-------------------------------------------------------------------------------------
\begin{assumption}[Regressors \citep{arraiz2010spatial}]\label{assump:no_multi_gmmhet}
The regressor matrices $\mX_n$ have full column rank (for $n$ large enough). Furthermore, the elements of the matrices $\mX_n$ are uniformly bounded in absolute value.
\end{assumption}
%-------------------------------------------------------------------------------------

This assumption prevents multicollinearity and ensures that the regressors remain well-behaved in large samples.

%-------------------------------------------------------------------------------------
\begin{assumption}[Instruments I \citep{arraiz2010spatial}] The instruments matrices $\mH_n$ have full column rank $L \geq K + 1$ (for all $n$ large enough). Furthermore, the elements of the matrices $\mH_n$ are uniformly bounded in absolute value. Additionally, $\mH_n$ is assumed to, at least, contain the linearly independent columns of $(\mX_n, \mM_n\mX_n)$
\end{assumption}
%-------------------------------------------------------------------------------------

There are some papers that discuss the use of optimal instruments for the spatial \citep[see for example][]{lee2003best, das2003finite, Keliejian2004, lee2007gmm}.

\begin{remark}
The effect of the selection of instruments on the efficiency of the estimators remains to be further investigated.
\end{remark}

%-------------------------------------------------------------------------------------
\begin{assumption}[Instruments II (Identification) \citep{arraiz2010spatial}]\label{assumption:instruments-arraiz} 
The instruments $\mH_n$ satisfy furthermore:
\begin{enumerate}
\item $\mQ_{HH} = \lim_{n \to \infty}n^{-1}\mH_n^\top\mH_n$ is finite and nonsingular.
\item $\mQ_{HZ} = \plim_{n \to \infty}n^{-1}\mH_n^\top\mZ_n$ and $\mQ_{HMZ} = \plim_{n \to \infty}n^{-1}\mH_n^\top\mM\mZ_n$ are finite and have full column rank. Furthermore $\mQ_{HZ,s}(\lambda_0) = \mQ_{HZ}-\lambda_0\mQ_{HMZ}$ has full column rank.
\item $\mQ_{H\Sigma H} = \lim_{n \to \infty}n^{-1}\mH_n^\top\Sigma_n\mH_n$ is finite and nonsingular, where $\mSigma_n= \diag(\sigma^2_{i,n})$
\end{enumerate}
\end{assumption}
%-------------------------------------------------------------------------------------

In treating $\mX_n$ and $\mH_n$ as non-stochastic our analysis should be viewed as conditional on $\mX_n$ and $\mH_n$.


%-----------------------------------------------------
\subsection{Estimators and Estimation Procedure in a Nutshell}
%----------------------------------------------------

Consider again the transformed model:
\begin{equation*}
  \vy_{s,n}(\lambda_0) = \mZ_{s,n}(\lambda_0)\vdelta_0 + \epsilon_n, 
\end{equation*}
%
where $\vy_{s,n}(\lambda_0) = \vy_n - \lambda_0\mM_n\vy_n$ and $\mZ_{s,n}(\lambda_0) = \mZ_n - \lambda_0\mM_n\mZ_n$. If $\lambda_0$ were known, we could directly apply the S2SLS estimator to the transformed model. However,  since $\lambda_0$ is unknown, it must be estimated before $\vdelta_0$. The estimation procedure consists of the following steps

\begin{enumerate}
  \item Obtain an initial IV estimator of $\vdelta_0$ to construct consistent residuals.
  \item Use these residuals to derive the moment conditions that provide a consistent estimate  of $\lambda_0$ via GMM.
  \item Use the estimate of $\lambda_0$ to define a \textbf{weighting matrix} for the moment conditions to obtain a consistent and efficient estimator. 
  \item Estimate $\delta_0$ from the \textbf{transformed model}.
  \item Finally, a \textbf{consistent and efficient} estimate of $\lambda$ is based on GS2SLS residuals. 
\end{enumerate}

These steps are shown in Figure \ref{figure:flowchart}. Now we will consider each step in detail.

\begin{figure}[H]
\caption{Estimation steps for SAC model}\label{figure:flowchart}
\centering
    \begin{tikzpicture}[FlowChart,
    node distance = 5mm and 7mm,
      start chain = A going below
                        ]
% nodes in chain                        
\node [startstop] {\textbf{Obtain a consistent estimate of $\widehat{\lambda}$}};                 % node name: A-1
\node [process]   {\textbf{Estimate S2SLS}:\\
                     $\widetilde{\delta}_{2SLS} = \left(\widetilde{\mZ}^\top\mZ\right)^{-1}\widetilde{\mZ}^\top\vy$\\
                     and get $\widehat{\vu}_{2SLS}$};        
\node [process]   {\textbf{Initial GMM estimator of $\lambda$}:\\
                   Use $\widehat{\vu}_{2SLS}$ to obtain $\breve{\lambda}_{GMM}$};
\node [process]   {\textbf{Efficient GMM estimator of $\lambda$}:\\
                   Use $\widehat{\vu}_{GMM}$ to compute \\
                   the weighting matrix $\widetilde{\mPsi}$\\
                   and obtain $\widetilde{\lambda}_{OGMM}$};          
\node [startstop] {\textbf{Obtain a consistent estimate of $\widehat{\vdelta}$}};  
\node [process]   {\textbf{Estimate FGS2SLS  using $\widetilde{\lambda}_{OGMM}$}:\\

                     $\widehat{\vdelta}_{FGS2SLS } = \left[\widehat{\mZ}_{s}^\top \mZ\right]^{-1}  \widehat{\mZ}_{s}^\top \vy_s$\\
                     and get $\widehat{\vu}_{FGS2SLS }$};     
\node [process]   {\textbf{Efficient GMM estimator of $\lambda$ using} :\\
                   Use $\widehat{\vu}_{FGS2SLS}$ to compute \\
                   the weighting matrix $\widetilde{\mPsi}$\\
                   and obtain $\widetilde{\lambda}_{OGMM}$};                    % A-7
        \end{tikzpicture}
\end{figure}




%***************************************
\subsubsection{Step 1a: S2SLS estimator}\label{sec:step-s2slsestimator}
%***************************************

In the first step, $\vdelta_0$ is estimated by 2SLS applied to \textbf{untransformed model}  $\vy_n = \mZ_n\vdelta_0 + \vu_n$ using the instruments matrix $\mH_n$. Then:
\begin{equation*}
\widetilde{\delta}_{S2SLS, n} = \left(\widetilde{\mZ}^\top_n\mZ_n\right)^{-1}\widetilde{\mZ}^\top_n\vy_n, 
\end{equation*}
%
where $\widetilde{\mZ}_n = \mH_n\left(\mH^\top_n\mH_n\right)^{-1}\mH^\top_n\mZ_n = \mP_{H, n}\mZ_n= (\mX_n, \widetilde{\mW_n\vy_n})$. The estimates $\widetilde{\delta}_{S2SLS, n}$ yield an initial vector of residuals, $\vu_{n, S2SLS}$ as:
\begin{equation*}
\widetilde{\vu}_{S2SLS, n} = \vy_n - \mZ_n\widetilde{\vdelta}_{n, S2SLS}.
\end{equation*}

The following Theorem states that $\widetilde{\vdelta}_{S2SLS, n}$ is consistent:

%----------------------------------------------------------------
\begin{theorem}[Consistency of $\widetilde{\vdelta}_{S2SLS, n}$ \citep{kelejian2010specification}]\label{teo:Consistency-2sls}
Suppose the assumptions hold. Then $\widetilde{\vdelta}_{S2SLS, n} = \vdelta_0 + O_p(n^{-1/2})$, and hence $\widetilde{\vdelta}_{n, S2SLS}$ is consistent for $\vdelta_0$:
\begin{equation*}
\widetilde{\vdelta}_{S2SLS, n}\pto \vdelta_0.
\end{equation*}
\end{theorem}
%----------------------------------------------------------------

\begin{proof}[Sketch of proof for Theorem \ref{teo:Consistency-2sls}]
The model is:
\begin{equation*}
\begin{aligned}
	\vy_n & = \mZ_n\vdelta_0 + \vu_n,\\
	\vu_n & = \lambda_0 \mM_n \vu_n + \vepsi_n.
\end{aligned}
\end{equation*}


The sampling error is given by:
\begin{equation*}
  \begin{aligned}
     \widehat{\vdelta}_n & =  \vdelta_0 + \left(\widehat{\mZ}^\top_n\widehat{\mZ}_n\right)^{-1}\widehat{\mZ}^\top_n\vu_n, \\
     & = \vdelta_0 + \left[\left(\mH_n(\mH_n^\top\mH)^{-1}\mH_n^\top\mZ_n\right)^\top\left(\mH_n(\mH^\top_n\mH_n)^{-1}\mH^\top_n\mZ_n\right)\right]^{-1}\left(\mH_n(\mH_n^\top\mH_n)^{-1}\mH_n^\top\mZ_n\right)^\top\vu_n,\\
     & = \vdelta_0 + \left[\mZ^\top_n \mH_n (\mH^\top_n\mH_n)^{-1}\mH^\top_n\mZ_n\right]^{-1}\mZ^\top_n\mH_n(\mH^\top_n\mH_n)^{-1}\mH^\top_n\left(\mI -\lambda_0\mM_n\right)^{-1}\vepsi_n.
  \end{aligned}
\end{equation*}

Solving for $\widehat{\vdelta}_{n} - \vdelta_0$ and multiplying by $\sqrt{n}$ we obtain:
\begin{equation*}
\begin{aligned}
\sqrt{n}(\widehat{\vdelta}_{n} - \vdelta_0) & = \left[\left(\frac{1}{n}\mH^\top_n\mZ_n \right)^\top\left(\frac{1}{n}\mH^\top_n\mH_n\right)^{-1}\left(\frac{1}{n}\mH^\top_n\mZ_n \right)\right]^{-1} \\
& \times \left(\frac{1}{n}\mH^\top_n\mZ_n \right)^\top\left(\frac{1}{n}\mH^\top_n\mH_n\right)^{-1}\frac{1}{\sqrt{n}}\mH^\top_n\left(\mI -\lambda\mM_n\right)^{-1}\vepsi_n, \\
             & = \left[\left(\frac{1}{n}\mH^\top_n\mZ_n \right)^\top\left(\frac{1}{n}\mH^\top_n\mH_n\right)^{-1}\left(\frac{1}{n}\mH^\top_n\mZ_n \right)\right]^{-1}\left(\frac{1}{n}\mH^\top_n\mZ_n \right)^\top\left(\frac{1}{n}\mH^\top_n\mH_n\right)^{-1}\frac{1}{\sqrt{n}}\mF_n^\top\vepsi_n,
\end{aligned}
\end{equation*}
%
where:
\begin{equation*}
\mF_n^\top = \mH^\top_n\left(\mI_n -\lambda_0\mM_n\right)^{-1} \quad \mbox{whose elements are bounded in absolute value.}
\end{equation*}

Assumption \ref{assumption:instruments-arraiz} implies that:
\begin{equation*}
\begin{aligned}
  \lim \frac{1}{n}\mH_n^\top\mH_n &= \mQ_{HH}, \\
  \plim \frac{1}{n}\mH_n^\top\mZ_n & = \mQ_{HZ},
\end{aligned}
\end{equation*}
%
which are finite and nonsingular. 

Furthermore, note that $\E(n^{-1/2}\mF_n^\top\vepsi_n) = \vzeros$ and, under homoskedasticity,  
\begin{equation*}
\begin{aligned}
\E\left[(n^{-1/2}\mF_n^\top\vepsi_n)(n^{-1/2}\mF_n^\top\vepsi_n)^\top\right] & = \frac{1}{n}\E\left[\mH_n^\top\left(\mI_n -\lambda\mM_n\right)^{-1}\vepsi_n\vepsi_n^\top\left(\mI_n -\lambda\mM_n^\top\right)^{-1} \mH_n \right], \\
& = \sigma^2_0\frac{1}{n}\mH_n^\top\left(\mI_n -\lambda_0\mM_n\right)^{-1}\left(\mI_n -\lambda_0\mM_n^\top\right)^{-1} \mH_n. 
\end{aligned}
\end{equation*}

Assume that 
\begin{equation*}
\lim_{n\to\infty}\frac{1}{n}\mH_n^\top\left(\mI_n -\lambda_0\mM_n\right)^{-1}\left(\mI_n -\lambda_0\mM_n^\top\right)^{-1} \mH_n = \frac{1}{n}\mF^\top_n\mF_n =\mPhi\quad \mbox{exists}
\end{equation*}

Then using Theorem \ref{teo:CLT_tri_arr}:
\begin{equation*}
n^{-1/2}\mF_n^\top\vepsi_n\dto \rN(\vzeros, \sigma^2_0\mPhi).
\end{equation*}

Therefore:
\begin{equation*}
\sqrt{n}(\widehat{\vdelta}_{n} - \vdelta_0) \dto \rN(\vzeros, \mDelta),
\end{equation*}
%
and
\begin{equation*}
  \mDelta = \sigma^2_{0}\left[\mQ_{HZ}^\top\mQ_{HH}^{-1}\mQ_{HZ}\right]^{-1}\mQ_{HZ}^\top\mQ_{HH}^{-1}\mPhi\mQ_{HH}^{-1}\mQ_{HZ}\left[\mQ_{HZ}^\top\mQ_{HH}^{-1}\mQ_{HZ}\right]^{-1}.
\end{equation*}

Then we can say that $\widetilde{\vdelta} = \vdelta + O_p(n^{-1/2})$. Consistency follows if $n^{-1}\mF_n^\top\vepsi_n\pto \vzeros$. Note that $\E(n^{-1}\mF_n^\top\vepsi_n) = \vzeros$ and 
\begin{equation*}
\var\left(n^{-1}\mF_n^\top\vepsi_n\right)=\sigma^2_0\frac{1}{n^2}\mH_n^\top\left(\mI_n -\lambda_0\mM_n\right)^{-1}\left(\mI_n -\lambda_0\mM_n^\top\right)^{-1} \mH_n,
\end{equation*}
%
which converges to $\vzeros$, then using Chebyshev's Theorem \ref{teo:chebyshev}:
\begin{equation*}
  n^{-1}\mF_n^\top\vepsi_n\pto \vzeros\quad\mbox{and hence}\quad \widetilde{\vdelta}_n\pto\vdelta_0.
\end{equation*}
\end{proof}


Although $\widetilde{\vdelta}_{S2SLS, n}$ is consistent, it does not utilize information relating to the spatial correlation error term. We therefore turn to the second step of the procedure. (Question: Why we cannot use the OLS residuals for the next step?)


%**********************************************************************************
\subsubsection{Step 1b: Initial GMM estimator of $\lambda$ based on S2SLS residuals}
%**********************************************************************************

Using the consistent estimate $\vu_n$ in the previous step, now we create the sample moments for $q= 1, 2$ based on the estimated residuals. Define:
\begin{equation*}
\begin{aligned}
\widetilde{\vu}_{S2SLS, n} & = \vy_n - \mZ_n\widetilde{\vdelta}_{n, S2SLS}, \\
\widetilde{\vu}_{s}        & = \mM_n\widetilde{\vu}_{n, S2SLS},
\end{aligned}
\end{equation*}
%
and the sample moments are:
\begin{equation}\label{eq:momdent_conditions_step}
	\begin{aligned}
		\vm_n(\lambda_n,\widetilde{\vdelta}_{S2SLS, n}) & = \frac{1}{n}
		\begin{pmatrix}
		\widetilde{\vu}_{n, S2SLS}^\top \left(\mI_n - \lambda_n\mM^\top_n\right)\mA_{1,n}\left(\mI_n - \lambda_n\mM_n\right)\widetilde{\vu}_{n, S2SLS} \\
		\widetilde{\vu}_{n, S2SLS}^\top \left(\mI_n - \lambda_n\mM^\top_n\right)\mA_{2,n}\left(\mI_n - \lambda_n\mM_n\right)\widetilde{\vu}_{n, S2SLS}
		\end{pmatrix}, \\
		& = \widetilde{\mG}_n\begin{pmatrix} \lambda_n \\
                \lambda^2_n
\end{pmatrix} - \widetilde{\vg}_n,
	\end{aligned}
\end{equation}
%
where,
\begin{equation*}
\begin{aligned}
\widetilde{\mG}_n & = \frac{1}{n}
                    \begin{pmatrix}
                      \widetilde{\vu}^\top_n\left(\mA_{1n} + \mA^\top_{1n}\right)\widetilde{\vu}_{n,s} & - \widetilde{\vu}^\top_{n,s}\mA_{1n}\widetilde{\vu}^\top_{s,n} \\
                      \vdots & \vdots \\
                      \widetilde{\vu}^\top_n\left(\mA_{qn} + \mA^\top_{qn}\right)\widetilde{\vu}_{n,s} & - \widetilde{\vu}^\top_{n,s}\mA_{qn}\widetilde{\vu}^\top_{s,n}
                    \end{pmatrix} \\
\widetilde{\vg}_n & = \frac{1}{n}\begin{pmatrix}
                                \widetilde{\vu}^\top_n\mA_{1n}\widetilde{\vu}_n \\
                                \vdots \\
                                \widetilde{\vu}^\top_n\mA_{qn}\widetilde{\vu}_n
                                \end{pmatrix}.
\end{aligned}
\end{equation*}


The initial GMM estimator for $\lambda$ is then defined as
\begin{equation*}
\breve{\lambda}_{gmm} = \underset{\lambda_n}{\argmin}\left\lbrace \left[\widetilde{\mG}_n\begin{pmatrix} \lambda_n \\
                \lambda^2_n
\end{pmatrix} - \widetilde{\vg}_n \right]^\top \left[\widetilde{\mG}_n\begin{pmatrix} \lambda_n \\
                \lambda^2_n
\end{pmatrix} - \widetilde{\vg}_n\right]\right\rbrace = \underset{\lambda_n}{\argmin}\left[\vm_n(\lambda_n,\widetilde{\vdelta}_{S2SLS, n})^\top \vm_n(\lambda_n,\widetilde{\vdelta}_{S2SLS, n})\right]
\end{equation*}
%
where $\mUpsilon^{\lambda\lambda}_n = \mI$. This estimator is consistent but not efficient. For efficiency we need to replace $\mUpsilon^{\lambda\lambda}_n$ by the variance-covariance matrix of the sample moments. Furthermore, the expression above can be interpreted as a nonlinear least squares system of equations. The initial estimate is thus obtained as a solution of the above system. 

Now, we need to define the expression for the matrices $\mA_{n,s}$. \cite{drukker2013two} suggest, for the homoskedastic case, the following expressions:
\begin{equation*}
  \begin{aligned}
    \mA_{1n} & = \upsilon \left[\mM^\top_n\mM_n - \frac{1}{n}\tr\left(\mM^\top_n\mM_n\right)\mI_n\right],\\ 
    \mA_{2n} & = \mM_n,
  \end{aligned}
\end{equation*}
%
where $\upsilon$ is the scaling factor needed to obtain the same estimator of \cite{kelejian1998generalized, kelejian1999generalized}. 

On the other hand, when heteroskedasticity is assumed, \cite{kelejian2010specification} recommend the following expressions:
\begin{equation*}
\begin{aligned}
\mA_{1n} & = \mM^\top_n\mM_n  - \Diag(\mM^\top_n\mM_n), \\
\mA_{2n} & = \mM_n
\end{aligned}
\end{equation*}


%**********************************************************************************
\subsubsection{Step 1c: Efficient GMM estimator of $\lambda_0$ based on S2SLS residuals}
%**********************************************************************************

The efficient GMM estimator of $\lambda_0$ is a weighted nonlinear least squares estimator. Specifically, this estimator is $\widetilde{\lambda}_n$ where:
\begin{equation}\label{eq:nonlinear-gmm-lambda}
\widetilde{\lambda}_{ogmm} = \underset{\lambda_n}{\argmin} \left[\vm_n(\lambda_n,\widetilde{\vdelta}_n)^\top\widetilde{\mPsi}^{-1}_n\vm_n(\lambda_n,\widetilde{\vdelta}_n) \right],
\end{equation}
%
and where the weighting matrix is $\widetilde{\mPsi}^{-1}_n$, where $\mPsi_n$ is the variance of the moment conditions $\vm(\lambda_0,\widetilde{\vdelta})$. 


The matrix $\widetilde{\mPsi}^{-1}_n= \widetilde{\mPsi}^{-1}_n (\breve{\lambda}_{gmm})$ is defined as follows. Let $\widetilde{\mPsi}_n = \left[\widehat{\Psi}_{rs}\right]_{r,s = 1, 2}$ with
\begin{equation*}
\widetilde{\Psi}_{rs} = (2n)^{-1}\tr\left[(\mA_{r, n} + \mA_{r, n}^\top)\widetilde{\mSigma}_n(\mA_{s, n} + \mA_{s, n}^\top)\widetilde{\mSigma}_n\right] + n^{-1}\widetilde{\va}_{r, n}^\top\widetilde{\mSigma}\widetilde{\va}_{s, n},
\end{equation*}
%
where:
\begin{equation*}
  \begin{aligned}
    \widetilde{\mSigma}_n  & = \diag_{i=1, \ldots,n}\left(\widetilde{\epsilon}_{in}^2\right) \\
    \widetilde{\vepsi}_n   & = \left(\mI_n - \breve{\lambda}_{gmm}\mM_n\right)\widetilde{\vu}_n \\
    \widetilde{\va}_{r,n} & = \left(\mI_n - \breve{\lambda}_{gmm}\mM_n\right)\mH_n\widetilde{\mP}_n\widetilde{\valpha}_{r,n} \\
    \widetilde{\valpha}_{r,n} & = - n^{-1}\left[\mZ^\top_n\left(\mI_n - \breve{\lambda}_{gmm}\mM _n\right)(\mA_{r, n} + \mA_{r, n}^\top)\left(\mI_n - \breve{\lambda}_{gmm}\mM_n\right)\widetilde{\vu}_n\right] \\
    \widetilde{\mP}_n & = \left(\frac{1}{n}\mH^\top_n\mH_n\right)^{-1}\left(\frac{1}{n}\mH_n^\top\mZ_n \right)\left[\left(\frac{1}{n}\mH^\top\mZ_n\right)^\top\left(\frac{1}{n}\mH^\top_n\mH_n\right)^{-1}\left(\frac{1}{n}\mH^\top_n\mZ_n\right)\right]^{-1}
  \end{aligned}
\end{equation*}

It is important to note that this step is not necessary since the previous estimator of $\lambda$ is already consistent. 

%**********************************************************************************
\subsubsection{Step 2a: GS2SLS Estimator}
%**********************************************************************************

Using $\breve{\lambda}_{ogmm}$ from step 1c (or the consistent estimator from step 1b) in the transformed model we have:
\begin{equation*}
  \widehat{\vdelta}_n(\widetilde{\lambda}_{ogmm}) = \left[\widehat{\mZ}_{s,n}^\top(\widetilde{\lambda}_{ogmm}) \mZ(\widetilde{\lambda}_{ogmm})\right]^{-1}  \widehat{\mZ}_{s, n}^\top(\widetilde{\lambda}_{ogmm}) \vy_{s, n}(\widetilde{\lambda}_{ogmm})
\end{equation*}
%
where
\begin{equation*}
  \begin{aligned}
    \vy_{s, n}           & = \vy_n - \widetilde{\lambda}_{ogmm}\mM_n\vy_n, \\
    \mZ_{s, n}           & = \mZ_n - \widetilde{\lambda}_{ogmm}\mM_n\mZ_n, \\
    \widehat{\mZ}_{s, n} & = \mP_{H, n}\mZ_{s, n},\\
    \mP_{H, n}           & = \mH_n\left(\mH^\top_n\mH_n\right)^{-1}\mH^\top_n.
  \end{aligned}
\end{equation*}


%**********************************************************************************
\subsubsection{Step 2b: Efficient GMM estimator of $\lambda$ using GS2SLS  residual}
%**********************************************************************************

In this last step, the \textbf{efficient} GMM estimator of $\lambda_0$ based on the GS2SLS residuals is obtained by minimizing the following expression:
\begin{equation*}
\widehat{\lambda}_n = \underset{\lambda_n}{\argmin}\left\lbrace \left[\widehat{\mG}_n
\begin{pmatrix} \lambda_n \\
                \lambda^2_n
\end{pmatrix}
- \widehat{\vg}_n \right]^\top (\widehat{\mPsi}_n^{\widehat{\lambda}\widehat{\lambda}})^{-1} \left[\widehat{\mG}_n\begin{pmatrix} \lambda_n \\
                \lambda^2_n
\end{pmatrix} - \widehat{\vg}_n \right]\right\rbrace
\end{equation*}
%
where $\widehat{\mPsi}_n^{\widehat{\lambda}\widehat{\lambda}}$ is an estimator for the variance-covariance matrix of the (normalized) sample moment vector based on the GS2SLS residuals. This estimator differs for the cases of homoskedastic and heteroskedastic errors.

For the \textbf{homoskedastic} case the $r, s$ (with $r,s = 1,2$) element of $\widehat{\mPsi}^{\widehat{\lambda}\widehat{\lambda}}_n$ is given by:
\begin{equation*}
\begin{aligned}
  \widehat{\mPsi}^{\widehat{\lambda}\widehat{\lambda}}_{rs} & = \left[\widetilde{\sigma}^2\right]^2(2n)^{-1}\tr\left[\left(\mA_r + \mA_r^\top\right)\left(\mA_s + \mA_s^\top\right)\right] \\
& + \widetilde{\sigma}^2n^{-1}\widetilde{\va}_r^\top\widetilde{\va}_s^\top \\
& + n^{-1}\left(\widetilde{\mu}^{(4)} - 3\left[\widetilde{\sigma}^2\right]^2\right)\vec_D\left(\mA_r\right)^\top\vec_D\left(\mA_s\right) \\
& + n^{-1}\widetilde{\mu}^{(3)}\left[\widetilde{\va}_r^\top\vec_D\left(\mA_s\right) + \widetilde{\va}_s^\top\vec_D\left(\mA_r\right)\right],
\end{aligned}
\end{equation*}
%
where
\begin{equation*}
  \begin{aligned}
    \widetilde{\va}_r & = \widehat{\mT}\widetilde{\alpha}_r \\
    \widehat{\mT} & = \mH\widehat{\mP}, \\
    \widehat{\mP} & = \widehat{\mQ}_{HH}^{-1}\widehat{\mQ}_{HZ}\left[\widehat{\mQ}_{HZ}^\top\widehat{\mQ}_{HH}^{-1} \widehat{\mQ}_{HZ}^\top\right]^{-1} \\
    \widehat{\mQ}_{HH}^{-1} & = \left(n^{-1}\mH^\top\mH\right), \\
  \widehat{\mQ}_{HZ} & = \left(n^{-1}\mH^\top\mZ\right), \\
  \mZ & = \left(\mI - \widetilde{\lambda}\mM\right)\mZ, \\
  \widetilde{\alpha}_r & = - n^{-1}\left[\mZ^\top\left(\mA_r + \mA_r^\top\right)\widehat{\vepsi}\right] \\
  \widehat{\sigma}^2 & = n^{-1}\widehat{\vepsi}\widehat{\vepsi}, \\
  \widehat{\mu}^{(3)} & = n^{-1}\sum_{i = 1}^n\widehat{\epsilon}_i^3 , \\
  \widehat{\mu}^{(4)} & = n^{-1}\sum_{i = 1}^n\widehat{\epsilon}_i^4.
  \end{aligned}
\end{equation*}

For the \textbf{heteroskedastic} case the $r, s$ (with $r,s = 1,2$) element of $\widehat{\mPsi}^{\widehat{\lambda}\widehat{\lambda}}$ is given by:

\begin{equation}
  \widehat{\mPsi}^{\widehat{\lambda}\widehat{\lambda}}_{rs} = (2n)^{-1}\tr\left[\left(\mA_r + \mA_r^\top\right)\widehat{\mSigma}\left(\mA_s + \mA_s^\top\right)\widehat{\mSigma}\right]  + n^{-1}\widetilde{\va}_r^\top\widehat{\mSigma}\widetilde{\va}_s^\top,
\end{equation}
%
where, $\widehat{\mSigma}$ is a diagonal matrix whose $i$th diagonal element is $\widehat{\epsilon}_i^2$. 


%=============================================
\section{GMM Estimator for the SAC model}
%=============================================

To be added. 
% Consider the SAC model
% \begin{equation*}
% \begin{aligned}
%   \vy_n & = \rho_0\mW_n\vy_n + \mX_n\vbeta_0 + \vu_n, \\
%   \vu_n & = \lambda_0\mM_n\vu_n + \vepsi_n, 
% \end{aligned}
% \end{equation*}
% %
% where $\epsilon_{ni}, i = 1, \ldots, n$ of $\vepsi_n$ are i.i.d. $(0, \sigma^2_0)$ with zero mean and variance $\sigma_0^2$. Let $\vtheta = (\lambda, \rho, \vbeta^\top)^\top$, and denote at the true $\vtheta = \vtheta_0$
% \begin{equation*}
% \begin{aligned}
% \mS_0 & = \mI_n - \rho_0\mW_n, \\
% \mR_0 & = \mI_n - \lambda_0 \mM_n, \\
% \vepsi_n & = \mR_0\vu_n. 
% \end{aligned}
% \end{equation*}
% 
% The reduced form equation of this model can be written as
% \begin{equation*}
% \begin{aligned}
%  \vy_n & = \mS_0^{-1}\mX_n\vbeta_0 + \mS_0^{-1}\mR_0^{-1}\vepsi_n,  \\
%        & =  \mS_0^{-1}\mX_n\vbeta_0 + \mF_0^{-1}\vepsi_n, 
% \end{aligned}
% \end{equation*}
% %
% where $\mF_0 = \mR_0\mS_0$. The empirical moment conditions are
% \begin{equation*}
% \vg_n(\vtheta) = \begin{pmatrix}
%                   \left(\mP_{1n}\vepsi_n(\vtheta)\right)^\top\\
%                    \vdots \\
%                    \left(\mP_{mn}\vepsi_n(\vtheta)\right)^\top \\
%                    \mH_n^\top
%                  \end{pmatrix}\vepsi_n(\vtheta) 
%                  =
%                  \begin{pmatrix}
%                   \left(\mP_{1n}\mR_n(\lambda)\vu_n(\vtheta)\right)^\top\\
%                    \vdots \\
%                    \left(\mP_{mn}\mR_n(\lambda)\vu_n(\vtheta)\right)^\top \\
%                    \mH_n^\top
%                  \end{pmatrix}\mR_n(\lambda)\vu_n(\vtheta), 
% \end{equation*}
% %
% where $\mP_{jn}, j = 1, \ldots, m$ are from $\calP_{1n}$, the IV matrix $\mH_n$ is constructed from $\mX_n, \mW_n$ and $\mM_n$. 

%===========================
\section{Application in R}
%===========================

In this example we will use the \textbf{simulated} US Driving Under the Influence (DUI) county data set used in \cite{drukker2011command}. The dependent variable \code{dui} is defined as the alcohol-related arrest rate per 100,000 daily vehicle miles traveled (DVMT). The explanatory variables include 

\begin{itemize}
  \item \code{police}: number of sworn officers per 100,000 DVMT,
  \item \code{nondui}: non-alcohol-related arrests per 100,000 DVMT,
  \item \code{vehicles}: number of registered vehicles per 1,000 residents, and
  \item \code{dry}: a dummy for counties that prohibit alcohol sale within their borders
\end{itemize}

We load the required packages and dataset:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(}\hlstr{"maptools"}\hlstd{)}
\hlkwd{library}\hlstd{(}\hlstr{"spdep"}\hlstd{)}
\hlkwd{library}\hlstd{(}\hlstr{"sphet"}\hlstd{)}
\hlcom{# Load Data}
\hlstd{us_shape} \hlkwb{<-} \hlkwd{readShapeSpatial}\hlstd{(}\hlstr{"ccountyR"}\hlstd{)}  \hlcom{# Load shape file}
\end{alltt}


{\ttfamily\noindent\color{warningcolor}{\#\# Warning: shapelib support is provided by GDAL through the sf and terra packages among others}}

{\ttfamily\noindent\color{warningcolor}{\#\# Warning: shapelib support is provided by GDAL through the sf and terra paackages among others}}

{\ttfamily\noindent\color{warningcolor}{\#\# Warning: shapelib support is provided by GDAL through the sf and terra packages among others}}\begin{alltt}
\hlkwd{names}\hlstd{(us_shape)}                           \hlcom{# Names of variables in dbf}
\end{alltt}
\begin{verbatim}
## [1] "dry"      "nondui"   "vehicles" "elect"    "dui"      "police"
\end{verbatim}
\begin{alltt}
\hlcom{# Load weight matrix}
\hlstd{queen.w} \hlkwb{<-} \hlkwd{read.gal}\hlstd{(}\hlstr{"ccountyR_w.gal"}\hlstd{)}
\hlstd{lw} \hlkwb{<-} \hlkwd{nb2listw}\hlstd{(queen.w,} \hlkwc{style} \hlstd{=} \hlstr{"W"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}


\subsection{SAC Model with Homokedasticity (GS2SLS)}\index{GS2SLS!gstsls function}


First, we estimate the SAC model assuming homoskedasticity \citep{kelejian1998generalized} using the \code{gstsls} function from \pkg{spdep} package. We will also assume that $\mW = \mM$. The code is the following:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{GS2SLS} \hlkwb{<-} \hlkwd{gstsls}\hlstd{(dui} \hlopt{~} \hlstd{police} \hlopt{+} \hlstd{nondui} \hlopt{+} \hlstd{vehicles} \hlopt{+} \hlstd{dry,}
                 \hlkwc{data} \hlstd{= us_shape,}
                 \hlkwc{listw} \hlstd{= lw)}
\hlkwd{summary}\hlstd{(GS2SLS)}
\end{alltt}
\begin{verbatim}
## 
## Call:gstsls(formula = dui ~ police + nondui + vehicles + dry, data = us_shape, 
##     listw = lw)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -1.655535 -0.362165 -0.070363  0.277261  2.418849 
## 
## Type: GM SARAR estimator
## Coefficients: (GM standard errors) 
##                Estimate  Std. Error  z value  Pr(>|z|)
## Rho_Wy       0.04692763  0.01696580   2.7660  0.005675
## (Intercept) -6.40991922  0.41795923 -15.3362 < 2.2e-16
## police       0.59810726  0.01490338  40.1323 < 2.2e-16
## nondui       0.00024688  0.00108594   0.2273  0.820158
## vehicles     0.01571247  0.00066816  23.5160 < 2.2e-16
## dry          0.10608849  0.03492867   3.0373  0.002387
## 
## Lambda: 0.00095701
## Residual variance (sigma squared): 0.3175, (sigma: 0.56347)
## GM argmin sigma squared: 0.31789
## Number of observations: 3109 
## Number of parameters estimated: 8
\end{verbatim}
\end{kframe}
\end{knitrout}

The results show that all the variables are significant, except for \code{nondui}. Importantly, higher number of sworn officers is positively correlated with the DUI arrest rate, after controlling for \code{nondui}, \code{vehicles} and \code{dry}! The spatial autoregressive coefficient $\rho$ is positive and significant indicating autocorrelation in the dependent variable. \cite{drukker2011command} give some theoretical explanation of this results. On the one hand, the positive coefficient may be explained in terms of coordination effort among police departments in different countries. On the other hand, it might well be that an enforcement effort in one of the counties leads people living close to the border to drink in neighboring counties. The estimate is $\lambda$ negative, however the output does not produce inference for it. Lastly, it is important to stress that the standard errors has a degrees of freedom correction in the variance-covariance matrix. 

\subsection{SAC Model with Homokedasticity and Additional Endogeneity (GS2SLS)}\index{GS2SLS!spreg function}

The size of the \code{police} force may be related with the arrest rates \code{dui}. As a consequence, \code{police} produces endogeneity. We will use the dummy variable \code{elect}, where elect is 1 if a country government faces an election, 0 otherwise. To do so, we use the \code{spreg} function from \pkg{sphet}. Note that $\lambda$ is $\rho$. The estimate of $\rho$ is positive and significant thus indicating spatial autocorrelation in the dependent variable (coordination effort among police departments in different counties). 


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{G2SLS_en_in} \hlkwb{<-} \hlkwd{spreg}\hlstd{(dui} \hlopt{~} \hlstd{nondui} \hlopt{+} \hlstd{vehicles} \hlopt{+} \hlstd{dry,}
                \hlkwc{data} \hlstd{= us_shape,}
                \hlkwc{listw} \hlstd{= lw,}
               \hlkwc{endog} \hlstd{=} \hlopt{~} \hlstd{police,}
               \hlkwc{instruments} \hlstd{=} \hlopt{~} \hlstd{elect,}
               \hlkwc{model} \hlstd{=} \hlstr{"sarar"}\hlstd{,}
               \hlkwc{het} \hlstd{=} \hlnum{FALSE}\hlstd{,}
               \hlkwc{lag.instr} \hlstd{=} \hlnum{TRUE}\hlstd{)}
\hlkwd{summary}\hlstd{(G2SLS_en_in)}
\end{alltt}
\begin{verbatim}
## ====================================================
## ====================================================
##                GS2SLS SARAR Model
## ====================================================
## ====================================================
## 
## Call:
## spreg(formula = dui ~ nondui + vehicles + dry, data = us_shape, 
##     listw = lw, endog = ~police, instruments = ~elect, lag.instr = TRUE, 
##     model = "sarar", het = FALSE)
## 
## Residuals:
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -6.1862 -0.8838  0.0147 -0.0161  0.9213  8.3616 
## 
## Coefficients:
##                Estimate  Std. Error t-value  Pr(>|t|)    
## (Intercept) 11.60596811  1.66674437  6.9633 3.325e-12 ***
## nondui      -0.00019624  0.00275912 -0.0711  0.943299    
## vehicles     0.09299562  0.00564911 16.4620 < 2.2e-16 ***
## dry          0.39825983  0.09090201  4.3812 1.180e-05 ***
## police      -1.35130834  0.14101772 -9.5825 < 2.2e-16 ***
## lambda       0.19319018  0.04431011  4.3600 1.301e-05 ***
## rho         -0.08597523  0.03018333 -2.8484  0.004393 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Wald test that rho and lambda are both zero:
##  Statistics: 7.6185 p-val: 0.0057773
\end{verbatim}
\end{kframe}
\end{knitrout}




An important issue here is that \textbf{the optimal instrument are unknown}. It is not recommended the inclusion of the spatial lag of these additional exogenous variables in the matrix of instruments. However, results reported in ? do consider the spatial lags of \code{elect}. 

Now we assume that the error are heteroskedastic of unknown form. 


%-----------------------
\section{Exercises}
%-------------------

\begin{exercises}
    \exercise Consider the following model:
  		\begin{equation*}
  			\begin{aligned}
  				\vy & = \mX\vbeta + \vu \\
  				\vu & = \lambda\mW\vepsi + \vepsi
  			\end{aligned}
  		\end{equation*}
  		%
  		where $\left|\lambda\right| < 1$, $\vepsi$ has zero mean and variance $\sigma^2\mI_n$, respectively. Determine moment equations for a GMM approach you would use to estimate $\lambda$ and $\sigma^2$. (Hint: This model is known as the spatial moving average model for the error term).
  		\exercise Consider the following model:
        \begin{equation*}
        \begin{aligned}
        \vy  = \mX\vbeta + \rho_1\mW_1\vy + \rho_2\mW_2\vy+ \vepsi
        \end{aligned}
        \end{equation*}
        %
        where $\vepsi$ has zero mean and variance $\sigma^2\mI_n$, respectively, and $\mW_1$ and $\mW_2$ are observed exogenous weighting matrices. Suggest an instrumental variable estimation procedure for this model which accounts for the endogeneity of $\mW_1\vy$ and $\mW_2\vy$. 
        \exercise Consider the following model:
    \begin{equation*}
    	\begin{aligned}
    		\vy & = \mX\vbeta + \rho_1\mW_1\vy + \rho_2\mW_2\vy+ \vu \\
    		\vu & = \lambda\mM\vu + \vepsi
    	\end{aligned}
    \end{equation*}
    %
    where $\vepsi$ has zero mean and variance $\sigma^2\mI_n$, respectively, and $\mW_1$, $\mW_2$ and $\mM$ are observed exogenous weighting matrices. Suggest an instrumental variable estimation procedure for this model which accounts for the endogeneity of $\mW_1\vy$ and $\mW_2\vy$, as well as for the spatially correlated term. 
\end{exercises}   


\section*{Appendix}


\begin{subappendices}

%-----------------------------------------------------------------
\section{Asymptotic Distribution of GMME for SEM Model}
%-----------------------------------------------------------------

The error term for the SEM model can be expressed as:
\begin{equation*}
\begin{aligned}
\vepsi_n(\vtheta) & = \left(\mI_n - \lambda\mM_n\right)\vu_n, \\
& = \mR_n\left(\vy_n - \mX\vbeta\right), \\
& = \mR_n\left(\mX_n\vbeta_0 + \mR_0^{-1}\vepsi_n - \mX\vbeta\right),\quad \mbox{replacing $\vy_n$} \\
& = \mR_n\mX_n\vbeta_0 + \mR_n\mR_0^{-1}\vepsi_n - \mR_n\mX_n\vbeta, \\
& = \mR_n\mX_n(\vbeta_0 - \vbeta) + \mR_n\mR_0^{-1}\vepsi_n, \\
& = \mR_n\mX_n(\vbeta_0 - \vbeta) + \left(\mI_n + (\lambda_0 - \lambda)\mQ_0\right)\vepsi_n, \quad \mbox{using the result in \eqref{eq:expansion-S-2}} \\
& = \mR_n\vd_n(\vbeta) + \left(\mI_n + (\lambda_0 - \lambda)\mQ_0\right)\vepsi_n,
\end{aligned}
\end{equation*}
%
where in this case $\mQ_0 = \mM_n\mR_0^{-1}$. 

We next find $\mD_n$. For the first element of the Jacobian, we note that $\mR_n = \mR_0 + (\lambda_0 - \lambda)\mW_n$:
\begin{equation*}
\frac{1}{n}\mX_n^\top\mR_n\mX_n \pto \frac{1}{n}\mX_n^\top\mR_0\mX_n +\frac{1}{n}(\lambda_0 - \lambda)\mX^\top_n\mM_n\mX_n^\top.
\end{equation*}

In addition, 
\begin{equation*}
\begin{aligned}
  \frac{1}{n}\mX_n^\top\mM_n\vu_n & = \frac{1}{n}\mX_n^\top\mM_n\mR_0^{-1}\vepsi_n, \\
                                  & = \frac{1}{n}\mX_n^\top\mM_n\mR_0^{-1}\left[\mR_n\mX_n(\vbeta_0 - \vbeta) + \left(\mI_n + (\lambda_0 - \lambda)\mQ_0\right)\vepsi_n\right] \\
                                  & = \frac{1}{n}\mX_n^\top\mM_n\mR_0^{-1}\mR_n\mX_n(\vbeta_0 - \vbeta) + \frac{1}{n}(\lambda_0 - \lambda)\mX_n^\top\mM_n\mR_0^{-1}\mQ_0\vepsi_n, \\
                                  & \pto \frac{1}{n}\mX_n^\top\mM_n\mR_0^{-1}\mR_n\mX_n(\vbeta_0 - \vbeta) + \frac{1}{n}(\lambda_0 - \lambda)\mX_n^\top\mM_n\mR_0^{-1}\mQ_0\E(\vepsi_n), \\
                                  & \pto \frac{1}{n}\mX_n^\top\mM_n\mR_0^{-1}\mR_n\mX_n(\vbeta_0 - \vbeta).
\end{aligned}
\end{equation*}

Similarly, 
\begin{equation}
\begin{aligned}
\frac{1}{n}\vepsi_n^\top \mP_{jn}^s\mM_n\vu_n & = \frac{1}{n}\left(\mR_n\mX_n(\vbeta_0 - \vbeta) + \mR_n\mR_0^{-1}\vepsi_n\right)^\top\mP_{jn}^s\mM_n\vu_n, \\
& = \frac{1}{n}(\vbeta_0 - \vbeta)^\top\mX_n^\top\mR_n^\top \mP_{jn}^s\mM_n\vu_n + \frac{1}{n}\vepsi_n^\top\mR_0^{-1\top}\mR_n\mP_{jn}^s\mM_n\vu_n.
\end{aligned}
\end{equation}

For the first element
\begin{equation*}
\begin{aligned}
  \frac{1}{n}(\vbeta_0 - \vbeta)^\top\mX_n^\top\mR_n^\top \mP_{jn}^s\mM_n\vu_n & = \frac{1}{n}(\vbeta_0 - \vbeta)^\top\mX_n^\top\mR_n^\top \mP_{jn}^s\mM_n\mR_0^{-1}\vepsi_n,\\
  & \pto \frac{1}{n}(\vbeta_0 - \vbeta)^\top\mX_n^\top\mR_n^\top \mP_{jn}^s\mM_n\mR_0^{-1}\E(\vepsi_n), \\
  & \pto \vzeros.
  \end{aligned}
\end{equation*}

For the second element
\begin{equation*}
\begin{aligned}
  \frac{1}{n}\vepsi_n^\top\mR_0^{-1\top}\mR_n\mP_{jn}^s\mM_n\vu_n & = \frac{1}{n}\vepsi_n^\top\mR_0^{-1\top}\mR_n\mP_{jn}^s\mM_n\mR_0^{-1}\vepsi_n,  \\
  = & \frac{1}{n}\vepsi_n^\top\left[\mI_n + (\lambda_0-\lambda)\mQ_0\right]^\top\mP_{jn}^s\mM_n\mR_0^{-1}\vepsi_n, \\
  & = \frac{1}{n}\vepsi_n^\top\mP_{jn}^s\mQ_0\vepsi_n + (\lambda_0 - \lambda)\frac{1}{n}\vepsi_n^\top\mQ_0^\top\mP_{jn}^s\mQ_0\vepsi_n, \\
  & \pto \frac{1}{n}\E\left(\vepsi_n^\top\mP_{jn}^s\mQ_0\vepsi_n\right)  + \frac{1}{n} (\lambda_0 - \lambda) \E\left(\vepsi_n^\top\mQ_0^\top\mP_{jn}^s\mQ_0\vepsi_n \right),\\
  & \pto \sigma_0^2 \tr\left(\mP_{jn}^{s}\mQ_0\right) + (\lambda_0- \lambda)\frac{\sigma_0^2}{n}\tr\left(\mQ_0^\top\mP_{jn}^s\mG_0\right).
\end{aligned}
\end{equation*}

Now:
\begin{equation*}
\begin{aligned}
\frac{1}{n}\vepsi_n^\top\mP_{jn}^s\mR_n\mX_n & = \frac{1}{n}\left(\mR_n\mX_n(\vbeta_0 - \vbeta) + \mR_n\mR_0^{-1}\vepsi_n\right)^\top\mP_{jn}^s\mR_n\mX_n, \\
& = \frac{1}{n}(\vbeta_0 - \vbeta)^\top\mX_n^\top\mR_n^\top\mP_{jn}^s\mR_n\mX_n + \frac{1}{n}\vepsi_n^\top\left(\mR_n\mR_0^{-1}\right)^\top\mP_{jn}^s\mR_n\mX_n, \\
& = \frac{1}{n}(\vbeta_0 - \vbeta)^\top\mX_n^\top\mR_n^\top\mP_{jn}^s\mR_n\mX_n + \frac{1}{n}\vepsi_n^\top\left(\mI_n + (\lambda_0 - \lambda)\mQ_0\right)^\top\mP_{jn}^s\mR_n\mX_n, \\
& \pto \frac{1}{n}(\vbeta_0 - \vbeta)^\top\mX_n^\top\mR_n^\top\mP_{jn}^s\mR_n\mX_n.
\end{aligned}
\end{equation*}


As a result, 
\begin{equation}
\begin{aligned}
\frac{1}{n}\frac{\partial \E\left[\vg_n(\vtheta)\right]}{\partial \vtheta^\top} & = \begin{pmatrix}
  \frac{1}{n}\mX_n^\top\mR_0\mX_n +\frac{1}{n}(\lambda_0 - \lambda)\mX^\top_n\mM_n\mX_n^\top & \frac{1}{n}\mX_n^\top\mM_n\mR_0^{-1}\mR_n\mX_n(\vbeta_0 - \vbeta) \\
  \frac{1}{n}(\vbeta_0 - \vbeta)^\top\mX_n^\top\mR_n^\top\mP_{1n}^s\mR_n\mX_n & \sigma_0^2 \tr\left(\mP_{jn}^{s}\mQ_0\right) + (\lambda_0- \lambda)\frac{\sigma_0^2}{n}\tr\left(\mG_0^\top\mP_{jn}^s\mQ_0\right) \\
  \frac{1}{n}(\vbeta_0 - \vbeta)^\top\mX_n^\top\mR_n^\top\mP_{2n}^s\mR_n\mX_n & \sigma_0^2 \tr\left(\mP_{jn}^{s}\mQ_0\right) + (\lambda_0- \lambda)\frac{\sigma_0^2}{n}\tr\left(\mG_0^\top\mP_{jn}^s\mQ_0\right)
\end{pmatrix}
\end{aligned}
\end{equation}

At $\vtheta = \vtheta_0$,
\begin{equation*}
\underset{(k + 2)\times (k + 1)}{\mD_n} = 
\begin{pmatrix}
  \underset{k\times k}{\mX_n^\top\mR_0\mX_n} & \underset{k\times 1}{\vzeros} \\
  \underset{1 \times k}{\vzeros} & \underset{1\times 1}{\sigma_0^2 \tr\left(\mP_{1n}^{s}\mQ_0\right)}\\
  \underset{1\times k}{\vzeros} & \underset{1\times 1}{\sigma_0^2 \tr\left(\mP_{2n}^{s}\mQ_0\right)}
\end{pmatrix}
\end{equation*}


%-----------------------------------------------------------------
\section{Proof Theorem 3 in KP 1998}
%-----------------------------------------------------------------

Recall that the GS2SLS is given by:

\begin{equation}
  \widehat{\vdelta}_n = \left[\widehat{\mZ}_s(\lambda)^\top\widehat{\mZ}_s(\lambda)\right]^{-1}\widehat{\mZ}_s(\lambda)^\top\vy_s(\lambda)
\end{equation}

Whereas, the FGS2SLS is given by:

\begin{equation}
  \widehat{\vdelta}_{F, n} = \left[\widehat{\mZ}_s(\widehat{\lambda})^\top\widehat{\mZ}_s(\widehat{\lambda})\right]^{-1}\widehat{\mZ}_s(\widehat{\lambda})^\top\vy_s(\widehat{\lambda})
\end{equation}
%
where

\begin{equation}
  \begin{aligned}
    \widehat{\mZ}_s(\widehat{\lambda}_n) & = \mP_{H_n}\mZ_s(\widehat{\lambda}_n) \\
    \mZ_s(\widehat{\lambda}_n) & = \mZ_n - \widehat{\lambda}_n\mM_n\mZ_n \\
    \vy_s(\widehat{\lambda}_n) & = \vy_n - \widehat{\lambda}_n\mM_n\vy_n \\
    \widehat{\mZ}_s(\widehat{\lambda}_n) & = \left(\mX_n - \widehat{\lambda}_n\mM_n\mX_n, \mW_n\vy_n - \widehat{\widehat{\lambda}_n\mM_n}\mW_n\vy_n\right) \\
    \widehat{\widehat{\lambda}_n\mM_n}\mW_n\vy_n & = \mP_{H_n}\left(\mW_n\vy_n - \widehat{\lambda}_n\mM_n\mW_n\vy_n\right).
 \end{aligned}
\end{equation}

The sampling error is:

\begin{equation}
  \widehat{\vdelta}_{F, n} - \vdelta = \left[\widehat{\mZ}_s(\widehat{\lambda})^\top\widehat{\mZ}_s(\widehat{\lambda})\right]^{-1}\widehat{\mZ}_s(\widehat{\lambda})^\top\vu_s(\widehat{\lambda}_n)
\end{equation}
%
where:

\begin{equation}
  \begin{aligned}
    \vu_s(\widehat{\lambda}_n) & = (\mI - \widehat{\lambda}_n)\vu \\
                               & = (\mI - \widehat{\lambda}_n)\vu + \vepsi_n - \vepsi_n \\
                               & = \vepsi_n  + (\mI - \widehat{\lambda}_n\mM_n)\vu - (\mI - \lambda\mM_n)\vu \\
                               & = \vepsi_n  + \vu  - \widehat{\lambda}_n\mM_n\vu - \vu + \lambda\mM_n\vu \\
                               & = \vepsi_n - \left(\widehat{\lambda}_n- \lambda\right)\mM_n\vu_n
  \end{aligned}
\end{equation}

Then:

\begin{equation}
  \begin{aligned}
    \widehat{\vdelta}_{F, n} - \vdelta & = \left[\widehat{\mZ}_s(\widehat{\lambda})^\top\widehat{\mZ}_s(\widehat{\lambda})\right]^{-1}\widehat{\mZ}_s(\widehat{\lambda})^\top\left[\vepsi_n - \left(\widehat{\lambda}_n- \lambda\right)\mM_n\vu_n\right] \\
     & = \left[\widehat{\mZ}_s(\widehat{\lambda})^\top\widehat{\mZ}_s(\widehat{\lambda})\right]^{-1}\widehat{\mZ}_s(\widehat{\lambda})^\top\vepsi_n - \left[\widehat{\mZ}_s(\widehat{\lambda})^\top\widehat{\mZ}_s(\widehat{\lambda})\right]^{-1}\widehat{\mZ}_s(\widehat{\lambda})^\top\left(\widehat{\lambda}_n- \lambda\right)\mM_n\vu_n \\
     & = \left[\frac{1}{n}\widehat{\mZ}_s(\widehat{\lambda})^\top\widehat{\mZ}_s(\widehat{\lambda})\right]^{-1}\frac{1}{n}\widehat{\mZ}_s(\widehat{\lambda})^\top\vepsi_n - \left[\frac{1}{n}\widehat{\mZ}_s(\widehat{\lambda})^\top\widehat{\mZ}_s(\widehat{\lambda})\right]^{-1}\left(\widehat{\lambda}_n- \lambda\right)\frac{1}{n}\widehat{\mZ}_s(\widehat{\lambda})^\top\mM_n\vu_n \\
 \sqrt{n} (\widehat{\vdelta}_{F, n} - \vdelta)   & = \left[\frac{1}{n}\widehat{\mZ}_s(\widehat{\lambda})^\top\widehat{\mZ}_s(\widehat{\lambda})\right]^{-1}\frac{1}{\sqrt{n}}\widehat{\mZ}_s(\widehat{\lambda})^\top\vepsi_n - \left[\frac{1}{n}\widehat{\mZ}_s(\widehat{\lambda})^\top\widehat{\mZ}_s(\widehat{\lambda})\right]^{-1}\left(\widehat{\lambda}_n- \lambda\right)\frac{1}{\sqrt{n}}\widehat{\mZ}_s(\widehat{\lambda})^\top\mM_n\vu_n
  \end{aligned}
\end{equation}

By consistency $\widehat{\lambda}_n- \lambda = o_p(1)$. Now, we need to show that: 

\begin{equation}\label{eq:proofT3_1}
  \frac{1}{n}\widehat{\mZ}_s(\widehat{\lambda})^\top\widehat{\mZ}_s(\widehat{\lambda})\pto \frac{1}{n}\widehat{\mZ}_s(\lambda)^\top\widehat{\mZ}_s(\lambda) = \bar{\mQ}
\end{equation}

\begin{equation}\label{eq:proofT3_2}
  \frac{1}{\sqrt{n}}\widehat{\mZ}_s(\widehat{\lambda})^\top\vepsi_n\dto \rN(\vzeros, \sigma^2_{\epsilon}\bar{\mQ}),
\end{equation}

\begin{equation}\label{eq:proofT3_3}
\left(\widehat{\lambda}_n- \lambda\right)\frac{1}{\sqrt{n}}\widehat{\mZ}_s(\widehat{\lambda})^\top\mM_n\vu_n \pto 0
\end{equation}
%
where:

\begin{equation}
  \bar{\mQ} = \left[\mQ_{HZ} - \lambda \mQ_{mHZ}\right]^\top \mQ_{HH}^{-1}\left[\mQ_{HZ} - \lambda \mQ_{mHZ}\right]
\end{equation}
%
is finite and nonsingular. For \ref{eq:proofT3_1}, note that:

\begin{equation}
\begin{aligned}
  \frac{1}{n}\widehat{\mZ}_s(\widehat{\lambda})^\top\widehat{\mZ}_s(\widehat{\lambda}) & = \frac{1}{n}\left(\mZ_n - \widehat{\lambda}_n\mM_n\mZ_n\right)^\top\mP_{H_n}\left(\mZ_n - \widehat{\lambda}_n\mM_n\mZ_n\right) \\
  & =  \frac{1}{n}\left(\mZ_n - \widehat{\lambda}_n\mM_n\mZ_n\right)^\top\mH_n\left(\mH_n^\top\mH\right)^{-1}\mH_n^\top\left(\mZ_n - \widehat{\lambda}_n\mM_n\mZ_n\right) \\
  & =  \left(\underbrace{\frac{1}{n}\mZ_n^\top \mH_n}_{\pto \mQ_{HZ}^\top} - \underbrace{\widehat{\lambda}_n}_{\pto \lambda}\frac{1}{n}\underbrace{\mZ_n^\top\mM_n^\top\mH_n}_{\pto \mQ_{HMZ}^\top}\right)\underbrace{\left(\frac{1}{n}\mH_n^\top\mH\right)^{-1}}_{\to \mQ_{HH}^{-1}}\underbrace{\left(\frac{1}{n}\mH_n^\top\mZ_n  - \widehat{\lambda}_n\frac{1}{n}\mH_n^\top\mM_n\mZ_n\right)}_{\pto \mQ_{HZ}-\lambda\mQ_{HMZ}}
\end{aligned}
\end{equation}

For \ref{eq:proofT3_2}, note that:

\begin{equation}
\begin{aligned}
   \frac{1}{\sqrt{n}}\widehat{\mZ}_s(\widehat{\lambda})^\top\vepsi_n & = \frac{1}{\sqrt{n}}\left(\mZ_n -\widehat{\lambda}_n\mM_n\mZ_n\right)^\top\mP_{H_n}\vepsi \\
                                                                     & = \left(\underbrace{\frac{1}{n}\mZ_n^\top \mH_n}_{\pto \mQ_{HZ}^\top} - \underbrace{\widehat{\lambda}_n}_{\pto \lambda}\frac{1}{n}\underbrace{\mZ_n^\top\mM_n^\top\mH_n}_{\pto \mQ_{HMZ}^\top}\right)\underbrace{\left(\frac{1}{n}\mH_n^\top\mH\right)^{-1}}_{\to \mQ_{HH}^{-1}}\underbrace{\frac{1}{\sqrt{n}}\mH_n^\top\vepsi_n}_{\dto \rN(\vzeros, \sigma^2_{\epsilon}\mQ_{HH})}
\end{aligned}
\end{equation}

For \ref{eq:proofT3_3} note that:

\begin{equation}
    \left(\widehat{\lambda}_n- \lambda\right)\frac{1}{\sqrt{n}}\widehat{\mZ}_s(\widehat{\lambda})^\top\mM_n\vu_n  = \underbrace{\left(\widehat{\lambda}_n- \lambda\right)}_{o_p(1)}\left(\underbrace{\frac{1}{n}\mZ_n^\top \mH_n}_{\pto \mQ_{HZ}^\top} - \underbrace{\widehat{\lambda}_n}_{\pto \lambda}\frac{1}{n}\underbrace{\mZ_n^\top\mM_n^\top\mH_n}_{\pto \mQ_{HMZ}^\top}\right)\underbrace{\left(\frac{1}{n}\mH_n^\top\mH\right)^{-1}}_{\to \mQ_{HH}^{-1}}\frac{1}{\sqrt{n}}\mH_n^\top\mM_n\vu_n
\end{equation}

Note that $\E\left(n^{-1/2}\mH_n^\top\mM_n\vu_n\right) = 0$ and $\E\left(n^{-1}\mH_n^\top\mM_n\vu_n\vu^\top_n\mM_n^\top\mH^\top_n\right) = n^{-1} \mH_n^\top\mM_n\mSigma_{u_n}\mM_n^\top\mH^\top_n$, whose elements are bounded, where

\begin{equation*}
\mSigma_{u_n} = \sigma^2_{\epsilon}\left(\mI - \lambda\mM_n\right)^{-1}\left(\mI - \lambda\mM_n^\top\right)^{-1}
\end{equation*}

Then $\frac{1}{\sqrt{n}}\mH_n^\top\mM_n\vu_n = O_p(1)$ and finally

\begin{equation}
   \sqrt{n} (\widehat{\vdelta}_{F, n} - \vdelta)\dto \rN(\vzeros, \sigma^2_{\epsilon}\bar{\mQ}^{-1})
\end{equation}

The small sample approximation is

\begin{equation}
  \widehat{\vdelta}_{F, n} \sim \rN\left(\vdelta, \widehat{\sigma}^2\left[\widehat{\mZ}_s(\widehat{\lambda})^\top\widehat{\mZ}_s(\widehat{\lambda})\right]^{-1}\right)
\end{equation}
%
where:

\begin{equation}
  \widehat{\sigma}^2 = \widehat{\vepsi}^\top\widehat{\vepsi} / n
\end{equation}

and $\widehat{\vepsi} = \vy_s(\widehat{\lambda}) - \mZ_{s}(\widehat{\lambda})\widehat{\vdelta}_{F}$.

\end{subappendices}








