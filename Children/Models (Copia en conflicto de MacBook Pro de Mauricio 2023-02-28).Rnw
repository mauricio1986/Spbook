%==============================
\chapter{Spatial Models}
%==============================

Previously, we have reviewed some preliminary concepts in spatial econometric, such as spatial dependency and spatial autocorrelation. In this chapter we will analyze the formulation of spatial models. In particular, in Section \ref{sec:taxonomy} we will derive a complete taxonomy of spatial models including the Spatial Lag Model, Spatial Durbin Model, Spatial Error Model and the Spatial Autocorrelation Model. We will give a brief motivation of each of them and some examples.  In Section \ref{sec:interpretation} we show how to understand the `spillover' effects and how to interpret marginal effects in the spatial model framework. 

%*******************************
\section{Taxonomy of Models}\label{sec:taxonomy}
%*******************************

As we showed in previous chapter (in particular in Section~\ref{sec:spatial_dependence}), it is no possible to model spatial dependencies using traditional econometrics. The main problem is that we have more parameters than observations. However, using the spatial weight matrix we can solve that problem by reducing the number of parameters to just one parameter using the weighted average of the $\vy$ values in the neighborhood of $i$. 

\subsection{Spatial Lag Model}\index{Spatial lag model}

So, given the problem of insufficient degree of freedom, how to model a situation where the dependent variable depends also of the spatially lagged variable? Instead of using a full system of equation, we can model the spatial dependency as:

\begin{equation}\label{eq:slm_model_1}
y_i = \alpha +\rho \sum_{j = 1}^n w_{ij}y_j + \epsilon_i, \quad i = 1,...,n,
\end{equation}
%
where $w_{ij}$ is the $(i,j)$th element of the spatial weight matrix $\mW$ matrix (see Definition \ref{def:W}); $y_i$ is the dependent variable for spatial unit $i$, so that $\sum_{j = 1}^n w_{ij}y_j$ is the weighted average of the dependent variable for the neighbors of $i$ (or spatial lag); $\epsilon_i$ is the error term such that $\E(\epsilon_i)= 0$; and $\rho$ is the spatial autoregressive parameter which measures the intensity of the spatial interdependence: $\rho > 0$ indicates a positive spatial dependence, whereas $\rho < 0$ indicates a negative spatial dependence. It should be clear that if $\rho = 0$, then we have the traditional linear regression model. By including a spatially lagged variable we are making explicit the the existence of spatial spillovers effects due to, for example, geographical proximity. This data generating process is known as a \emph{Spatial Autoregressive Process}\index{Spatial autoregressive process} or also labeled as SAR or \emph{Spatial Lag Model} SLM. Since the model (\ref{eq:slm_model_1}) does not include explanatory variables the model is known as the pure SLM or SAR model. 

\begin{figure}[ht]

	\caption{The SLM for two tegions}\label{figure:slm}
		\centering
		\begin{tikzpicture}
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (y1) at (-1,0) {$y_1$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (x1) at (-1,2) {$\vx_1$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (e1) at (-1,-2) {$\epsilon_1$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (y2) at (1, 0) {$y_2$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (x2) at (1,2) {$\vx_2$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (e2) at (1,-2) {$\epsilon_2$};
			
			\draw[->, solid] (x1) to (y1);
			\draw[->, solid] (x2) to (y2);
			\draw[->, solid] (e1) to (y1);
			\draw[->, solid] (e2) to (y2);
			\draw[dashed, ->] (y1) to[bend right] (y2);
			\draw[dashed, ->] (y2) to[bend right] (y1);

			% Legend
       \node (leyend) at (4, 0){
    \begin{tabular}{l@{: }l}
      \multicolumn{2}{c}{\textbf{Effects}}              \\
      $\rightarrow$                        & non-spatial effects    \\
      $\dashrightarrow$                     & spatial effects  \\
      \end{tabular}
      };
		\end{tikzpicture}
\end{figure}

Figure \ref{figure:slm}	represents the spatial autoregressive model in \eqref{eq:slm_model_1} for two regions. The variables $(x_1, x_2)$ and unobserved terms $(\epsilon_1, \epsilon_2)$ have a direct effect on $y$ for both regions. Note that the model incorporates spatial spillover effects by the effect of $y_1$ on $y_2$ and vice versa. That is, the model reflects the \emph{`simultaneity'} inherent in spatial autocorrelation\index{Spillover effects}.  

The model can also be  written in vector form as

\begin{equation*}
y_i = \alpha + \rho \underset{(1\times n)}{\vw_i}^\top \underset{(n\times 1)}{\vy} + \epsilon_i, \quad i = 1,...,n,   
\end{equation*}
%
where $\vw_i$ is the $i$th row of $\mW$. A full SLM specification with covariates in matrix form can be written as:

\begin{equation}\label{eq:Sar}
\vy  =  \alpha \vones_n+ \rho \mW\vy + \mX\vbeta + \vepsi,
\end{equation}
%
where $\vy$ is a $n\times 1$ vector of observations on the dependent variable; $\mX$ is an $n\times k$ matrix of observations on the explanatory variables; $\vbeta$ is the $k\times 1$ vector of parameters and $\alpha$ is the constant; and $\vones_n$ is a $n\times 1$ vector of ones.

%-------------------------------------------------------------------------
\subsubsection{Reduced Form and Parameter Space}\index{Reduced form!Spatial lag model}
%-------------------------------------------------------------------------

An important concept in the context of spatial models is the difference between structural and reduced form model. Roughly, the reduced form of a model is the one which the endogenous variables are expressed as functions of the exogenous variables. The structural form is the `behavioral model' that relates the variables. For example, Equation (\ref{eq:Sar}) is the structural model for the SLM, which relates the all the (exogenous and endogenous) variables with the dependent variable $\vy$. However, a better way of think about the model is how the dependent variable is generated. This is the so-called data generating process (DGP). If we solve the system (\ref{eq:Sar}) for the endogenous variables, $\vy$, we will obtain the reduced-form model. Thus, the implied 
DGP or ``reduced form equation'' for the SLM given in Equation (\ref{eq:Sar}) is:

\begin{equation}\label{eq:Sar_dgp}
\vy  =  \left(\mI_n - \rho \mW\right)^{-1}\left(\alpha \vones_n + \mX\vbeta\right) + \left(\mI_n - \rho \mW\right)^{-1}\vepsi,
\end{equation}
%
which no longer contains any spatially lagged dependent variable on the right-hand size. The Equation (\ref{eq:Sar_dgp}) expresses the simultaneous nature of the spatial autoregressive process.

\begin{remark}
The \textbf{reduced form} of a system of equations is the result of solving the system for the \textbf{endogenous variables}. This gives the latter as functions of the exogenous variables, if any. For example, the general expression of a structural form is $f(\vy, \mX, \vepsi) = \vzeros$, whereas the reduced form of this model is given by $\vy = g(\mX, \vepsi)$, with $g$ as function. 
\end{remark}

Without restrictions on $(\mI_n-\rho\mW)$---and $\left(\alpha \vones_n + \mX\vbeta\right)$---the coefficients cannot be identified from data. In other words, in order to obtain the reduced form we need $(\mI_n-\rho\mW)$ to be invertible. From standard algebra theory a matrix $\mA$ is invertible if $\det(\mA)\neq 0$. Thus, we require that $\det(\mI_n -\rho\mW)\neq 0$. The question is: which values of $\rho$ lead to non-singular $(\mI_n -\rho\mW)$? For \textbf{symmetric} matrices, the compact open interval for $\rho \in \left( \omega_{min}^{-1}, \omega_{max}^{-1}\right)$ will lead to a symmetric positive definite $(\mI_n -\rho\mW)$, where $\omega_{min}$ and $\omega_{max}$ are the minimum and maximum eigen value of $\mW$, respectively. This gives rise to the following Theorem:

\begin{theorem}[Invertibility]\index{Weight matrix!Invertibility}\index{eigen values}
Let $\mW$ be a weighting matrix, such that $w_{ii} = 0$ for all $i = 1,...,n$, and assume that all of the roots of $\mW$ are real. Assume also that $\mW$ is not row normalized. Let $\omega_{min}$ and $\omega_{max}$ be the minimum and maximum eigen value of $\mW$. Assume also that $\omega_{max} > 0$ and $\omega_{min} < 0$. Then $\left(\mI_n - \rho\mW\right)$ is nonsingular for all:

\begin{equation*}
  \omega_{min}^{-1} < \rho < \omega_{max}^{-1}
\end{equation*}
\end{theorem}

\begin{remark}
The roots of a non-symmetric matrix will typically not all be real, e.g., some will be complex
\end{remark}

Recall that for ease of interpretation, it is common practice to normalize $\mW$ such that the elements of each row sum to unity. Since $\mW$ is nonnegative, this ensures that all weights are between 0 and 1, and has the effect that the weighting operation can be interpreted as an averaging of neighboring values. 

According to our Theorem \ref{teo:eigen_values} (Eigenvalues of row-stochastic Matrix) the eigenvalues of the row-stochastic (i.e., row-normalized) neighborhood matrix $\mW$ are in the range $\left[-1, +1\right]$. In this case $\rho \in (-1, 1)$, however it is misleading to consider $\rho$ as a conventional correlation coefficient vector between $\vy$ and its spatial lag $\mW\vy$. This is only the result of considering the standard row-standardized matrix. Other standardization methods will lead to other potential parameter space of $\rho$. 


\begin{theorem}[Invertibility of Row-Normalized $\mW$ matrix]
If $\mW$ is row-normalized, then  $\left(\mI_n - \rho\mW\right)^{-1}$ exists for all $\left|\rho \right| < 1$
\end{theorem}

In spite of its popularity, row-normalized weighting has it drawbacks. As we suggested in the remark in Section \ref{sec:spatial_lag_var}, row normalization alters the internal weighting structure of $\mW$ so that comparisons between rows become somewhat problematic. In view of this limitation, it is natural to consider simple scalar normalization which multiply $\mW$ by a single number, say $a \cdot \mW$, which removes any measure-unit effect but preserves relations between all rows of $\mW$. 

In particular, let

\begin{equation*}
  \begin{aligned}
    a & = \min \left\lbrace r, c \right\rbrace \\
    r & = \max_i \sum_j \left|w_{ij}\right|\quad \mbox{maximal row sum of the absolute values} \\
    c & = \max_j \sum_i \left|w_{ij}\right|\quad \mbox{maximal column sum of the absolute values}.
  \end{aligned}
\end{equation*}

Then, assuming that the elements of $\mW$ are nonegative, $(\mI_n - \rho\mW)$ will be nonsingular for all $\left|\rho\right| < 1/a$. Note that this normalization has the advantage of ensuring that the resulting spatial weights, $w_{ij}$, are all between 0 and 1, and hence can still be interpreted as relative influence intensities. This could be taken as the parameter space.\index{Parameter space} 

This is an important result because a model which has a weighting matrix which is not row normalized can always be normalized in such a way that the inverse needed to solve the model will exists in an easily established region. 


\begin{remark}
For further details on normalizing $\mW$ and the parameter space of $\rho$ see \citet[][section 2.4]{elhorst2014spatial} and \citet[][section 2.2]{kelejian2010specification}
\end{remark}

When $\left|\rho\right|< 1$, $\left(\mI_n - \rho\mW\right)^{-1}$ implies an infinite series also called the Leontief expansion. An approximation for this serie is given in the following Lemma.

\begin{lemma}[Leontief Expansion]\label{lemma:Leontief}\index{Leontief expansion}
If $\left|\rho\right|  < 1$, then
	\begin{equation*}
	(\mI - \rho \mW)^{-1} = \sum_{i = 0} ^{\infty}(\rho \mW)^{i}
	\end{equation*}
\end{lemma}

Then, using Lemma \ref{lemma:Leontief} (Leontief Expansion), the reduced model in Equation \eqref{eq:Sar_dgp} can be written as:

\begin{equation}\label{eq:infinite_series_slm}
  \begin{aligned}
      \vy  = &\left(\mI_n + \rho\mW + \rho^2\mW^2 + ...\right)\left(\alpha \vones_n + \mX\vbeta\right) + \left(\mI_n + \rho\mW + \rho^2\mW^2 + ...\right)\vepsi, \\
           = & \alpha\vones_n + \rho\mW\vones_n\alpha + \rho^2\mW^2\vones_n\alpha + ... + \mX\vbeta + \rho\mW\mX\vbeta + \rho^2\mW^2\mX\vbeta + ... \\
          & + \vepsi + \rho\mW\vepsi + \rho^2\mW^2\vepsi.
  \end{aligned}
\end{equation}

Expression (\ref{eq:infinite_series_slm}) can be simplified since the infinite series:

\begin{equation*}
\alpha\vones_n + \rho\mW\vones_n\alpha + \rho^2\mW^2\vones_\alpha + ... \to \frac{\vones_n\alpha}{\left(1 - \rho\right)},
\end{equation*}
%
since $\alpha$ is a scalar, the parameter $\left|\rho\right|  < 1$, and $\mW$ is row-stochastic. By definition $\mW\vones_n = \vones_n$ and therefore $\mW\mW\vones_n = \mW\vones_n = \vones$. Consequently, $\mW^l\vones_n = \vones_n$ for $l\geq 0$ (recall that $\mW^0 = \mI_n$). This allows us to write:

\begin{equation*}
\vy = \frac{1}{(1-\rho)}\vones_n\alpha + \mX\vbeta + \rho\mW\mX\vbeta + \rho^2\mW^2\mX\vbeta + ... + \vepsi + \rho\mW\vepsi + \rho^2\mW^2\vepsi + ...
\end{equation*}

This expression allows defining two effects: a multiplier effect affecting the explanatory variables and a spatial diffusion effect affecting the error terms. With respect to the explanatory variables, this expression means that, on average, the value of $\vy$ at one location $i$ is not only explained by the values of the explanatory variables associated to this location but also by those associated to all other locations (neighbors or not) via the inverse spatial transformation $\left(\mI_n - \rho \mW\right)^{-1}$. This spatial multiplier effect decreases with distance. This can be seen if we consider the powers of $\mW$ in the series expansion of $\left(\mI_n - \rho \mW\right)^{-1}$.\index{Multiplier effect} 

With respect to the error process, this expression means that a random (unobserved) shock in a location $i$ not only affects the value of $y$ in this location but also has an impact on the values of $y$ in all other locations via the same spatial inverse transformation. To see this, recall that $\mW^2$ will reflect second-order contiguous neighbors, those that are neighbors to the first-order neighbors (review Section \ref{sec:HSO}). Since the neighbor of the neighbor (second-order neighbor) to an observation $i$ includes observation $i$ itself, $\mW^2$ has positive elements on the diagonal when each observations has at least one neighbor. That is, higher-order spatial lags can lead to a connectivity relation for an observations $i$ such that $\mW\vepsi$ will extract observations from the vector $\vepsi$ that point back to the observation $i$ itself. This implies that there exists a simultaneous feedback. This diffusion effect also declines with distance. We will explore this mechanism more deeply in Section \ref{sec:interpretation}.

Considering the reduced form Equation (\ref{eq:Sar_dgp}), we might be able to find the mean and variance-covariance matrix of the complete system as function of exogenous variables. The expectation is given by:

\begin{equation}\label{eq:expected_value_slm}
  \begin{aligned}
\E(\vy|\mX,\mW) & = \E\left[\left.\left(\mI_n - \rho \mW\right)^{-1}\left(\alpha \vones_n + \mX\vbeta\right) + \left(\mI_n - \rho \mW\right)^{-1}\vepsi\right|\mX,\mW\right] \\
                & = \left(\mI_n - \rho \mW\right)^{-1}\left(\alpha \vones_n + \mX\vbeta\right).
\end{aligned}
\end{equation}

From Equation (\ref{eq:Sar_dgp}), we derive the variance-covariance matrix of $\vy$:

\begin{equation}
  \begin{aligned}
    \var\left(\left.\vy\right|\mW, \mX\right) & = \E\left(\left.\vy\vy^\top\right|\mW,\mX\right) \\ 
                               & = \left(\mI_n - \rho\mW\right)^{-1}\E\left(\left.\vepsi\vepsi^\top\right|\mW, \mX\right)\left(\mI_n- \rho\mW^\top\right)
  \end{aligned}
\end{equation}

This $n\times n$ variance-covariance matrix is full, which implies that each location is correlated with every other location in the system. However, this correlation decreases with distance. Since we have not assumed anything about the error variance, we can say that $\E\left(\left.\vepsi\vepsi^\top\right|\mW, \mX\right)$ is a full matrix, say $\mOmega_{\epsilon}$. This covers the possibility of heteroskedasticity, spatial autocorrelation, or both. In absence of either of these complications, the variance matrix simplifies to the usual $\sigma^2\mI_n$.

\begin{example}[County homicide rates in US]
In the criminology literature there has been a great emphasis of spatial diffusion of crime. The idea is that criminal violence may spread geographically via a diffusion process. For example, some researchers suggests that certain social processes such as illegal drug markets and gang rivalries may be important for explaining the pattern and mechanisms of the spread of homicides \citep{cohen1999diffusion}. 

In particular, empirical literature has focused on homicide rates and their determinants using the following OLS specification:

\begin{equation*}
y_i = \vx_i^\top\vbeta + \epsilon_i\quad i = 1, ..., n,
\end{equation*}
%
where $y_i$ is the homicide rate in spatial unit $i$ and $\vx_i$ is a $k\times 1$ set of covariates that explain homicide rates across spatial units. However, this model does not allow capturing the idea of spatial diffusion and spatial effects of homicide rates. For example, \cite{baller2001structural}, after rejecting the null hypothesis of spatial randomness on homicide rates, propose (among other spatial models) the following SLM process for modeling homicide rates using a county-level data for the decennial years in the 1960 to 1990 time period:

\begin{equation*}
\vy  =  \alpha \vones_n + \rho \mW\vy + \mX\vbeta + \epsilon,
\end{equation*}
%
where $\vy$ is the homicide rates for the US counties, $\mX$ includes a deprivation, population density, median age, the unemployment rate, percent divorced, and a Southern dummy variable based on census definitions.  As explained by \cite{baller2001structural}, if homicides rates are determined solely by the structural factors included in the $\mX$ matrix, there should be no spatial patterning of homicide beyond that created by socio-demographic similarities of geographically proximate counties. If this is the case, once all $x_k$ are included in the model, the spatial relationship between $y_i$ and $y_j$ will become nonsignificant. This implies that $\rho = 0$.

This is the model most compatible with common notions of diffusion processes because it implies an influence of neighbors’ homicide rates that is not simply an artifact of measured or unmeasured independent variables. Rather, homicide events in one place actually increase the likelihood of homicides in nearby locales. 
\end{example}

%====================================
\subsection{Spatial Durbin Model}
%====================================

The Spatial Durbin Model (SDM) model is shown in Equation (\ref{eq:Sdm}) along with its associated data generating process in Equation (\ref{eq:Sdm_dgp})\index{Spatial durbin model}\index{Spatial durbin model!reduced form}:

\begin{eqnarray}
\vy & = & \rho \mW\vy + \alpha \vones_n + \mX\vbeta + \mW\mX\vgamma + \vepsi \label{eq:Sdm} \\
\vy & = & \left(\mI_n - \rho \mW\right)^{-1}\left(\alpha \vones_n + \mX\vbeta + \mW\mX\vgamma + \vepsi\right) \label{eq:Sdm_dgp}
\end{eqnarray}


The SDM results in a spatial autoregressive model of a special form, including not only the spatially lagged dependent variable and the explanatory variables, but also the spatially lagged explanatory variables, $\mW\mX$: $\vy$ depends on own-regional factors from matrix $\mX$, plus the same factors averaged over the $n$ neighboring regions. This idea is shown in Figure \ref{figure:sdm}. Note that Region 1 not only exerts an impact on Region 2 (an vice versa) via $y$, but also via the the independent variable $x$.

\begin{figure}[ht]
	\centering
	\caption{The SDM for Two Regions}\label{figure:sdm}	
		\begin{tikzpicture}
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (y1) at (-1,0) {$y_1$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (x1) at (-1,2) {$\vx_1$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (e1) at (-1,-2) {$\epsilon_1$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (y2) at (1, 0) {$y_2$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (x2) at (1,2) {$\vx_2$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (e2) at (1,-2) {$\epsilon_2$};

			\draw[->, solid] (x1) to (y1);
			\draw[->, solid] (x2) to (y2);
			\draw[->, solid] (e1) to (y1);
			\draw[->, solid] (e2) to (y2);
			\draw[dashed, ->] (y1) to[bend right] (y2);
			\draw[dashed, ->] (y2) to[bend right] (y1);
			\draw[dashed, ->] (x1) to (y2);
			\draw[dashed, ->] (x2) to (y1);
			%\draw[->, solid] (0,-4) to node[right, align = center] {$\sigma_i = 1$} (6,-5.4);
			%\draw[->, solid] (-6,-6.6) to node[right] {$\sigma_i = \sigma = 1$} (-2.5,-8);
			%\draw[->, solid] (6,-6.6) to node[right] {$\var(\veta_i)=\vzeros$} (2.5,-8);
			
			% Legend
       \node (leyend) at (4, 0){
    \begin{tabular}{l@{: }l}
      \multicolumn{2}{c}{\textbf{Effects}}              \\
      $\rightarrow$                        & non-spatial effects    \\
      $\dashrightarrow$                     & spatial effects  \\
      \end{tabular}
      };
		\end{tikzpicture}
\end{figure}

As an example, consider that $y$ is some measure of air pollution in each region. Thus, $\mW\vy$ states that air pollution in region  1 might affect pollution in region 2, and vice versa. If $\mX$ contains a measure of population density, the variable $\mW\mX$ would indicate that density and region 1 (2) would affect air pollution in region 2 (1). 

This model has also very good properties in terms of calculation of marginal effects that will explore later. 

% This model have very good properties \citep[see for example][]{lesage2010introduction,elhorst2010applied}. The strengths of this model are the following:
% 
% \begin{enumerate}
% 	\item SDM produces unbiased coefficient estimates also if the true data-generation process is a spatial lag or spatial error model.
% 	\item It  does not impose prior restrictions on the magnitude of potential spatial spillover effects. 
% 	\item In contrast to other spatial regression specifications, these spillovers effects can be global or local and be different for different explanatory variables.  
% \end{enumerate}

%====================================
\subsection{Spatial Error Model}\label{sec:tax_SEM}
%====================================

Another form of spatial dependence occurs when the dependence works through the error process, in that the errors from different areas may display spatial autocorrelation. The Spatial Error Model (SEM) is formulated as\index{Spatial error model}:

\begin{equation}\label{eq:sem_model_comp}
  \begin{aligned}
\vy &= \alpha \vones + \mX\vbeta + \vu, \\
\vu &= \lambda \mW\vu + \vepsi. \\
 \end{aligned}
\end{equation}
%
where $\lambda$ is the autoregressive parameter for the error lag $\mW\vu$ (to distinguish the notation from the spatial autoregressive coefficient $\rho$ in a spatial lag model), and $\vepsi$ is a generally a i.i.d noise. Figure \ref{figure:sem} visualizes the SEM for two regions. Note that the error term of both regions are related, and the only spatial effect goes from $\epsilon_1$ to $\epsilon_2$ and vice versa. 	

\begin{figure}[ht]
	\centering
	\caption{The SEM for two regions}\label{figure:sem}	
		\begin{tikzpicture}
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (y1) at (-1,0) {$y_1$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (x1) at (-1,2) {$\vx_1$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (e1) at (-1,-2) {$\epsilon_1$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (y2) at (1, 0) {$y_2$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (x2) at (1,2) {$\vx_2$};
			\node[rectangle, text  width = 1cm, fill = red!10, align = center] (e2) at (1,-2) {$\epsilon_2$};

			\draw[->, solid] (x1) to (y1);
			\draw[->, solid] (x2) to (y2);
			\draw[->, solid] (e1) to (y1);
			\draw[->, solid] (e2) to (y2);
			\draw[dashed, <->] (e1) to (e2);

			% Legend
       \node (leyend) at (4, 0){
    \begin{tabular}{l@{: }l}
      \multicolumn{2}{c}{\textbf{Effects}}              \\
      $\rightarrow$                        & non-spatial effects    \\
      $\dashrightarrow$                     & spatial effects  \\
      \end{tabular}
      };
		\end{tikzpicture}
\end{figure}

As stated by \cite{AnselinBera1998}, spatial error dependence may be interpreted as a nuisance (and the parameter $\lambda$ as a nuisance parameter) in the sense that it reflects spatial autocorrelation in measurement errors or in variables that are otherwise not crucial to the model (i.e., the ``ignored'' variables spillovers across the spatial units of observations). 

Unlike previous models, interactions effects among the error terms do not require a theoretical model for a spatial or social interaction process, but instead, are consistent with a situation where determinants of the dependent variable omitted from the model are spatially autocorrelated, or with a situation where unobserved shocks follows a spatial pattern. 

The spatial diffusion of this model can be analyzed if we consider the reduced form equation. If the matrix $(\mI_n - \lambda\mW)$ is not singular, then \eqref{eq:sem_model_comp} can be written under the following reduced form\index{Spatial error model!reduced form}:

\begin{equation}
  \vy =  \alpha \vones + \mX\vbeta + (\mI_n - \lambda\mW)^{-1}\vepsi.
\end{equation}

This expression leads to a global spatial diffusion effect, but there is not spatial multiplier effect. The variance-covariance matrix is given by:

\begin{equation}
  \begin{aligned}
\E\left(\left.\vy\vy^\top\right|\mW, \mX\right)  & = \E\left(\left.\vepsi\vepsi^\top\right|\mW, \mX\right) \\
                 & = \left(\mI_n- \lambda\mW\right)^{-1}\E\left(\left.\vepsi\vepsi^\top\right|\mW, \mX\right)\left(\mI_n- \lambda\mW^\top\right)^{-1}
\end{aligned}
\end{equation}

\begin{remark}
Interaction effects among the unobserved terms may also be interpreted to reflect a mechanism to correct rent-seeking politicians for unanticipated fiscal policy changes. See for example \cite{allers2005tax}.
\end{remark}

%=========================
\subsection{Spatial Autocorrelation Model}
%=========================

A more general model is the one that includes the key modeling insights from both spatial lag and spatial error model describe above.  This model is called the Spatial Autocorrelation Model (SAC) and the its structural representation is the following:

\begin{equation*}
  \begin{aligned}
    y_i & = \alpha + \rho    \sum_{j = 1}^n w_{ij} y_j + \sum_{k=1}^K x_{ik}\beta_k + u_i \\
    u_i & = \lambda \sum_{j = 1}^n m_{ij} u_j + \epsilon_i
  \end{aligned}
\end{equation*}
%
or more compactly, 

\begin{equation}
  \begin{aligned}
\vy &= \alpha \vones_n + \rho \mW\vy + \mX\vbeta + \vu \\
\vu &= \lambda \mM\vu + \vepsi \\
  \end{aligned}
\end{equation}
%
where the matrix $\mW$ and $\mM$ are $n\times n$ spatial-weighting matrices.\footnote{This model is also known as SARAR(1, 1) model or Cliff-Ord models because of the impact that \cite{cliff1973spatial}  had on the subsequent literature. Note that SARAR(1, 1) is  a special case of the more general SARAR(p, q) model.} In this model, spatial interactions in the dependent variable and the disturbances are considered. As standard, the spatial weight matrices $\mW$ and $\mM$ are taken to be known and nonsthocastic. These matrices are part of the model definition, and in many applications, $\mM = \mW$. When $\rho = 0$, the model reduces to the SEM. When $\lambda = 0$ the model reduces to the SLM (SAR) specification. Setting $\rho = 0$ and $\lambda = 0$ causes the model to reduce to a linear regression model with exogenous variables. 

The reduced form is given by:

\begin{equation}
\vy = \left(\mI_n - \rho \mW\right)^{-1}\left(\mX\vbeta + \alpha \vones_n\right) + \left(\mI_n - \rho\mW\right)^{-1}\left(\mI_n - \rho \mM\right)^{-1}\vepsi.
\end{equation}

Figure \ref{fig:taxonomy} gives a more complete taxonomy for different spatial models. The more complete model is the General Nesting Spatial Model (GNS or Manski's Model), which includes spatial dependence in the dependent variable, exogenous variables and the error term. Different restrictions give rise to different spatial models. 


\begin{figure}[h]
\caption{Taxonomy of spatial models}
\label{fig:taxonomy}
\centering
\begin{tikzpicture}[scale = 0.8]
			\node[rectangle, text  width = 5cm, fill = red!10, align = center] (gns) at (0,0) {\scriptsize General nesting spatial model \\
			\begin{eqnarray*}
			\vy &=& \rho\mW\vy + \mX\vbeta + \mW\mX\vgamma + \vu \\
			\vu &=& \lambda\mW\vu + \vepsi
			\end{eqnarray*}};
			
			\node[rectangle, text  width = 3cm, fill = red!10, align = center] (sac) at (6,4) {\scriptsize  SAC \\
			\begin{eqnarray*}
			\vy &=& \rho\mW\vy + \mX\vbeta + \vu \\
			\vu &=& \lambda\mW\vu + \vepsi
			\end{eqnarray*}};
			
			\node[rectangle, text  width = 3cm, fill = red!10, align = center] (sdm) at (6,0) {\scriptsize  Spatial durbin model \\
			\begin{equation*}
			\vy = \rho\mW\vy + \mX\vbeta + \mW\mX\vgamma + \vepsi
			\end{equation*}};
			
			\node[rectangle, text  width = 3cm, fill = red!10, align = center] (sde) at (6,-4) {\scriptsize Spatial durbin error model \\
			\begin{eqnarray*}
			\vy &=& \mX\vbeta + \mW\mX\vgamma + \vu \\
			\vu &=& \lambda\mW\vu + \vepsi
			\end{eqnarray*}};
			
			\node[rectangle, text  width = 3cm, fill = red!10, align = center] (slm) at (12,4) {\scriptsize Spatial Lag Model \\
			\begin{equation*}
			\vy = \rho\mW\vy + \mX\vbeta + \vepsi
			\end{equation*}};
			
			\node[rectangle, text  width = 3cm, fill = red!10, align = center] (slx) at (12,0) {\scriptsize SLX \\
			\begin{equation*}
			\vy = \mX\vbeta + \mW\mX\vgamma + \vepsi
			\end{equation*}};
			
			\node[rectangle, text  width = 3cm, fill = red!10, align = center] (sem) at (12,-4) {\scriptsize Spatial error model \\
			\begin{eqnarray*}
			\vy &=& \mX\vbeta + \vu \\
			\vu &=& \lambda\mW\vu + \vepsi
			\end{eqnarray*}};
			
			\node[rectangle, text  width = 2cm, fill = red!10, align = center] (ols) at (16,0) {\scriptsize OLS \\
			\begin{equation*}
			\vy = \mX\vbeta + \vepsi
			\end{equation*}};

			\draw (gns) -- (sac) node [->, solid, midway, above, sloped] {\scriptsize $\vgamma = 0$};
			\draw (gns) -- (sdm) node [->, solid, midway, above, sloped] {\scriptsize $\lambda = 0$};
			\draw (gns) -- (sde) node [->, solid, midway, above, sloped] {\scriptsize $\rho = 0$};
			\draw (sac) -- (slm) node [->, solid, midway, above, sloped] {\scriptsize $\lambda = 0$};
			\draw (sac) -- (sem) node [->, solid, near end, above, sloped] {\scriptsize $\rho = 0$};
			\draw (sdm) -- (slx) node [->, solid, near end, above, sloped] {\scriptsize $\rho = 0$};
			\draw (sde) -- (slx) node [->, solid, near start, above, sloped] {\scriptsize $\lambda = 0$};
			\draw (sdm) -- (slm) node [->, solid, midway, above, sloped] {\scriptsize $\gamma = 0$};
			\draw (sdm) -- (sem) node [->, solid, midway, above, sloped] {\scriptsize $\gamma = -\rho\beta$};
			\draw (sde) -- (sem) node [->, solid, midway, above, sloped] {\scriptsize $\gamma = 0$};
			\draw (slm) -- (ols) node [->, solid, midway, above, sloped] {\scriptsize $\rho = 0$};
			\draw (slx) -- (ols) node [->, solid, midway, above, sloped] {\scriptsize $\gamma = 0$};
			\draw (sem) -- (ols) node [->, solid, midway, above, sloped] {\scriptsize $\lambda = 0$};
			%\draw[->, solid] (x2) to (y2);
			%\draw[->, solid] (e1) to (y1);
			%\draw[->, solid] (e2) to (y2);
			%\draw[dashed, <->] (e1) to (e2);

			% Legend
     %  \node (leyend) at (4, 0){
    %     \begin{tabular}{l@{: }l}
    %  \multicolumn{2}{c}{\textbf{Effects}}              \\
    %  $\rightarrow$                        & non-spatial effects    \\
    %  $\dashrightarrow$                     & spatial effects  \\
    %  \end{tabular}
    %  };
\end{tikzpicture}
\end{figure}

Starting with the GNS model:

\begin{itemize}
  \item Imposing the restriction $\vgamma = \vzeros$ leads to the SAC model that includes both a spatial lag for the dependent variable and spatial lag for the error term, but excludes the influence of the spatially lagged explanatory variables. 
  \item Imposing the restriction $\lambda = 0$ leads to the SDM.
  \item Imposing the restriction $\rho = 0$ leads to the Spatial Durbin Error Model (SDEM). 
\end{itemize}

Starting with the SDM:

\begin{itemize}
  \item The so-called common factor parameter restrictions $(\vgamma = -\rho\vbeta)$ yields the spatial error regression model (SEM) specification that assumes that externalities across spatial unites are mostly a nuisance spatial dependence problem caused by the regional transmission of random shocks. 
  \item Imposing the restriction $\gamma = 0$ leads to the spatial lag model (SLM), whereas the restriction $\rho = 0$ results in a least-squares spatially lagged $\mX$ regression model (labeled SLX) that assumes independence between regions in the dependent variable, but includes characteristics from neighboring regions in the form of spatially lagged explanatory variables.  
\end{itemize}

Finally, if $\rho = \lambda = 0$ and $\vtheta = 0$, then we obtain the traditional linear regression model. 

%==============================================
\section{Motivation of Spatial Models}\label{sec:motivation}
%==============================================

\subsection{SLM as a Long-run Equilibrium}\label{sec:SLM_lre}

It can be shown that a SLM can be considered as a simultaneous dependence system over time that culminate in a new steady state equilibrium, even using a cross-sectional dataset.

To illustrate this, consider $\vy_t$, which represent some dependent variable vector at time $t$. Assume that this variable is determined by a spatial autoregressive scheme that depends on space-time lagged values of the dependent variable from neighboring observations. This would lead to a time lag of the average neighboring values of the dependent variable observed during previous period,  $\mW\vy_{t-1}$. We can also include current period own-region characteristics $\mX_t$ in the model. If the characteristics of regions remain relatively fixed over time, we can write $\mX_t = \mX$ As an illustration, consider a model involving pollution as the dependent variable $\vy_t$, which depend on past period pollution of neighboring regions, $\mW\vy_{t-1}$. Then, the more appropriate process is the following:

\begin{equation}\label{eq:slm_with_time}
\vy_t = \rho \mW\vy_{t-1} + \mX\vbeta + \vepsi_t.
\end{equation}

Note that we can replace $\vy_{t-1}$ on the right-hand side of (\ref{eq:slm_with_time}) with:

\begin{equation*}
\vy_{t-1} = \rho \mW\vy_{t-2} + \mX\vbeta + \vepsi_{t-1},
\end{equation*}
%
producing:

\begin{equation}\label{eq:recur_2}
\begin{aligned}
\vy_t  = & \mX\vbeta +\rho\mW\left(\mX\vbeta + \rho \mW\vy_{t-2} + \vepsi_{t-1}\right) + \vepsi_t\\
=& \mX\vbeta +\rho\mW\mX\vbeta + \rho^2\mW^2\vy_{t-2} + \epsilon_t + \rho\mW\vepsi_{t-1}.
\end{aligned}
\end{equation}

Recursive substitution for past values of the vector $\vy_{t-r}$ on the right-hand side of (\ref{eq:recur_2}) over $q$ periods leads to:

\begin{equation*}
  \begin{aligned}
    \vy_t &= \left(\mI_n + \rho\mW + \rho^2\mW^2 + ... + \rho^{q-1}\mW^{q-1}\right)\mX\vbeta + \rho^q\mW^q\vy_{t-q} + \vu,\\
    \vu  & = \vepsi_{t} + \rho\mW\vepsi_{t-1} + \rho^2\mW^2\vepsi_{t-2} + ... + \rho^{q-1}\mW^{q-1}\vepsi_{t - (q-1)}. 
  \end{aligned}
\end{equation*}

The expected value of this spatial process is:

\begin{equation}\label{eq:expectation_time}
  \E\left(\vy_t\right) = \left(\mI_n + \rho\mW + \rho^2\mW^2 + ... + \rho^{q-1}\mW^{q-1}\right)\mX\vbeta + \rho^q\mW^q\vy_{t-q},
\end{equation}
%
where we use the fact that $\E(\vepsi_{t-r}) = 0, r = 0, ..., q-1$, which also implies that $\E(\vu) = \vzeros$. Finally, taking the limit of \eqref{eq:expectation_time}, 

\begin{equation}\label{eq:steady_state}
\lim_{q\to \infty} \E\left(\vy_t\right) = \left(\mI_n - \rho\mW\right)^{-1}\mX\vbeta.
\end{equation}
%
Note that we use the fact that the magnitude of $\rho^q\mW^q\vy_{t-q}$ tends to zero for large $q$, under the assumption that $\lvert \rho \rvert <1$ and assuming that $\mW$ is row-stochastic, so the matrix $\mW$ has a principal eigenvalue of 1. 

Equation (\ref{eq:steady_state}) states that we can interpret the observed cross-sectional relation as the outcome or expectation of a long-run equilibrium or steady state. Note that this provides a dynamic motivation for the data generating process of the cross-sectional SLM that serves as a \textbf{workhorse} of spatial regression modeling. That is, a cross-sectional SLM relation can arise from time-dependence of decisions by economic agents located at various point in space when decisions depend on those neighbors. 



\subsection{SEM and Omitted Variables Motivation}

Consider the following process:

\begin{equation*}
\vy = \vx\beta + \vz\theta,
\end{equation*}
%
where $\vx$ and $\vz$ are \textbf{uncorrelated} vectors of dimension $n\times 1$, and the vector $\vz$ follows the following spatial autoregressive process:

\begin{eqnarray*}
  \vz &=& \rho \mW\vz + \vr \\
  \vz &=& \left(\mI_n - \rho\mW\right)^{-1}\vr
\end{eqnarray*}
%
where $\vr \sim \rN(0, \sigma^2_{\epsilon}\mI_n)$. Examples of $\vz$ are culture, social capital, or neighborhood prestige. 

If $\vz$ is not observed, then:


\begin{equation}\label{eq:sem_omited_moti}
  \begin{aligned}
    \vy & = \vx\beta + \vu \\
    \vu & = \left(\mI_n - \rho\mW\right)^{-1}\vepsi
  \end{aligned}
\end{equation}
%
where $\vepsi = \theta\vr$. Then, we have the DGP for the SEM.

\subsection{SDM and Omitted Variables Motivation}

Now suppose that $\mX$ and $\vepsi$ from (\ref{eq:sem_omited_moti}) are correlated, given by the following process:

\begin{equation}\label{eq:cor_x_epsi}
\begin{aligned}
  \vepsi & = \vx \gamma + \vv \\
  \vv    & \sim \rN(0, \sigma^2\mI_n) 
\end{aligned}
\end{equation}
%
where the scalar parameters $\gamma$ and $\sigma^2$ govern the strength of the relationship between $\mX$ and $\vz = (\mI_n-\rho\mW)^{-1}\vr$. Inserting (\ref{eq:cor_x_epsi}) into (\ref{eq:sem_omited_moti}), we obtain:

\begin{equation}
  \begin{aligned}
    \vy & = \vx\beta + \left(\mI_n - \rho\mW\right)^{-1}\vepsi \\
        & = \vx\beta + \left(\mI_n - \rho\mW\right)^{-1}\left(\vx \gamma + \vv\right) \\
        & = \vx\beta + \left(\mI_n - \rho\mW\right)^{-1}\vx \gamma + \left(\mI_n - \rho\mW\right)^{-1}\vv \\
       \left(\mI_n - \rho\mW\right)\vy & = \left(\mI_n - \rho\mW\right)\vx\beta  + \vv \\
       \vy & = \rho\mW\vy + \vx\left(\beta + \gamma\right) + \mW\vx(-\rho\beta) + \vv
  \end{aligned}
\end{equation}

This is the Spatial Durbin Model (SDM), which includes a spatial lag of the dependent variable $\vy$, as well as the explanatory variables $\vx$.


%==============================================
\section{Interpreting Spatial Models}\label{sec:interpretation}
%==============================================

\subsection{Measuring Spillovers}

A major focus of regional science is measuring spatial spillover. A basic definition of spillovers in a spatial context would be that changes occurring in one region exert impacts on other regions \citep{LeSage2014}. Some examples are:

\begin{itemize}
  \item Changes in tax rate by one spatial unit might exert an impact on tax rate setting decisions of nearby regions, a phenomenon that has been labeled tax mimicking and yardstick competition between local government. 
  \item Situations where home improvements made by one homeowner exert a beneficial impact on selling prices of neighboring homes.
  \item Innovation by university researchers diffuses to nearby firms.
  \item Air or water pollution generated in one region spills over to nearby regions. 
\end{itemize}

The models reviewed in the previous section can be use to formally define the concept of a spatial spillover and,  more importantly, to provide estimates of the quantitative magnitude of spillovers and test their statistical significance.  There is however a distinction between global and local spillovers, which is discussed in \cite{anselin2003spatial} and \cite{LeSage2014}. 

We start our discussion about spillovers by formally defining global spillovers. 

\begin{definition}[Global Spillovers]\index{Spillover effects!Global spillovers}
	 Global spillovers arise when changes in a characteristic of one region impact all regions' outcomes. This applies even to the region itself since impacts can pass to the neighbors and back to the own region (feedback). Specifically, global spillovers impact the neighbors, neighbors to the neighbors, neighbors to the neighbors to the neighbors and so on. 
\end{definition}

The endogenous interactions produced by global spillovers lead to a scenario where changes in one region set in motion a sequence of adjustments in (potentially) all regions in the sample such that a new long-run steady state equilibrium arises \citep{lesage2014regional}.

As explained by \cite{lesage2014regional}, global spillovers might arise when considering local policies interactions. For example: \emph{``it seems plausible that changes in levels of public assistance (cigarette taxes) in state $A$ would lead to a reaction by neighboring states $B$ to change their levels of assistances (taxes), which in turn produces a game-theoretic (feedback) response of state $A$, and also responses of states $C$ who are neighbors to neighboring states $B$, and so on.''}


The following definition corresponds to local spillovers.

 \begin{definition}[Local Spillovers]\index{Spillover effects!Local spillovers}
 	Local spillovers represent a situation where the impact fall only on nearby or immediate neighbors, dying out before they impact regions that are neighbors to the neighbors.
 \end{definition}	

As it can be noted from the previous definitions, the main difference is that feedback or endogenous interaction is only possible for global spillovers. 

%\begin{remark}
%According to \cite{anselin2003spatial} and \cite{LeSage2014}, different spatial models give rise to different measures of spillovers.
%\end{remark}


\subsection{Marginal Effects}\index{Spillover effects!Marginal effects}

Mathematically, the notion of spillover can be thought as the derivative $\partial y_i/ \partial x_j$. This means that changes to explanatory variables in region $i$ impact the dependent variable in region $j\neq i$.

As an illustration, consider the SDM model, which can be re-written as:

\begin{equation*}
\begin{aligned}
(\mI_n - \rho\mW)\vy & = \mX\vbeta + \mW\mX\vtheta + \vepsi, \\
\vy & =   (\mI_n - \rho\mW)^{-1}\mX\vbeta + (\mI_n - \rho\mW)^{-1}\mW\mX\vtheta + (\mI_n - \rho\mW)^{-1}\vepsi, \\
\vy & =   \mA(\mW)^{-1}\mX\vbeta + \mA(\mW)^{-1}\mW\mX\vtheta + \mA(\mW)^{-1}\vepsi,\quad \mbox{since $\mA(\mW) = (\mI_n - \rho\mW)^{-1}$}\\
\vy & =   \mA(\mW)^{-1}\left(\mX\vbeta + \mW\mX\vtheta\right) + \mA(\mW)^{-1}\vepsi, \\
\vy & =   \sum_{r=1}^K\mA(\mW)^{-1}\left(\mI_n\beta_r + \mW\theta_r\right)\vx_r + \mA(\mW)^{-1}\vepsi, \\
\underbrace{\vy}_{(n\times 1)} & =   \sum_{r=1}^K\underbrace{\mS_r(\mW)}_{(n\times n)}\underbrace{\vx_r}_{n\times 1} + \underbrace{\mA(\mW)^{-1}}_{(n\times n)}\underbrace{\vepsi}_{n\times 1}
\end{aligned}
\end{equation*}
%
where $\mS_r = \mA(\mW)^{-1}\left(\mI_n\beta_r + \mW\theta_r\right)$, and

\begin{equation*}
 \vx_r = \begin{pmatrix}
          x_{r1} \\
          x_{r2} \\
          \vdots \\
          x_{rn}
        \end{pmatrix}.
\end{equation*}

Assuming that $\E(\epsilon_i) = 0$, then the expansion of the expected value yields:

\begin{equation}\label{eq:system_expected}
\begin{pmatrix}
\E(y_1) \\ \E(y_2) \\ \vdots \\ \E(y_n)
\end{pmatrix}
=
\sum_{r=1}^K\begin{pmatrix}
\mS_r(\mW)_{11} & \mS_r(\mW)_{12} & \hdots & \mS_r(\mW)_{1n} \\
\mS_r(\mW)_{21} & \mS_r(\mW)_{22} & \hdots & \mS_r(\mW)_{2n} \\
\vdots & \vdots & \ddots & \vdots \\ 
\mS_r(\mW)_{n1} & \mS_r(\mW)_{n2} & \hdots & \mS_r(\mW)_{nn} 
\end{pmatrix}
\begin{pmatrix}
x_{1r} \\ x_{2r} \\ \vdots \\ x_{nr} 
\end{pmatrix}.
\end{equation}

For the dependent variable for spatial unit $i$, Equation (\ref{eq:system_expected}) would be:

\begin{equation}
\E(y_i) = \sum_{r=1}^k\left[\mS_r(\mW)_{i1}x_{1r} + \mS_r(\mW)_{i2}x_{2r} + ... + \mS_r(\mW)_{in}x_{nr}\right].
\end{equation}

So, the impact on the expected value of location $i$ given a change in the explanatory variable $x_r$ in location $j$ is\index{Spillover effects!Indirect effects}

\begin{equation}\label{eq:indirect_impact}
\frac{\partial \E(y_i)}{\partial x_{jr}} = \mS_r(\mW)_{ij}
\end{equation}
%
where $\mS_r(\mW)_{ij}$ is this equation represents the $i,j$th element of the matrix $\mS_r(\mW)$. This result implies that, unlike the OLS model, a change in some variable in certain region will potentially affect the expected value of the dependent variable in all other regions. Given this characteristic, this type of effect is known as \textbf{indirect effect}.

The impact of the expected value of region $i$, given a change in certain variable for the same region is given by\index{Spillover effects!Direct effects}

\begin{equation}\label{eq:direct_impact}
\frac{\partial \E(y_i)}{\partial x_{ir}} = \mS_r(\mW)_{ii}.
\end{equation}

This impact includes the \textbf{effect of feedback loops} where observation $i$ affects observation $j$ and observation $j$ also affects observation $i$: a change in $x_{ir}$ will affect the expected value of dependent variable in $i$, then will pass through the neighbors of $i$ and back to the region itself. To shed more light on this, let us write the all the marginal effects in matrix notation as follows:

\begin{equation}\label{eq:matrix_marginal_effects}
\begin{aligned}
 \underset{(n \times n)}{\begin{pmatrix}
  \frac{\partial \E(\vy)}{\partial x_{1r}} & \frac{\partial \E(\vy)}{\partial x_{2r}} & \hdots & \frac{\partial \E(\vy)}{\partial x_{nr}} 
   \end{pmatrix}} & = 
  \begin{pmatrix}
  \frac{\partial \E(y_1)}{\partial x_{1r}} & \frac{\partial \E(y_1)}{\partial x_{2r}} & \hdots & \frac{\partial \E(y_1)}{\partial x_{nr}} \\
  \frac{\partial \E(y_2)}{\partial x_{1r}} & \frac{\partial \E(y_2)}{\partial x_{2r}} & \hdots & \frac{\partial \E(y_2)}{\partial x_{nr}} \\
  \vdots & \vdots & \ddots & \vdots \\
  \frac{\partial \E(y_n)}{\partial x_{1r}} & \frac{\partial \E(y_n)}{\partial x_{2r}} & \hdots & \frac{\partial \E(y_n)}{\partial x_{nr}} 
  \end{pmatrix} \\
  & = \mA(\mW)^{-1}\left(\mI_n\beta_r + \mW\theta_r\right) = \mS_r(\mW) \\
  & = (\mI_n - \rho\mW)^{-1}
  \begin{pmatrix}
    \beta_r  & w_{12} \theta_r  & \hdots & w_{1n}\theta_r \\
    w_{21} \theta_r  & \beta_r   & \hdots & w_{2n}\theta_r \\
    \vdots & \vdots & \ddots & \vdots \\
    w_{n1} \theta_r  & w_{n2} \theta_r  & \hdots  & \beta_r \\
  \end{pmatrix}
\end{aligned}  
\end{equation}

This expression is somewhat difficult to understand. To provide a better understanding we follow \cite{elhorst2010applied} and consider a model with 3 regions arranged linearly\footnote{Unit 1 is neighbor of unit 2, unit 2 is a neighbor of both units 1 and 3, and unit 3 is a neighbor of unit 2.} with the following matrices:

\begin{equation}\label{eq:w_mat_ex_spill}
  \mW = \begin{pmatrix}
          0      & 1 & 0 \\
          w_{21} & 0 & w_{23} \\
          0      & 1 & 0
        \end{pmatrix}
\end{equation}
%
and

\begin{equation}\label{eq:w_Imat_ex_spill}
  \mA(\mW)^{-1} = \frac{1}{1 - \rho^2}
  \begin{pmatrix}
          1 - w_{23}\rho^2     & \rho & \rho^2 w_{23}\\
          \rho w_{21} & 1 & \rho w_{23} \\
          \rho^2 w_{21}      & \rho & 1 - w_{21}\rho^2 
        \end{pmatrix}
\end{equation}
%
where $w_{12} = w_{31}= 1$ since units 1 and 3 have only one neighbor, and $w_{21} + w_{23} = 1$, so we explicitly consider a row-standardized matrix.  Substituting Equations (\ref{eq:w_mat_ex_spill}) and (\ref{eq:w_Imat_ex_spill}) into Equation (\ref{eq:matrix_marginal_effects}) we get:

\begin{equation*}
  \begin{pmatrix}
  \frac{\partial \E(\vy)}{\partial x_{1r}} & \frac{\partial \E(\vy)}{\partial x_{2r}} & \frac{\partial \E(\vy)}{\partial x_{3r}} 
   \end{pmatrix} = \frac{1}{1 - \rho^2}
   \begin{pmatrix}
    \left(1 -  w_{23}\rho^2\right)\beta_r + \left(w_{21}\rho\right) \theta_r & \rho \beta_r + \theta_r & \left(w_{23}\rho^2\right)\beta_r + (\rho w_{23})\theta_r \\
    (w_{21}\rho)\beta_r + w_{21}\theta_r & \beta_r + \rho \theta_r & (w_{23}\rho)\beta_r + w_{23}\theta_r \\
    (w_{21}\rho^2)\beta_r + (w_{21}\rho)\theta_r & \rho \beta_r + \theta_r & (1 -  w_{21}\rho^2)\beta_r + (w_{23}\rho)\theta_r
   \end{pmatrix}
\end{equation*}

Every diagonal element of this matrix represents a direct effect. Consequently, indirect effect do not occur if both $\rho = 0$ and $\theta_k = 0$, since all non-diagonal elements will then be zero. Another important insight is that direct and indirect effects are different for different spatial units in the sample. Direct effects are different because the diagonal elements of the matrix $(\mI_n - \rho\mW)^{-1}$ are different for different units, provided that $\rho \neq 0$. Indirect effects are different because both the non-diagonal elements of the matrix $(\mI_n - \rho\mW)^{-1}$ and of the matrix $\mW$ are different for different units, provided that $\rho \neq 0$ and/or $\theta_k\neq 0$. Finally, note that indirect effects that occur if $\theta_k\neq 0$ are \textbf{local effects}, whereas indirect effects that occur if $\rho\neq 0$ are \textbf{global effects}. 


\subsubsection{Summary Measures}\label{sec:summary-measures}

In general, the change of each variable in each region implies $n^2$ potential marginal effects.  If we have $K$ variables in our model, this implies $K\times n^2$ potential measures. Even for small values of $n$ and $K$, it may already be rather difficult to report these results compactly. To overcome this problem, \citet[][p. 36-37]{lesage2010introduction} propose the following scalar summary measures: 

\begin{definition}[Average Direct Impact]\label{def:ADI}
Let $\mS_r = \mA(\mW)^{-1}\left(\mI_n\beta_r + \mW\theta_r\right)$ for variable $r$. The impact of changes in the $i$th observation of $x_r$, which is denoted $x_{ir}$, on $y_i$ could be summarized by measuring the average $S_r(\mW)_{ii}$, which equals
	
	\begin{equation}
		\mbox{ADI} = \frac{1}{n}\tr\left(\mS_r(\mW)\right)
	\end{equation}
\end{definition}


Averaging over the direct impact associated with all observations $i$ is similar in spirit to typical regression coefficient interpretations that represent average response of the dependent to independent variables over the sample of observations. 	

\begin{definition}[Average Total Impact to an Observation]\label{def:ATIT}
	Let $\mS_r = \mA(\mW)^{-1}\left(\mI_n\beta_r + \mW\theta_r\right)$ for variable $r$. The sum across the $i$th row of $\mS_r(\mW)$ would be represent the total impact on individual observation $y_i$ resulting from changing the $r$th explanatory variable by the same amount across all $n$ observations. There are $n$ of these sums given by the column vector $\vc_r = \mS_r(\mW)\vones_n$, so an average of these total impacts is:
		\begin{equation}
		\mbox{ATIT} = \frac{1}{n} \vones_n'\vc_r
		\end{equation} 
\end{definition}

\begin{definition}[Average Total Impact from an Observation]\label{def:ATIF}
	Let $\mS_r = \mA(\mW)^{-1}\left(\mI_n\beta_r + \mW\theta_r\right)$ for variable $r$. The sum down the $j$th column of $\mS_r(\mW)$ would yield the total impact over all $y_i$ from changing the $r$th explanatory variable by an amount in the $j$th observation. There are $n$ of these sums given by the row vector $\vr_r = \vones_n'\mS_{r}(\mW)$, so an average of these total impacts is:
				\begin{equation}
				\mbox{ATIF} = \frac{1}{n} \vr_r\vones_n
				\end{equation} 
\end{definition}

The definition \ref{def:ATIF} relates how changes in a single observation $j$ influences all observations. In contrast, definition \ref{def:ATIT} considers how changes in all observations influences a single observation $i$. In both cases, averaging over all $n$ observations, leads to the same numerical result. The implication of this interesting result is that the \textbf{average total impact} is the average of all derivatives of $y_i$ with respect to $x_{jr}$ for any $i, j$. 

Therefore:

\begin{eqnarray}
\bar{M}(r)_{\mbox{direct}} & = & n^{-1}\tr\left(\mS_r(\mW)\right) \\
\bar{M}(r)_{\mbox{total}} & = & n^{-1}\vones_n'\mS_r(\mW)\vones_n \\
\bar{M}(r)_{\mbox{indirect}} & = & \bar{M}(r)_{\mbox{total}} - \bar{M}(r)_{\mbox{direct}}
\end{eqnarray}	


Given our example above, we obtain a direct effect of:

\begin{equation*}
  \frac{(3- \rho^2)}{3(1 - \rho^2)}\beta_k + \frac{2p}{3(1-\rho^2)}\theta_k,
\end{equation*}
%
and an indirect effect of

\begin{equation*}
  \frac{3\rho + \rho^2}{(3(1-\rho^2))}\beta_k + \frac{3 + \rho}{3(1- \rho^2)}\theta_k.
\end{equation*}

Unfortunately, since every application will have its own unique number of observations $n$ and spatial weight matrix $(\mW)$, these formulae cannot be generalized. 


\begin{example}[The effect of number of workers on commuting times]\label{example:commuting-Kirby}
\cite{kirby2009changes} use an SDM specification to consider changes in the (logged) number of workers in the US census tracts with commuting times exceeding 45 minutes one way, between 1990 and 2000 (See also the example in Section \ref{sec:lesage-example}). The motivation of this investigation is the fact that the percentage of the US workers with these long commute times in 1990 was 12.5\% compared to 15.4\% in 2000, an increase of more than 10\%. When deciding which model to estimate, they note that spillover impacts from an increase in commuters traveling long distances to work would seem \textbf{global} in nature, since the congestion effects of more travelers on one segment of a metropolitan area roadway network impact travel times of other travelers on the entire network. Furthermore, they state that \textbf{feedback} effects seem likely since congestion arising from commuting decisions by workers in one tract will spillover to neighboring tracts, which in turn create congestion feedback to the own tract. These two observations led the authors to specify the following SDM:

\begin{equation*}
  \vy = \rho\mW\vy + \alpha \vones_n + \mX\vbeta + \mW\mX\vtheta + \vepsi,
\end{equation*}
%
where $\mX$ includes the (logged) number of workers with long commute times $(\vy)$, variables related to location decision of households; age, gender and income distribution of resident population, and geographical characteristics of the tract, and $\mW\mX$ includes these same characteristics of neighboring census tracts. 
Based on a comparison of \textbf{direct, indirect} and \textbf{total effects} estimates form the 1990 and 2000 models, they conclude that the suite of variables reflecting the age and gender distribution of population in the tracts represents the primary explanation for changes in the number of workers with long commute times between 1990 and 2000. The spillover impacts of the number of employed females in the 1990 model was positive suggesting that more employed females in a tract produced an increase in long commute times for neighboring tract commuters. In contrast, for the 2000 model, spillovers associated with employed females were negative, so that more employed females in a tract reduced long commute times for workers located in neighboring tracts. 
\end{example}


\begin{example}[Effect of pollution on housing price]\label{example:pollution-kim}
\cite{kim2003measuring} use a spatial-lag hedonic model in order to assess the direct and indirect effect of quality air on housing price.  The main model is the following:

\begin{equation*}
\vp = \rho\mW\vp + \mX_1\vbeta_1 + \mX_2\vbeta_2 + \mX_3\vbeta_3 + \vepsi,
\end{equation*}
%
where $\vp$ is the vector of housing prices, $\rho$ is a spatial autocorrelation parameter, $\mW$ is the $n\times n$ spatial weight matrix, $\mX_1$ is a matrix with observations on structural characteristics, $\mX_2$ is a matrix with observations on neighborhood characteristics, and $\mX_3$ is a matrix with observations on environmental quality (SO$_2$ and NO$_x$). 

The marginal implicit price (marginal benefit) of the hedonic equation is derived as

\begin{equation*}
\begin{pmatrix}
  \frac{\partial \E(\vp)}{\partial x_{1r}} & \frac{\partial \E(\vp)}{\partial x_{2r}} & \hdots & \frac{\partial \E(\vp)}{\partial x_{nr}} 
   \end{pmatrix} = \mA(\mW)^{-1}\mI_n\beta_r\quad \mbox{where}\quad \mA(\mW)^{-1} = (\mI_n - \rho\mW)^{-1}
\end{equation*}

Focusing on the first row the interpretation if the following: the housing price of location $i$ is not only affected by a marginal change air quality of location $i$ but also is affected by marginal changes of air quality in other locations. That is, the total impact of a change in air quality on housing price at location $i$ is the sum of the direct impacts $\partial p_1/\partial x_{1k}$ plus induced impacts $\sum_{i = 2}^n \partial p_1 / \partial x_{ik}$ (See our Definition \ref{def:ATIT}). 

An important point evidenced by \cite{kim2003measuring} is that, if  the row-sums of $\mW$ is less than or equal to one and $\rho$ in the proper parameter space, i.e., $\rho < 1$, then the total average effect can be computed as $\beta_r / (1-\rho)$. To see this note that

\begin{equation}\label{eq:total_effect_simple}
  \begin{aligned}
    n^{-1}\vones^\top\mS_r(\mW)\vones & = n^{-1}\vones^\top\left[\mA(\mW)^{-1}\left(\mI\beta_r\right)\right]\vones \\
                                  & = n^{-1}\vones^\top\left[(\mI_n - \rho\mW)^{-1}\right]\left(\mI\beta_r\right)\vones \\
                                  & =   n^{-1}\vones^\top\left[\mI_n + \rho\mW + \rho^2\mW^2 + ... \right]\left(\mI\beta_r\right)\vones \quad\mbox{using Lemma \ref{lemma:Leontief}} \\
                                  & = n^{-1} \vones^\top\left[\mI_n\beta_r + \rho\mW\beta_r + \rho^2\mW^2\beta_r + ... \right]\vones \\
                                  & = n^{-1} \vones^\top\left[\mI_n\vones\beta_r + \rho\mW\vones\beta_r + \rho^2\mW(\mW\vones)\beta_r + \rho^3\mW\mW(\mW\vones) \right] \\
                                  & = n^{-1} \vones^\top\left[\beta_r\vones + \rho\beta_r\vones + \rho^2\beta_r\vones + \rho^3\beta_r\vones + ... \right]\quad \because \mW^l\vones = \vones \\
                                  & = n^{-1} \vones^\top\left[\beta_r + \rho\beta_r + \rho^2\beta_r + \rho^3\beta_r + ... \right]\vones\\
                                  & = n^{-1} \left[\beta_r + \rho\beta_r + \rho^2\beta_r + \rho^3 + ... \right] \vones^\top\vones\\
                                  & = n^{-1} \left[\beta_r + \rho\beta_r + \rho^2\beta_r + \rho^3\beta_r + ... \right] n \\
                                  & = \frac{\beta_r}{(1 - \rho)}
  \end{aligned}
\end{equation}

The model is estimated in a semi-log functional form, therefore the estimated coefficients can be interpreted as semi-elasticities. In particular, note that the elasticity for SO$_2$ is given by:

\begin{equation}
  \begin{aligned}
    \epsilon_{SO_2} & = \left(\frac{SO_2}{p}\right)\left(\frac{d p}{d SO_2}\right) \\
                      & = \left(\frac{SO_2}{p}\right)\left(\frac{\beta_r}{(1 - \rho)}\cdot p\right)\quad \mbox{since the model is log-lin} \\
                      & = \frac{\beta_r}{(1 - \rho)}\cdot SO_2
  \end{aligned}
\end{equation}

Using the estimated $\widehat{\rho} =  0.549$ and replacing SO$_2$ by its mean value they obtain that the elasticity of housing price from a given small change in air quality is about $0.348 \approx 4\%$. The marginal benefits per household of a permanent 4\% improvement in air quality using $\beta_{SO_2}(\mI_n - \rho\mW)^{-1}\vp$ is about \$2333 (1.43\% of mean house value) for owners. 
\end{example}

\begin{example}[Human capital and labor productivity]
\cite{fischer2009impact} analyze the role of human capital in explaining labor productivity variation among European region. In particular they estimate the following model:

\begin{equation*}
\vy = \rho\mW\vy + \mX\vbeta + \mW\mX\vgamma +\vepsi
\end{equation*}
%
where $\vy$ is the vector of observations on the (log of) labor productivity level at the end of the sample period (2004) and $\mX$ contains (the log of) labor productivity and human capital at the beginning of the sample period (1995). The parameter $\rho$ is expected to be positive indicating that regional productivity levels are positively related to a linear combination of neighboring regions' productivity. The parameter vector $\vgamma$ captures two types of spatial externalities:spatial effects working through the level of labor productivity and spatial effects working through the level of human capital, both at the beginning of the sample period. 

The estimated parameter of the spatial autoregressive parameter is $\widehat{\rho} = 0.664$ providing evidence for the existence of significant spatial effects working through the dependent variable. 

The mean direct impact for the human capital is 0.1317, whereas the indirect impact is -0.1968. They interpret the indirect impact in two ways. First, they argue that the indirect impact reflects how a change in the human capital level of all regions by some constant would impact the labor productivity of a typical region (observation). The sign of the estimated mean indirect impact implies that an increase in the initial level of human capital of all other regions would decrease the productivity level of a typical region. This indirect impact takes into account the fact that the change in initial human capital level negatively impacts other regions' labor productivity, which in turn negatively influences our typical region's labor productivity due to the presence of positive spatial dependence on neighboring regions' labor productivity levels. 

Second \cite{fischer2009impact} measure the cumulative impact of a change in region's $i$ initial level of human capital averaged over all other regions.  The impact from changing a single region's initial level of human capital on each of the other region's labor productivity is small, but cumulatively the impact measures -0.1968.
\end{example}

\begin{remark}
A very good paper for those interesting in making the connection between global/local spillovers and different spatial model specifications is \cite{lesage2014regional}. This is a must-read paper. 
\end{remark}

%%==============================================
%\section{Motivation of Spatial Models}
%==============================================

%\subsection{A Time-Dependence Motivation}

%Economic agents often make current period decisions that are influenced by the behavior of other agents in previous periods. For example, local governments might set tax rates observing rates set by neighboring region in previous time periods.\footnote{This section is heavily based on \cite{lesage2010introduction}} Although the tax rates were set over time by the cross-section of regions representing our sample, the observed cross-sectional tax rates would exhibit a pattern of spatial dependence. 

%Consider the dependent variable vector at time $t$, denoted $\vy_t$, determined using a spatial autoregressive scheme that depends on space-time lagged values of the dependent variable from observations. 

%=================================================================
\subsection{Partitioning Global Effects Estimates Over Space}\label{sec:partitioning-effects}
%=================================================================

It should bear in mind that these scalar summary measures of impact reflect how these changes would work thought the simultaneous dependence system over time to culminate in a new steady state equilibrium. Therefore, they should be considered as those impacts that would take place once all regions reach their equilibrium after the initial change in the variable of interest (See our discussion in Section \ref{sec:SLM_lre}). However one could track the cumulative effects as the impacts pass through neighbors, neighbors of neighbors and so on. 

\begin{remark}
Cross-sectional observations could be viewed as reflecting a (comparative static) slice at one point in time of a long-run steady-sate equilibrium relationship, and the partial derivatives viewed as reflecting a comparative static analysis of changes that represent new steady-state relationship that would arise \citep{lesage2014regional}.
\end{remark}

Intuition tell us that impacts arising from a change in the explanatory variables will influence low-order neighbors more than higher-order neighbors. Therefore, we would expect a decline in the impacts' magnitude as we move from lower- to higher-order neighbors. To get a better idea of this process is necessary to consider the matrix $\mS_r(\mW)$ and recognize, by Lemma \ref{lemma:Leontief}, that this matrix can be expressed as a linear combination of power of the weight matrix $\mW$. In particular, recall that if $\mW$ is a row standardized matrix such that $\rho \in (-1, 1)$, then by Lemma \ref{lemma:Leontief}:

\begin{equation}
  \begin{pmatrix}
  \frac{\partial \E(\vy)}{\partial x_{1r}} & \frac{\partial \E(\vy)}{\partial x_{2r}} & \hdots & \frac{\partial \E(\vy)}{\partial x_{nr}} 
   \end{pmatrix} \approx \left(\mI_n  + \rho \mW + \rho^2\mW^2 + \rho^3\mW^3 + ... + \rho^l\mW^l\right)\mI_n\beta_r  
\end{equation}
    
This expression allow us to observe the impact associated with each power of $\mW$, where these powers corresponds to the observation themselves (zero-order), immediate neighbors (first-order), neighbors of neighbors (second-order), and so on. Using this expansion we could account for both the cumulative effects as marginal and total direct, indirect associated with different order of neighbors.

%==============================================
\section{Predictors for Spatial Models}
%==============================================

\cite{kelejian2007relative} consider different information sets and define predictors as conditional means based on these information sets. Consider the SAC (SARAR) model:

\begin{equation}\label{eq:sac_predictors}
  \begin{aligned}
    \vy & = \rho\mW\vy + \mX\vbeta + \vu, \\
    \vu & = \lambda\mW\vu + \vepsi. 
  \end{aligned}
\end{equation}

Thus, the reduced form equation is:

\begin{equation}
\vy = \left(\mI_n -\rho \mW\right)^{-1}\mX\vbeta + \left(\mI_n -\rho \mW\right)^{-1}\left(\mI_n -\lambda \mW\right)^{-1}\vepsi. \\
\end{equation}

Assume that $\vepsi\sim\rN(\vzeros, \sigma^2_{\epsilon}\mI_n)$ and let $\left(\mI_n -\rho \mW\right)^{-1}_i$ be the $i$th row of $n\times n$ matrix $\left(\mI_n -\rho \mW\right)^{-1}$. Thus, the predicted value for the $i$th spatial unit is:

\begin{equation}\label{eq:reduced_i}
y_i = \left(\mI_n -\rho \mW\right)^{-1}_i\odot\mX\vbeta + \left(\mI_n -\rho \mW\right)^{-1}_i\odot\vu, 
\end{equation}
%
where $\odot$ is the element-by-element (pointwise) multiplication. 

For further reference, note that the SAC model \eqref{eq:sac_predictors} for $y_i$ is:

\begin{equation}\label{eq:sac_predictors_i}
  \begin{aligned}
    y_i & = \rho\vw_i\odot \vy + \vx_i \odot \vbeta + u_i, \\
    u_i & = \lambda \vw_i\odot \vu + \epsilon_i, 
  \end{aligned}
\end{equation}
%
where $\vx_i$ is the $i$th row of $\mX$.

Let $\vtheta = \left(\vbeta^\top, \rho, \lambda\right)^\top$. The first predictor is based on the conditional mean of $y_i$ in Equation \eqref{eq:reduced_i}, namely

\begin{equation}
  y_i^{\left[1\right]} = \E\left(y_i|\mX, \mM, \vtheta \right) = \left(\mI_n -\rho \mW\right)^{-1}_i\odot\mX\vbeta,
\end{equation}
%
which is based on the information set $\mPsi_1 = \left(\mX, \mW, \vtheta\right)$. Note that $\mPsi_1$ does not contain any random element. 

As stated by \cite{kelejian2007relative}, we can also increase the precision of the predictor by increasing the information. Namely, $\mPsi_2 = \left(\mX, \mW, \vtheta, \vw_i\odot \vy\right)$. Considering, the expected value of $y_i$ in Equation \eqref{eq:sac_predictors_i}, we obtain

\begin{equation}
  \begin{aligned}
     y_i^{\left[2\right]} & = \E(y_i|\mPsi_2) \\
                          & 
  \end{aligned}
\end{equation}



%==============================================
\section{Lesage's Book Example}\label{sec:lesage-example}\index{Spillover effects!example}
%==============================================

\subsection{Commuting Times and Congestion}

In this section we use \cite{lesage2010introduction}'s example as an illustration of spatial spillovers.\footnote{This example is further explore in \cite{kirby2009changes} with a real application.} For this purpose consider a set of seven regions show in Figure \ref{fig:lesage-example}, which represent three regions to the west and three to the east of a central business district (CBD). In particular, consider region $R4$ as being the central business district. Since the entire region contains only a single roadway, all commuters share this route to and from the CBD.


\begin{figure}[ht]
\caption{Regions east and west of the CBD}\label{fig:lesage-example}
		    \centering 
		      \includegraphics[width = 8cm, height=6cm]{figure/lesage.png}
\end{figure}

We observe the following set of the sample data for these regions that relates travel times to the CBD (in minutes) contained in the dependent variable vector $\vy$ to distance (in miles) and population density (population per square block) of the regions in the two columns of the matrix $\mX$.

\begin{figure}[H]
		    \centering 
		      \includegraphics[width = 8cm, height=6cm]{figure/lesage2.png}
\end{figure}	

According to \cite{lesage2010introduction}, the pattern of longer travel times for more distant regions R1 and R7 versus nearer R3 and R5 found in vector $\vy$ seems to clearly violate independence, since travel times appear similar for neighboring regions (see also Example \ref{example:commuting-Kirby}). However one can argue that the observed pattern is not due to spatial dependence, but rather it is explained by the variables Distance and Density associated with each region, since these also appear similar for neighboring regions. Note that even for individual residing in the CBD, it takes time to go somewhere else in the CBD. Therefore, the travel time for intra-CBD travel is 26 minutes despite having a distance of 0 miles. 

If we assume that the observed data was collected in a given day and averaged over a 24-hour period, it can be hypothesized that congestion effects that arise from the shared highway can explain the observed patter of travel times.  It is reasonable to claim that longer travel times in one region should lead to longer travel times in neighboring regions on any given day. This is because commuters pass from one region to another as they travel along the highway to the CBD.

Congestion effects represent one type of spatial spillover, which do not occur simultaneously, but require some time for the traffic delay to arise. From a modeling point of view, this effect cannot be captured by OLS model with distance and density as independent variables. These are dynamic feedback effects from travel time on a particular day that impact travel times of neighboring regions in the short time interval required for the traffic delay to occur. Since the explanatory variable distance would not change from day to day, and population density would change very slowly on a daily time scale, these variables would not be capable of explaining daily delay phenomena. 

A better way of explaining congestion is by the following DGP: 

\begin{equation*}
\vy = \rho_0\mW\vy + \mX\vbeta_0 + \vepsi, 
\end{equation*}
%
such that:
\begin{equation*}
\widehat{\vy} = \left(\mI_n - \widehat{\rho}\mW\right)^{-1}\mX\widehat{\vbeta},
\end{equation*}
%
where the estimated parameters are $\widehat{\vbeta} = (0.135, 0.561)'$ and $\widehat{\rho} = 0.640$ (assume that somehow we have estimated these parameters). Note that the estimated spatial autoregressive parameters indicates positive spatial dependence in the commuting times. 

\subsection{Computing Effects in R}

Now think about the following question: What would be the estimated spillovers if region $R2$ doubles its population density? To answer this question we first obtain the predicted values of travel times before the change.\footnote{Note that there is a typo in \cite{lesage2010introduction}, because in their equation (1.19) they double distance, not density.} That is, we first obtain:

\begin{equation*}
\widehat{\vy}^{(1)} = \left(\mI_n - \widehat{\rho}\mW\right)^{-1}\mX\widehat{\vbeta}.
\end{equation*}

<<exam-effect>>=
# Estimated coefficients
b <- c(0.135, 0.561)
rho <- 0.642

# W and X
X <- cbind(c(10, 20, 30, 50, 30, 20, 10),
           c(30, 20, 10, 0, 10, 20, 30))
W <- cbind(c(0, 1, 0, 0, 0, 0, 0),
           c(1, 0, 1, 0, 0, 0, 0),
           c(0, 1, 0, 1, 0, 0, 0),
           c(0, 0, 1, 0, 1, 0, 0),
           c(0, 0, 0, 1, 0, 1, 0),
           c(0, 0, 0, 0, 1, 0, 1),
           c(0, 0, 0, 0, 0, 1, 0))
Ws <- W / rowSums(W)

# Prediction
yhat_1 <- solve(diag(nrow(W)) -  rho * Ws) %*% crossprod(t(X), b)
@

Now we estimate the predicted values of travel times after the change in population density in $R2$ using:

\begin{equation}
\widehat{\vy}^{(2)} = \left(\mI_n - \widehat{\rho}\mW\right)^{-1}\widetilde{\mX}\widehat{\vbeta}
\end{equation}
%
where $\widetilde{\mX}$ is the new matrix reflecting a doubling of the population density of region $R2$.\footnote{For more about prediction in the spatial context see \cite{kelejian2007relative}.} A comparison of predictions $\widehat{\vy}^{(1)}$ and $\widehat{\vy}^{(2)}$ are going to be used to illustrate how the model generates spatial spillovers.  

<<doubling-dens>>=
# Now we double the population density of a single region
X_d <- cbind(c(10, 40, 30, 50, 30, 20, 10),
             c(30, 20, 10, 0, 10, 20, 30))

# Compute predicted value after the change
yhat_2 <- solve(diag(nrow(W)) -  rho * Ws) %*% crossprod(t(X_d), b)

# Results
result <- cbind(yhat_1, yhat_2, yhat_2 - yhat_1)
colnames(result) <- c("y1", "y2", "y2 - y1")
round(result, 2)
sum(yhat_2 - yhat_1)
@

The two set of predictions show that the change in region $R2$ population density has a direct effect that increases the commuting times for residents of region $R2$ by $\approx$4 minutes. It also has an indirect or spillover effect that produces an increase in commuting times for the other six regions. Furthermore, it can be noticed that the increase in commuting times for neighboring regions $R1$ and $R3$ are the greatest and these spillovers decline as we move to regions in the sample that are located farther away from region $R2$ where the change in population density occurred. 

What is the cumulative indirect impacts? Adding up the increased commuting times across all other regions (excluding the own-region change in commuting time), we find that equals $\approx  4.86 (2.56 + 1.45 + 0.53 + 0.19 + 0.08 + 0.05)$ minutes, which is larger than the direct (own-region) impact of 4 minutes. Finally, the total impact of all residents of the seven regions from the change in population density of region $R2$ is the sum of the direct and indirect effects, or 8.85 minutes increase in travel times to the CBD. 

Now assume that the OLS estimates for the example above are: $\widehat{\vbeta}_{OLS} = \left[0.55, 1.25\right]$. Using these estimates we compute the OLS predictions based on the matrices $\mX$ and $\widetilde{\mX}$ as shown above.

<<ols_pred>>=
# Ols prediction
b_ols <- c(0.55, 1.25)
yhat_1 <- crossprod(t(X), b_ols)
yhat_2 <- crossprod(t(X_d), b_ols)
result <- cbind(yhat_1, yhat_2, yhat_2 - yhat_1)
colnames(result) <- c("y1", "y2", "y2 - y1")
round(result, 2)
@

The results show no spatial spillovers. Only the travel time of $R2$ is affected by the change in population density of region $R2$. It can be also observed that OLS prediction is upward bias.  This is the main message here. An OLS model does no allows for spatial spillover impacts and generates biased marginal effects. 

Now we further explore our formulas and definition from previous Section. As we showed in Equation (\ref{eq:direct_impact}), the impact of changes in the $i$th observation of $x_r$ on $y_i$  is $S_r(W)_{ii}$. Given the SLM structure of our example, this is equivalent to 

\begin{equation*}
\frac{\partial \E(\texttt{CT}_i)}{\partial \texttt{density}_i} = \mS_{density}(\mW)_{ii}, \quad \mbox{where $\mS_{\texttt{density}} = (\mI - \rho \mW)^{-1}\mI\beta_{\texttt{density}}$}.
\end{equation*}

We can compute our $\mS_{\texttt{density}}$ in the following way.
 
<<compu-S>>=
# Compute S(W) matrix for density
b_dens <- 0.135
S <- solve(diag(nrow(W)) -  rho * Ws)  %*% diag(nrow(W)) * b_dens 
colnames(S) <- rownames(S) <- c("R1", "R2", "R3", "R4", "R5", "R6", "R7")
@

Then, the direct impact of doubling population density of $R2$ on the expected value of commuting time for $R2$ is given by
   
\begin{equation*}
  \Delta \E(\texttt{CT}_2) = S_{\texttt{density}}(\mW)_{22} \Delta \texttt{density}_{2} = S_{\texttt{density}}(\mW)_{22} \cdot 20
\end{equation*}

In R, this equals :

<<effect1>>=
# Direct impact of R2 on R2
round(S[2,2] * 20, 2)
@

Note that this value is the same as that found using the predicted value procedure:  by doubling population density in  $R2$  increases the commuting times for residents of region $R2$ by $\approx$4 minutes. 

Finding the indirect impact on region $R1$ is similar given Equation \ref{eq:indirect_impact}. The indirect impact on region $R1$ is given by:

\begin{equation*}
  \Delta \E(\texttt{CT}_1) = S_{\texttt{density}}(\mW)_{12} \Delta \texttt{density}_{2} = S_{\texttt{density}}(\mW)_{12} \cdot 20
\end{equation*}

That is:

<<effect2>>=
# Indirect impact of R2 on R1
round(S[1,2] * 20, 2)
@

Again, note that is the same value computed before: An increase of 100\% of population density in $R2$ implies an increase of travel time of region $R1$ to CBD of about 2.56 minutes, after considering all feedback effects. 


An interesting question would be the following: What would be the impact on commuting time on $R1$ if population density increases by 20 in all the Regions? To answer this question, we should recall our definition \ref{def:ATIT} states that the sum across the $i$th row of $\mS_{r}(\mW)$ would be represent the total impact on individual observation $y_i$ resulting from changing the $r$th explanatory variable by the same amount across $n$ observations. 
   
<<effect3>>=
# ATIT
round(sum(S[1, ]) * 20, 2)
@

This number implies that the total impact to R1 will be an increase of commuting time of $\approx 7.5$ minutes. Using the formula for ATIT gives the same result:

<<effect4>>=
# ATIT
n <- nrow(W)
vones <- rep(1, n) 
round(((t(vones) %*% S %*% vones) / n ) * 20, 2)
@

Similarly, we could ask: What would be the impact of increasing density by 20 in $R1$ on all the other regions? This is equivalent to our definition \ref{def:ATIF} which state that the sum down the $j$th column of $\mS_r(\mW)$ would yield the total impact over all $y_i$ from changing the $r$th explanatory variable by an amount in the $j$th observation.

<<effect5>>=
# ATIF
round(sum(S[, 1]) * 20, 2)
@

In words, increasing density by 20 in $R1$ would imply a total effect  in all the regions of about 7.54 minutes.

Imagine that you are a policy maker and you are considering in implementing a policy to reduce population density and hence reduce commuting time in the regions. However, given that resources are scarce, you must select which region to implement this policy. In order to produce a greater effect of policy you could use the estimated spatial model and look for the region that will have the greatest overall impact (considering feedback effects). Basically, this involves calculating the column sum of $\mS_r(\mW)$ for each region in the following way: 

<<>>=
# Computing colsums of S(W)
round(colSums(S), 2)
@ 
   
Note that the impact of decreasing population density by 1 will have a greater reduction in commuting time if applied in regions $R2$ and $R6$ (why?)

Finally, the average direct, indirect and total effects of an increase in 1 in population density in all the regions can be computed as follows. 

<<average-effects>>=
# Average Direct Impact 
ADI <- sum(diag(S)) / nrow(W)
round(ADI, 4)

# Average Total Impact 
Total <- crossprod(rep(1, nrow(W)), S) %*% rep(1, nrow(W)) / nrow(W)
round(Total, 4)

# Average Indirect Impact
round(Total - ADI, 4)
@

Equation (\ref{eq:total_effect_simple}) of Example \ref{example:pollution-kim}, we show that the total effect can be also be computed as $\beta_r / (1 - \rho)$. We know show that this proposition is true for our example

<<check-total>>=
#Check total effect
b_dens / (1 - rho )
@

\subsection{Cumulative Effects}

The main idea of this exercise is to show how the change in some explanatory variable produces changes in the independent variable in all the spatial units by decomposing them into cumulative and marginal impacts for different order of neighbors as explained in Section \ref{sec:partitioning-effects}. 
    
First, we load the package \pkg{expm} which will allow us to compute power of matrices in a loop. Then we create the estimated coefficients along with the $\mW$ matrix:

<<message = FALSE>>=
# Package to compute power of a matrix
library("expm") 
@

In order to create the decomposition for the ADI, AII and ATI, we create the following loop from $q = 0$ to $q = 10$:

<<loop-decomp>>=
## Loop for decomposition
out <- matrix(NA, nrow = 11, ncol = 3)             # Matrix for the results 
colnames(out) <- c("Total", "Direct", "Indirect")  # colnames
rownames(out) <- paste("q", sep = "=", seq(0, 10)) # rownames

for (q in 0:10) {
  if (q == 0) {                                    # If q=0, then Sr = I * beta
    S <- diag(n) * b_dens
  } else {
    S <- (rho ^ q * Ws %^% q)  * b_dens
  }
  q <- q + 1                                       # the row = 0 doesn't exist!
  out[q, 2] <- sum(diag(S)) / n
  out[q, 1] <- crossprod(rep(1, n), S) %*% rep(1, n) / n
  out[q, 3] <- out[q, 1] - out[q, 2]
}
@

The results are the following

<<result-loop>>=
# Print results
round(out, 4)
round(colSums(out), 4)
@

This table shows both the cumulative and partitioned direct, indirect and total impacts associated with orders 0 to 10 for the SLM. The cumulative direct impact from previous section equal to 0.1837, which given the coefficient 0.1350 indicates that \emph{there is a feedback equal to} (0.1837 - 0.1350) = 0.0487 arising from each region impacting neighbors that in turn impacts neighbors to neighbors and so on.

The column sum of the matrix \texttt{out} shows that by the time we reach 10th-order neighbors we have accounted for 0.1834 of the 0.1837 cumulative direct effect. It is important noting that for $\mW^0$ there is no indirect effect, only direct effects, and for $\mW^1$ there is no direct effect, only indirect. To see this, note that when $q=0$ we obtain $\mW^0 = \mI_n$:

<<loop-w0>>=
Ws %^% 0
@

Thus,  we have $\mS_r(\mW) = \mI_n\beta_r = 0.1350 \mI_n$. When $q= 1$ we have only indirect effect since there are zero elements on the diagonal of the matrix $\mW$. This also occurs for  $q = 3, 5, 7, 9$: 

<<loop-w1>>=
Ws %^% 1
Ws %^% 3
@


Also, the row-stochastic nature of $\mW$ leads to an average of the sum of the rows that takes the form $\beta_r \times \rho= 0.135\times 0.642 = 0.0867$, when $q=1$.

The matrix \texttt{out} also shows that both direct and indirect effects fall out as the order of neighbors increases, however the indirect or spatial spillovers effects decay more slowly as we move to higher-order neighbors. 


%**********************************************
%\section{Criticisms of Spatial Econometrics}
%**********************************************

% The concepts of causality and endogeneity have not been elaborated so much in a spatial context.\\
% 
% The goal of spatial dependence is to measure the effects of proximity. Other studies try to isolate the effect of proximity.\\
% 
% Another problem using spatial data analysis is that important variables are highly correlated and no study includes all relevant variables (Problem of correlated missing variables \citep{mcmillen2010issues}).\\
% 
% In some cases when the units of observation can be located in some geographical space it is possible that location is irrelevant for understanding data pertaining to those units. In this case, we might say that the spatial dimension is random. In other cases, including the spatial dimension to the problem is important.\\
% 
% 
% There are three papers that critique spatial econometrics.
% 
% \begin{enumerate}
% 	\item \cite{gibbons2012mostly} state that identification is almost always impossible with standard spatial econometric practice.
% 	\item \cite{mcmillen2012perspectives} posits that spatial econometrics rests on very restrictive assumptions and that identification is difficult. Nonparametric approaches are better.
% 	\item \cite{corrado2012economics} state that standard spatial econometric has too often been misapplied. Namely, there is not enough emphasis on both theory or in simply forming a conceptual framework to understand spatial spillovers.
% \end{enumerate}	
% 
% 
% According to 

\section{Exercises}

\begin{exercises}

    \exercise Assume three regions with row-normalized spatial weight matrix given in Equation \eqref{eq:w_mat_ex_spill}. Derive the total, direct an indirect effects for the following models:
        \begin{enumerate}
            \item Spatial Durbin Model given by:
		\begin{equation}
			\vy = \rho \mW \vy + \alpha \vones_n + \mX \vbeta + \mW \mX \vtheta + \vepsi
		\end{equation}
            \item Spatial Lag Model given by:
		\begin{equation}
			\vy = \rho \mW \vy + \alpha \vones_n + \mX\vbeta + \vepsi
		\end{equation}
		   \item Spatial Durbin Error Model given by:
		\begin{eqnarray}
			\vy &=& \alpha \vones_n + \mX\vbeta + \mW\mX\vtheta + \vu \\
			\vu &=& \lambda \mW\vu + \vepsi
		\end{eqnarray}
		\item OLS given by:
		\begin{equation}
			\vy = \alpha \vones_n + \mX\vbeta + \vepsi
		\end{equation}
		\item Spatial Error model given by:
		
		\begin{equation}
			\begin{aligned}
				\vy &= \alpha \vones_n + \mX\vbeta + \vu \\
				\vu &= \lambda \mW\vu + \vepsi
			\end{aligned}
		\end{equation} 
        \end{enumerate}

    \exercise Consider your results for the SLM and SDM models from Exercise  \ref{lab:2.1}. Show that for the SLM model the ratio between the indirect and the direct effect of a particular explanatory variable is independent of $\beta_k$. Show that this is not the case for the SDM model. What do you conclude?
    
    \exercise  Recall that if the row-sums of $\mW$ is less than or equal to one and $\rho$ is in the proper parameter space, i.e., $\rho <1$, the total average effect for variable $r$ can be computed as $\beta_r / (1 - \rho)$. What is the sign of the parameter that matters the most when calculating the sign of the total effect? Does the $\rho$ or $\beta_r$?

\end{exercises}
