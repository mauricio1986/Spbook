\chapter{Maximum Likelihood Estimation}\label{chap:ML}

In this chapter, we begin the study of estimation methods for spatial models. In particular, we focus in the maximum likelihood (ML) estimation method. The ML procedure is one of the oldest and most widely used approaches in the literature for estimating spatial models. It provides a robust framework for obtaining consistent and efficient parameters estimates, even in the presence of spatial dependence.


%*********************************************************
\section{What Are The Consequences of Applying OLS?}\label{sec:consequences_slm}
%*********************************************************

In this section, we examine the implications of using the Ordinary Least Squares (OLS) estimator on data generated by a Spatial Autoregressive (SAR) process. The key result is that OLS estimates of the coefficients in such models are biased and inconsistent. This means that, even with a large dataset, the estimated parameters will not converge to the true population parameters, even if we have a very large data set, which is a serious problem.

%-----------------------------------------------
\subsection{Finite and Asymptotic Properties}\index{Spatial lag model!pure}
%-----------------------------------------------

Let's begin by demonstrating that the OLS estimate of $\rho$ becomes biased within the framework of the Spatial Lag Model (SLM). Consider the simplified first-order spatial autoregressive model:
\begin{equation}\label{eq:simply_lag}
\underset{(n\times 1)}{\vy} = \rho_0\underset{(n\times 1)}{\mW\vy} + \underset{(n\times 1)}{\vepsi},
\end{equation}
%
where $\rho_0$ represents the true population parameter. The reduced form for the \textbf{pure SLM} in Equation  \eqref{eq:simply_lag} is:
\begin{equation}\label{eq:reduced_form_pure_slm}
\vy =\left(\mI_n - \rho_0\mW\right)^{-1}\vepsi.
\end{equation}

Subsequently, the spatial lag term can be then expressed as:
\begin{equation}\label{eq:wy_pure}
  \mW\vy = \mW\left(\mI_n - \rho_0\mW\right)^{-1}\vepsi.
\end{equation}

Now, recalling that if the model is $\vy = \mX\vbeta + \vepsi$, then the OLS estimator is $\widehat{\vbeta} = \left(\mX^\top\mX\right)^{-1}\mX^\top\vy$. Then, considering Equation \eqref{eq:simply_lag} the OLS estimator for $\rho_0$ is: 
\begin{equation}\label{eq:rho_ols}
\widehat{\rho}_{OLS} = \left[\underbrace{\left(\mW\vy \right)^\top}_{(1\times n)}\underbrace{\left(\mW\vy \right)}_{(n\times 1)}\right]^{-1}\underbrace{\left(\mW\vy \right)^\top}_{(1\times n)}\underbrace{\vy}_{(n\times 1)}.
\end{equation}

Substituting the expression for $\vy$ from the population Equation \eqref{eq:simply_lag} into Equation \eqref{eq:rho_ols} yields the sampling error equation:
\begin{equation*}
  \begin{aligned}
          \widehat{\rho}_{OLS} & = \rho_0 + \left[\left(\mW\vy \right)^\top\left(\mW\vy \right)\right]^{-1}\left(\mW\vy \right)^\top\vepsi, \\
                               & = \rho_0 + \left(\sum_{i = 1}^n \vy_{Li}^2\right)^{-1}\left(\sum_{i = 1}^{n}\vy_{Li}\epsilon_i\right),
  \end{aligned}
\end{equation*}
%
where $\vy_{Li}$ is the $i$th element of the spatial lag operator $\mW\vy = \vy_L$. Assuming a nonstochastic $\mW$, the mathematical expectation of $\widehat{\rho}_{OLS}$ is expressed as:
\begin{equation}\label{eq:expectation_of_pureslm}
  \begin{aligned}
\E\left(\left.\widehat{\rho}_{OLS}\right| \mW\right) & = \rho_0 + \E\left(\left.\left[\left(\mW\vy \right)^\top\left(\mW\vy \right)\right]^{-1}\left(\mW\vy \right)^\top\vepsi\right| \mW \right), \\
                         & = \rho_0 + \left(\sum_{i = 1}^n \vy_{Li}^2\right)^{-1}\E\left(\left.\sum_{i = 1}^{n}\vy_{Li}\epsilon_i\right| \mW\right).
  \end{aligned}
\end{equation}

Examining \eqref{eq:expectation_of_pureslm}, it is evident that if the expectation of the last term is zero, $\widehat{\rho}_{OLS}$ is unbiased. However, as shown in \eqref{eq:Eyle}, this condition is not met, leading to bias unless $\rho_0 = 0$:
\begin{equation}\label{eq:Eyle}
  \begin{aligned}
      \E\left(\left.\sum_{i = 1}^{n}\vy_{Li}\epsilon_i \right| \mW\right) & =  \E\left[\left.\left(\mW\vy \right)^\top\vepsi\right| \mW\right], \\
      & =  \E\left[\left.\underset{(1\times 1)}{\vepsi^\top \left( \mI -\rho_0\mW^\top\right)^{-1}\mW^\top\vepsi} \right| \mW \right] \quad \mbox{using (\ref{eq:wy_pure})},\\
      & =  \E\left[\left.\vepsi^\top \mC^\top \vepsi \right| \mW\right], \\
      & = \E\left[\left.\tr \vepsi^\top  \mC^\top\vepsi\right|\mW \right], \\
      & = \E\left[\left.\tr \mC^\top \vepsi\vepsi^\top\right| \mW\right], \\
      & =  \tr \left(\mC\right) \E\left(\left.\vepsi\vepsi^\top\right| \mW\right)\quad \mbox{since $\tr(\mA) = \tr(\mA^\top)$}, \\
      & \neq 0,
  \end{aligned}
\end{equation}
%
where $\mC = \mW\left( \mI -\rho_0\mW\right)^{-1}$. Therefore, given the result in \eqref{eq:Eyle} we have that $\E\left(\left.\widehat{\rho}_{OLS}\right| \mW\right) = \rho_0$ if and only if $\tr \left(\mC\right) = 0$, which occurs if $\rho_0 = 0$. If $\rho = 0$, $\mC = \mW$, and $\tr(\mC) = \tr(\mW) = 0$ because the diagonal elements of $\mW$ are zeros (See Definition \ref{definition:trace} for properties of the trace).  In other words, if the true model follows a spatial autoregressive structure, the OLS estimate of $\rho$ will be biased. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{definition}[Some useful results on trace]\label{definition:trace}
  The \textbf{trace} of a squared matrix $\mA$, denoted $\tr(\mA)$, is defined to be the sum of the elements on the main diagonal of $\mA$:
  \begin{equation}
    \tr(\mA) = \sum_{i = 1}^n a_{ii} = a_{11} + a_{22} + ... + a_{nn}
  \end{equation}
  %
  where $a_{ii}$ denotes the entry on the $i$th row and $i$th column of $\mA$. 
  
 Some properties:
 \begin{enumerate}
  \item Let $\mA$ and $\mB$ be square matrices and $c$ a scalar. Then:
  \begin{align}
    \tr(\mA + \mB) & = \tr(\mA) + \tr(\mB) \\
    \tr(c\mA) & = c\tr(\mA)
  \end{align}
  \item $\tr(\mA) = \tr(\mA^{\top})$. 
  \item $\tr(\mA\mB) = \tr(\mB\mA)$.
  \item Trace of an idempotent matrix: Let $\mA$ be an idempotent matrix, then $\tr(\mA)= \rk(\mA)$. 
 \end{enumerate}
\end{definition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Regarding consistency, we can express $\widehat{\rho}_{OLS}$ as: 
\begin{equation}
          \widehat{\rho}_{OLS} = \rho_0 + \left(\frac{1}{n}\sum_{i = 1}^N \vy_{Li}^2\right)^{-1}\left(\frac{1}{n}\sum_{i = 1}^{n}\vy_{Li}\epsilon_i\right).
\end{equation}

Under certain conditions, we can show that:
\begin{equation}
\frac{1}{n}\sum_{i = 1}^n \vy_{Li}^2 \to q,
\end{equation}
%
where $q$ is some finite scalar (We need some assumptions here about $\rho$ and the structure of the spatial weight matrix ). However, for the second term we obtain
\begin{equation}
  \frac{1}{n}\sum_{i = 1}^{n}\vy_{Li}\epsilon_i \pto \E(\vy_{Li}\vepsi_i) = \tr \left(\mC\right) \E(\vepsi\vepsi^\top) \neq 0.
\end{equation}

As a result, the presence of the spatial weight matrix results in a quadratic form in the error terms, which in turns introduces a form of endogeneity because the spatial lag $\mW\vy$ will be correlated with the disturbance vector $\vepsi$. Therefore $\widehat{\rho}_{OLS}$ is inconsistent, and we need to account for the simultaneity by either in a maximum likelihood estimation framework, or by using a proper set of instrumental variables.


\begin{remark}
  \cite{lee2002consistency} demonstrates that, in some cases, the OLS estimator may still be consistent and asymptotically efficient relative to certain other estimators.
\end{remark}

%---------------------------------
\subsection{Illustration of Bias}
%---------------------------------

To examine the properties of the OLS estimator when the data generating process follows a SAR process, we will conduct a straightforward simulation experiment. The fundamental design of this experiment involves generating simulated observations based on a known data generating process, specifically, a Spatial Lag Model (SLM). Subsequently, we will estimate the parameters for each simulated sample. If the estimator is biased, the average estimated parameters should deviate significantly from the true parameter.

In our simulation experiment, we assume the true Data Generating Process (DGP) to be:
\begin{equation*}
  \vy = \rho_0\mW\vy + \vepsi,
\end{equation*}
%
where the true value $\rho_0 = 0.7$, and the sample size for each sample is $n = 225$. The error term $\vepsi$ follows a normal distribution $\rN(\vzeros, \sigma^2\mI_n)$, and $\mW$ is an artificial $n\times n$ weight matrix. The matrix $\mW$ is constructed from a neighbor list for rook contiguity on a $500 \times 500$ regular lattice.

In \proglang{R}, the syntax for creating the global parameters for the simulation is as follows:
<<sim_set_up1>>=
# Global parameters
library("spdep")
library("spatialreg")
set.seed(123)                                   # Set seed
S       <- 100                                  # Number of simulations
n       <- 225                                  # Spatial units
rho     <- 0.7                                  # True rho
w       <- cell2nb(sqrt(n), sqrt(n))            # Create artificial W matrix
iw      <- invIrM(w, rho)                       # Compute inverse of (I - rho*W)
rho_hat <- vector(mode = "numeric", length = S) # Vector to save results.
@

The function \code{cell2nb} creates a list of neighbors for a grid of cells. By default it creates neighbors based on rook criteria. The \code{invIrM} function generates the full weights $\mW$, checks that $\rho$ lies in its feasible range between $1 / \min{\vomega}$ and $1 / \max{\vomega}$, where $\vomega = eigen (\mW)$, and returns the $n\times n$ inverted matrix $(\mI_n - \rho\mW)^{-1}$. 

The simulation loop is structured as follows:
<<loop_sim_1>>=
# Loop for simulation
for (s in 1:S) {
  e <- rnorm(n, mean = 0 , sd = 1) # Create error term
  y <- iw %*% e                    # True DGP
  Wy <- lag.listw(nb2listw(w), y)  # Create spatial lag
  out <- lm(y ~ Wy)                # Estimate OLS
  rho_hat[s] <- coef(out)["Wy"]    # Save results
}
@


It is important to note that $\mW$ is treated as fixed (nonstochastic), and therefore, it is created outside the simulation loop.

The summary of the estimated $\rho$ is presented below:
<<sum-loop-1>>=
# Summary of rho_hat
summary(rho_hat)
@

Upon examination, it is evident that the estimated $\rho$ spans the range from 0.8 to 1.2, with this range failing to include the true parameter $\rho_0 = 0.7$. Furthermore, the mean of the estimated parameters is 1, a considerable distance from the true value of $0.7$. This discrepancy suggests a substantial bias in the OLS estimator for the pure Spatial Lag Model (SLM) when $\rho$ is high.

To visually explore the sampling distribution of the estimated parameters, a plot can be generated using the following approach:
<<ols-rho-sim-F, eval = FALSE>>=
# Plot density of estimated rho_hat. 
plot(density(rho_hat),
     xlab = expression(hat(rho)), 
     main = "")
abline(v = rho, col = "red")
@

Figure \ref{fig:ols-rho-sim} illustrates the sampling distribution of $\rho$ estimated by OLS for each sample in the Monte Carlo simulation study. Consistent with our earlier observations, the depicted pattern remains unchanged: the empirical distribution excludes the true parameter $\rho_0 = 0.7$.

\begin{figure}[ht]
  \caption{Distribution of $\widehat{\rho}$}
    \label{fig:ols-rho-sim}
    \centering 
	\begin{minipage}{.9\linewidth}
<<ols-rho-sim, echo = FALSE, message = FALSE, fig.align='center', out.width = '8cm', out.height = '8cm'>>=
plot(density(rho_hat),
     xlab = expression(hat(rho)), 
     main = "",
     xlim = c(0.6, 1.3)
     )
abline(v = rho, col = "red")
@
\footnotesize
		\emph{Notes:} This graph shows the sampling distribution of $\rho$ estimated by OLS for each sample in the Monte Carlo simulation study. The true DGP follows a pure Spatial Lag Model where the true parameter is $\rho_0 = 0.7$
	\end{minipage}	
\end{figure}

%******************************************************
\section{Maximum Likelihood Estimation of SLM}\index{Maximum likelihood!SLM}
%******************************************************

The Maximum Likelihood (ML) estimation of spatial lag and spatial error regression models was initially formulated by \cite{ord1975estimation}. The foundation of this approach lies in assuming normality for the error terms. The joint likelihood, in turn, emerges from the multivariate normal distribution for the dependent variable $\vy$. Unlike the conventional MLE estimator, the joint log likelihood for a spatial regression model does not simply equate to the sum of log likelihoods associated with individual observations. This departure arises from the inherent spatial simultaneity within the spatial system.

In this section, we will give further insights about these issues. In particular, we derived the ML estimation procedure for the Spatial Lag Model following very close to \cite{ord1975estimation} and \citet[][chapter 6]{anselin1988spatial}.

%=========================================
\subsection{Maximum Likelihood Function}\index{Maximum likelihood}
%=========================================

The SLM is given by the following structural model:
\begin{equation}
  \begin{aligned}
    \vy     & = \rho_0 \mW\vy + \mX\vbeta_0 + \vepsi, \\
     \vepsi & \sim \rN(\vzeros_n , \sigma^2_0\mI_n),
  \end{aligned}
\end{equation}
%
where $\vy$ is an $n\times 1$ vector of the dependent variable for each spatial unit; $\mW$ is an $n\times n$ spatial weight matrix; $\mX$ is an $n \times k$ matrix of independent and exogenous variables; $\vbeta_0$ is a $k$-dimensional vector of parameters; $\rho_0$ measures represents the spatial autoregressive parameter; and $\vepsi$ is an $n$-dimensional vector of error terms. 

We assume the error terms are normally distributed with mean zero and variance-covariance $\sigma^2_0\mI_n$, implying homoskedasticity across spatial units. These distributional assumptions enable us to apply ML estimation. While the MLE exhibit desirable asymptotic properties such as consistency and efficiency under correct model specification, they can be sensitive to violations of the uderlying asssumption

The MLE seeks parameter values $\widehat{\vtheta} = (\widehat{\vbeta}^\top, \widehat{\rho}, \widehat{\sigma}^2)^\top$ that maximizes the probability of observing the sample at hand. 

% How can we estimate $\vtheta_0$? Note that rewrite the model as:
% \begin{equation*}
% \vy - \rho_0\mW\vy = \mX\vbeta_0 + \vepsi.
% \end{equation*}
% 
% Using the OLS estimator, an estimate for $\vbeta_0$ would be
% \begin{equation*}
% \widehat{\vbeta}(\rho_0) = \left(\mX^\top \mX\right)^{-1}\mX^\top\left(\mI_n - \rho_0\mW\right)\vy,
% \end{equation*}
% %
% which depend on $\rho_0$. Similarly, an estimate for the variance parameter would be
% \begin{equation*}
% \widehat{\sigma}^2(\rho_0) = \frac{\widehat{\vepsi}(\rho_0)^\top\widehat{\vepsi}(\rho_0)}{n},
% \end{equation*}
% %
% which also depends on $\rho_0$, and where the residuals $\widehat{\vepsi}(\rho_0)$ are given by $\widehat{\vepsi}(\rho_0)=\vy - \rho_0\mW\vy - \mX\widehat{\vbeta}$. Since $\widehat{\vbeta}$ and $\widehat{\sigma}^2$ depend on $\rho_0$, we can \textbf{concentrate} the full log-likelihood with respect to the parameters $\vbeta, \sigma^2$ and reduce maximum likelihood to an univariate optimization problem in the parameter $\rho$. This will be very useful later in order to derive the ML algorithm.

To derive the joint distribution of the data, we need to find the probability density function $f(y_1, y_2,\ldots,y_n| \mX; \vtheta) = f(\vy|\mX;\vtheta)$, that is, the joint conditional distribution of $\vy$ given $\mX$. Using the \textbf{Transformation Theorem}, we know that
\begin{equation*}
  f(\vy | \mX; \vtheta) = f\left[\vepsi(\vy)| \mX; \vtheta\right] \left|\frac{\partial \vepsi}{\partial \vy}\right|.
\end{equation*}
%
where $\left|\cdot\right|$ is the determinant function and $\left|\frac{\partial \vepsi}{\partial \vy}\right|$ is known as the \textbf{Jacobian}. The error vector is a function of $\vy$ as $\vepsi = \mA\vy - \mX\vbeta$, with $\mA=\mI_n - \rho\mW$.\footnote{Since $y_i$, and not $\epsilon_i$, are the observed quantities, the parameters must be estimated by maximizing $L(\vy)$, not $L(\vepsi)$. For more details about this, see \cite{mead1967mathematical} and \cite{doreian1981estimating}.} Here $\mA\vy$ is the \textbf{spatially filtered dependent variable}, i.e., with the effect of spatial autocorrelation taken out. 

The Jacobian term is\index{Maximum likelihood!Jacobian}:
\begin{equation*}
\det\left(\frac{\partial \vepsi}{\partial \vy}\right)= \det\left(\mJ\right)=\det(\mA)=\det(\mI_n - \rho\mW),
\end{equation*}
%
where $\mJ = \left(\frac{\partial \vepsi}{\partial \vy}\right)$ is an $n\times n$ matrix, and $\det(\mI_n - \rho\mW)$ is the determinant of an $n \times n$ matrix. In contrast to the time-series case, the spatial Jacobian is not the determinant of a triangular matrix, but of a full matrix. This complicates its computation considerably. Note that the Jacobian reduces to a scalar 1 in the standard regression model, since the partial derivative becomes $\left|\partial (\vy - \mX\vbeta)/ \partial \vy\right| = \left|\mI_n\right| = 1$. 

Using the density function of the multivariate normal distribution we can find the joint pdf of $\vepsi|\mX$.\footnote{The multivariate normal distribution of an $n$-dimensional random vector $\vx$ with mean $\vmu$ and variance-covariance matrix $\mSigma$ can be written as 
\begin{equation*}
(2\pi)^{-n/2}\det(\mSigma)^{-1/2}\exp\left(-\frac{1}{2}\left(\vx - \vmu\right)^\top\mSigma^{-1}\left(\vx - \vmu\right)\right).
\end{equation*}
} By recognizing that $\vepsi \sim \rN(\vzeros, \sigma^2\mI_n)$, we can write: 
\begin{equation*}
	f(\vepsi | \mX) = (2\pi \cdot\sigma^2)^{-n/2}\exp\left[-\frac{1}{2\sigma^2}\vepsi^\top\vepsi\right].
\end{equation*}

Given an i.i.d sample of $n$ observations, $\vy$ and $\mX$, the joint density of the observed sample is:
\begin{equation*}
	f(\vy|\mX;\vtheta) = (2\pi \cdot\sigma^2)^{-n/2}\exp\left[-\frac{1}{2\sigma^2}(\mA\vy-\mX\vbeta)^\top (\mA\vy-\mX\vbeta)\right]\det\left(\frac{\partial (\mA\vy - \mX\vbeta)}{\partial \vy}\right).
	\end{equation*}
	
Note that the likelihood function is defined as the joint density treated as a function of the parameters: $L(\vtheta|\vy, \mX) = f(\vy|\mX;\vtheta)$. Finally, the log-likelihood function, which will be maximized, takes the form\footnote{Since the constant $- \frac{n\log(2\pi)}{2}$ is not a function of any of the parameters, some software programs do not include it when reporting maximized log-likelihood. See \cite{spdep}.}
\begin{equation} \label{eq:LL_SLM_2}
  \begin{aligned}
\ell (\vtheta) &= \log\left| \mA\right| - \frac{n\log(2\pi)}{2} - \frac{n\log(\sigma^2)}{2} - \frac{1}{2\sigma^2}(\mA\vy-\mX\vbeta)^\top (\mA\vy-\mX\vbeta), \\
&= \log\left| \mA\right| - \frac{n\log(2\pi)}{2} - \frac{n\log(\sigma^2)}{2} - \frac{1}{2\sigma^2}\left[\vy^\top \mA^\top\mA\vy - 2\left(\mA\vy\right)^\top\mX\vbeta + \vbeta^\top\mX^\top\mX\vbeta\right],
\end{aligned}
\end{equation}	
%	
in which we use the fact that the transpose of a scalar is the scalar, i.e., 	$\vy^\top\mA^\top\mX\vbeta = (\vy^\top\mA\mX\vbeta)^\top = \vbeta^\top\mX^\top\mA\vy$. This is similar to the typical linear-normal likelihood, except that the transformation from $\vepsi$ to $\vy$, is not by the usual factor of 1, but by $\log\left| \mA\right|$.

As we will show in Section \ref{sec:code-full-slm}, we can directly estimate the $\vtheta$ by maximizing the log-likelihood function \eqref{eq:LL_SLM_2} using a constrained optimization algorithm. However, as shown in the next Section, we can create a more easy estimation algorithm by concentrating the log-likelihood function. 

%=========================================
\subsection{Score Vector and Estimates}\label{sec:score_sml}
%=========================================

To find the MLE for the SLM model, we need to maximize $\ell(\vtheta)$ in Equation \eqref{eq:LL_SLM_2} with respect to $\vtheta = (\vbeta^\top, \sigma^2 , \rho)^\top$. To do so, we need to find the first order condition (FONC) of this optimization problem. 

Before taking derivatives, it is useful to review some important properties of matrix calculus given in the next definition. 

%--------------------------------------------------------------------------------------
\begin{definition}[Some useful results on matrix calculus]\label{definition:matrix_cal_sar}
Some important results  are the followings:
\begin{equation}\label{eq:mc_1}
\frac{\partial (\rho \mW)}{\partial \rho} = \mW.
\end{equation}

\begin{equation}\label{eq:mc_2}
  \begin{aligned}
\frac{\partial \mA}{\partial \rho} & = \frac{\partial (\mI_n - \rho\mW)}{\partial \rho}, \\
                                   & = \frac{\partial \mI_n}{\partial \rho} - \frac{\partial \rho\mW}{\rho}, \\
                                   & = -\mW.
 \end{aligned}
\end{equation}

\begin{equation}\label{eq:mc_3}
\frac{\partial \log \left|\mA\right|}{\partial \rho}  = \tr(\mA^{-1}\partial \mA / \partial \rho) = \tr\left[\mA^{-1}(-\mW)\right].
\end{equation}

Let $\vepsi = \mA\vy - \mX\vbeta$, then:
\begin{equation}\label{eq:mc_4}
  \frac{\partial \vepsi}{\partial \rho} =  \frac{\partial (\mA\vy - \mX\vbeta)}{\partial \rho} = -\mW\vy. 
\end{equation}

\begin{equation}\label{eq:mc_5}
  \frac{\partial \vepsi^\top \vepsi}{\partial \rho} = \vepsi^\top (\partial \vepsi / \partial \rho) + (\partial \vepsi ^\top / \partial \rho)\vepsi = 2\vepsi^\top (\partial \vepsi / \partial \rho) = 2\vepsi^\top(-\mW)\vy.
\end{equation}

\begin{equation}\label{eq:mc_6}
  \frac{\partial \mA^{-1}}{\partial \rho} = -\mA^{-1}(\partial \mA / \partial \rho)\mA^{-1} = \mA^{-1} \mW \mA^{-1}.
\end{equation}

\begin{equation}\label{eq:mc_7}
  \frac{\partial \tr\left(\mA^{-1} \mW\right)}{\partial \rho} = \tr\left(\partial \mA^{-1}\mW / \partial \rho\right). 
\end{equation}
\end{definition}

Taking the derivative of Equation \eqref{eq:LL_SLM_2} with respect to $\vbeta$ yields
\begin{equation}\label{eq:foc_ml_sar_beta}
\frac{\partial \ell(\vtheta)}{\partial \vbeta} = -\frac{1}{2\sigma^2}\left[-2\left(\left(\mA\vy\right)^\top\mX\right)^\top + 2 \mX^\top\mX\vbeta\right] = \frac{1}{\sigma^2}\mX^\top(\mA\vy - \mX\vbeta),
\end{equation}
%
and with respect to $\sigma^2$ yields
\begin{equation}\label{eq:foc_ml_sar_sigma}
\frac{\partial \ell(\vtheta)}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4}\left(\mA\vy - \mX\vbeta\right)^\top\left(\mA\vy - \mX\vbeta\right).
\end{equation}

Solving both Equation \eqref{eq:foc_ml_sar_beta} and \eqref{eq:foc_ml_sar_sigma}, we obtain:
\begin{eqnarray}
	\widehat{\vbeta}_{ML}(\rho) &=& \left(\mX^\top\mX\right)^{-1}\mX^\top\mA\vy \label{eq:beta_ML}, \\
	\widehat{\sigma}^2_{ML}(\rho) &=& \frac{\left(\mA\vy - \mX\vbeta_{ML}\right)^\top\left(\mA\vy - \mX\vbeta_{ML}\right)}{n}\label{eq:sigma_ML}.
\end{eqnarray}

Note that conditional on $\rho$ (assuming we know $\rho$), these estimators are simply OLS estimators applied to the \emph{spatial filtered} dependent variable $\mA\vy$ and the exploratory variables $\mX$. Moreover, after some manipulation, Equation \eqref{eq:beta_ML} can be rewritten as
\begin{equation}\label{eq:beta_Ml_2}
\begin{aligned}
\widehat{\vbeta}_{ML}(\rho) &= \left(\mX^\top\mX\right)^{-1}\mX^\top\vy - \rho\left(\mX^\top\mX\right)^{-1}\mX^\top\mW\vy, \\
&= \widehat{\vbeta}_O -\rho \widehat{\vbeta}_L.
\end{aligned}
\end{equation}

Note that the first term in \eqref{eq:beta_Ml_2} is just the OLS regression of $\vy$ on $\mX$, whereas the second term is just $\rho$ times the OLS regression of $\mW\vy$ on $\mX$. Next, define the following identities:
\begin{equation}\label{eq:SLM_aux_residuals}
\ve_O \equiv \vy - \mX\widehat{\vbeta}_0\,\,\mbox{and} \;\; \ve_L \equiv \mW\vy - \mX\widehat{\vbeta}_L.
\end{equation}

Then, plugging \eqref{eq:beta_Ml_2} into \eqref{eq:sigma_ML} yiels
\begin{eqnarray}\label{eq:sigma_ml_con}
\widetilde{\sigma}^2(\rho) &=& \frac{\left(\ve_O - \rho\ve_L\right)^\top\left(\ve_O - \rho\ve_L\right)}{n}.
\end{eqnarray}

Note that both \eqref{eq:beta_Ml_2} and \eqref{eq:sigma_ml_con}) rely only on observables, except for $\rho$, and so are readily calculable given some estimate of $\rho$. Therefore, plugging \eqref{eq:beta_Ml_2} and \eqref{eq:sigma_ml_con} back into the likelihood \eqref{eq:LL_SLM_2}  we obtain the \textbf{concentrated log-likelihood function}\index{Maximum likelihood!concentrated log-likelihood}:
\begin{equation}\label{eq:concentrated_ml_sar_1}
\ell(\rho)=-\frac{n}{2}-\frac{n}{2}\log(2\pi) - \frac{n}{2}\log\left[\frac{\left(\ve_O - \rho\ve_L\right)^\top\left(\ve_O - \rho\ve_L\right)}{n}\right] + \log\left|\mI_n - \rho\mW\right|,
\end{equation}	
%	
which is a \textbf{nonlinear} function of a single parameter $\rho$. A ML estimate for $\rho$ is obtained from a numerical optimization of the concentrated log-likelihood function \eqref{eq:concentrated_ml_sar_1}. Once we obtain $\widehat{\rho}$, we can easily obtain $\widehat{\vbeta}$ and $\widehat{\sigma}^2$.  The procedure can be summarized in the following steps.

%--------------------------------------------------------------
\begin{algorithm}[ML estimation of SLM]\label{algorithm:SLM}
The algorithm to perform the ML estimation of the SLM is the following: 
\begin{enumerate}
	\item Perform the two auxiliary regression of $\vy$ and $\mW\vy$ on $\mX$ to obtain $\widehat{\vbeta}_O$ and $\widehat{\vbeta}_L$ as in Equation \eqref{eq:beta_Ml_2}.
	\item Use $\widehat{\vbeta}_O$ and $\widehat{\vbeta}_L$ to compute the residuals in Equation \eqref{eq:SLM_aux_residuals}. 
	\item Maximize the concentrated likelihood given in Equation \eqref{eq:concentrated_ml_sar_1} by numerical optimization to obtain an estimate of $\rho$.
	\item Use the estimate of $\widehat{\rho}$ to plug it back in to the expression for $\vbeta$ (Equation \ref{eq:beta_ML}) and $\sigma^2$	 (Equation \ref{eq:sigma_ML}).
\end{enumerate}	
\end{algorithm}
%--------------------------------------------------------------

Since the score function will be important for understanding the asymptotic theory of MLE, we will derive also $\partial \ell(\vtheta) / \partial \rho$. Taking the derivative of Equation \eqref{eq:LL_SLM_2} with respect to $\rho$, we obtain:
\begin{equation}\label{eq:der1_rho}
  \begin{aligned}
      \frac{\partial \ell(\vtheta)}{\partial \rho} & =  \left(\frac{\partial}{\partial \rho}\right)\log \left| \mA\right| -  \frac{1}{2\sigma^2}\left(\frac{\partial}{\partial \rho}\right)\vepsi^\top \vepsi, \\
      & = - \tr(\mA^{-1}\mW) +\frac{1}{2\sigma^2}2\vepsi^\top\mW\vy\quad\mbox{using \eqref{eq:mc_3} and \eqref{eq:mc_5}}, \\
      & = - \tr(\mA^{-1}\mW) +\frac{1}{2\sigma^2}2\vepsi^\top\mW\vy, \\
      & = - \tr(\mA^{-1}\mW) +\frac{1}{\sigma^2}\vepsi^\top\mW\vy. 
  \end{aligned}
\end{equation}

Thus the complete gradient (or score function) is\index{score function!SLM}:
\begin{equation}\label{eq:full_agradient}
  \nabla_{\vtheta} = \frac{\partial \ell(\vtheta)}{\partial \vtheta} = 
    \begin{pmatrix}
    \frac{\partial \log L(\vtheta)}{\partial \vbeta} \\
    \frac{\partial \log L(\vtheta)}{\partial \sigma^2} \\
    \frac{\partial \log L(\vtheta)}{\partial \rho}
    \end{pmatrix}
    =
    \begin{pmatrix}
    \frac{1}{\sigma^2}\mX^\top\vepsi \\
    \frac{1}{2\sigma^4}(\vepsi^\top\vepsi-n\sigma^2) \\
    - \tr(\mA^{-1}\mW) +\frac{1}{\sigma^2}\vepsi^\top\mW\vy
    \end{pmatrix},
\end{equation}
%
where $\vepsi = \mA\vy - \mX\vbeta$.

% Note that if we replace $\vy = \mA^{-1}\mX\vbeta + \mA^{-1}\vepsi$ in Equation \eqref{eq:der1_rho}, we get
% \begin{equation*}
%  \frac{\partial \ell(\vtheta)}{\partial \rho} = \frac{1}{\sigma^2}(\mC\mX\vbeta)^\top \vepsi + \frac{1}{\sigma^2}(\vepsi^\top\mC\vepsi - \sigma^2 \tr(\mC)),
% \end{equation*}
% %
% where:
% \begin{equation}\label{eq:Cmatrix}
%   \mC = \mW\mA^{-1}.
% \end{equation}
% 
% This expression will be useful later. 

%**********************
\subsection{Hessian}\label{sec:Hessian}\index{Hessian!SLM}
%***********************

The Hessian matrix plays a critical role in subsequent sections, particularly for deriving the asymptotic variance-covariance matrix. To facilitate this, we dedicate this section to deriving the Hessian matrix for the SLM. 

The Hessian is a $(k + 2)\times (k + 2)$ matrix of second derivatives, expressed as:
\begin{equation*}
	\mH(\vbeta, \sigma^2, \rho) = 
	\begin{pmatrix}
		\frac{\ell(\vbeta, \sigma^2,\rho)}{\partial \vbeta \partial \vbeta^\top} & \frac{\ell(\vbeta, \sigma^2,\rho)}{\partial \vbeta \partial \sigma^2} & \frac{\ell(\vbeta, \sigma^2,\rho)}{\partial \vbeta \partial \rho} \\
		\frac{\ell(\vbeta, \sigma^2,\rho)}{\partial \sigma^2 \partial \vbeta^\top} & \frac{\ell(\vbeta, \sigma^2,\rho)}{\partial (\sigma^2)^2} & \frac{\ell(\vbeta, \sigma^2,\rho)}{\partial \sigma^2 \partial \rho} \\
		\frac{\ell(\vbeta, \sigma^2,\rho)}{\partial \rho \partial \vbeta^\top} & \frac{\ell(\vbeta, \sigma^2,\rho)}{\partial \rho \partial \sigma^2} & \frac{\ell(\vbeta, \sigma^2,\rho)}{\partial \rho^2}
	\end{pmatrix}. 
\end{equation*}

Using the first-order condition for $\vbeta$ from \eqref{eq:foc_ml_sar_beta}, the second derivatives are:
\begin{align}
  \frac{\partial^2 \ell(\vtheta)}{\partial \vbeta \partial \vbeta^\top}  & =  - \frac{1}{\sigma^2}(\mX^\top \mX), \label{eq:sar_der_beta_beta} \\
  \frac{\partial^2  \ell(\vtheta)}{\partial \vbeta \partial \sigma^2} &= - \frac{1}{(\sigma^2)^2} \mX^\top \vepsi,\label{eq:sar_der_beta_sigma} \\
  \frac{\partial^2  \ell(\vtheta)}{\partial \vbeta \partial \rho}  & =  - \frac{1}{\sigma^2} \mX^\top \mW\vy\label{eq:sar_der_beta_rho}.
\end{align}

Using the first-order condition for $\sigma^2$ from \eqref{eq:foc_ml_sar_sigma}, we obtain
\begin{equation}\label{eq:sar_der_sigma_sigma}
	\frac{\partial^2 \ell(\vtheta)}{\partial (\sigma^2)^2}  = \frac{n}{2(\sigma^2) ^2} - \frac{1}{(\sigma^2)^3} \vepsi^\top \vepsi,
\end{equation}
%
and:
\begin{equation}\label{eq:sar_der_sigma_rho}
\begin{aligned}
\frac{\partial^2 \ell(\vtheta) }{\partial \sigma^2 \partial \rho}  & = \frac{1}{2\sigma ^ 4}\left[2\vepsi^\top \left(\frac{\partial \vepsi}{\partial \rho}\right)\right] \quad \mbox{using Equation \eqref{eq:mc_5}},\\
& =  - \frac{1}{\sigma ^ 4}\vepsi ^\top \mW\vy, \\
& =  -\frac{\vepsi^\top\mW\vy}{\sigma ^ 4}.
\end{aligned}
\end{equation}

Finally, working in the second derivatives for $\rho$, and using \eqref{eq:der1_rho}, we obtain
\begin{equation}\label{eq:sar_der_rho_rho}
\begin{aligned}
\frac{\partial^2 \ell(\vtheta) }{\partial \rho^2}  & = -\left(\frac{\partial }{\partial \rho}\right)\tr(\mA^{-1}\mW) + \frac{1}{\sigma^2}\left(\frac{\partial }{\partial \rho}\right)\vepsi^\top\mW\vy, \\
& = - \tr\left( \frac{\partial \mA^{-1}\mW}{\partial \rho}\right) + \frac{1}{\sigma^2}\left(\frac{\partial }{\partial \rho}\right)(\mA\vy)^\top\mW\vy, \\
& = - \tr\left( \mA^{-1}\mW\mA^{-1}\mW \right)+ \frac{1}{\sigma^2}(- \vy^{\top}\mW^{\top}\mW\vy), \\
& = - \tr\left[(\mW\mA^{-1})^2\right] - \frac{1}{\sigma^2}(\vy^{\top}\mW^{\top}\mW\vy).
\end{aligned}
\end{equation}

Combining the results, the Hessian matrix for the SLM is:
\begin{equation}\label{eq:hessian_sml}
	\mH(\vbeta, \sigma^2, \rho) = 
	\begin{pmatrix}
	- \frac{1}{\sigma^2}(\mX^\top \mX) & - \frac{1}{(\sigma^2)^2} \mX^\top \vepsi & - \frac{1}{\sigma^2} \mX^\top \mW\vy \\
		 .& \frac{n}{2(\sigma^2) ^2} - \frac{1}{(\sigma^2)^3} \vepsi^\top \vepsi & -\frac{\vepsi^\top\mW\vy}{\sigma ^ 4} \\
		 .& . & - \tr\left[(\mC)^2\right] - \frac{1}{\sigma^2}(\vy^{\top}\mW^{\top}\mW\vy)
	\end{pmatrix} 
\end{equation}
%
which is symmetric and $\mC = \mW\mA^{-1}$.

%=============================
\subsection{Ord's Jacobian}
%=============================

A key feature of the concentrated log-likelihood function in Equation \eqref{eq:concentrated_ml_sar_1} is the Jacobian term $\left|\mI_n - \rho\mW\right|$. This term presents computational challenges, 
as estimating $\widehat{\rho}$ requires evaluating the determinant of the $n\times n$ matrix $\left|\mI_n - \rho\mW\right|$ at each iteration. However, \cite{ord1975estimation} provided a significant simplification based on the eigen values of $\mW$. Specifically, \cite{ord1975estimation} noted that:
\begin{equation*}
	\left|\omega\mI_n -\mW\right|=\prod_{i=1}^n(\omega-\omega_i),
\end{equation*}	
%
where $\omega_i$ are the eigenvalues of $\mW$. Consequently, for $\mI_n - \rho\mW$, the determinant can be expressed as:
\begin{equation*}
\left|\mI_n -\rho\mW\right|=\prod_{i=1}^n(1-\rho\omega_i).
\end{equation*}	

The corresponding log-determinant term follows as:
\begin{equation}\label{eq:Ord-determinant}
\log\left|\mI_n -\rho\mW\right|=\sum_{i=1}^n\log(1 - \rho\omega_i).
\end{equation}

This formulation offers a significant computational advantage: the eigenvalues of $\mW$ only need to be computed once. While the initial computation has some overhead, it drastically reduces the computational burden during iterative evaluations of the log-likelihood. For datasets with more than 4,000 observations, Ord's method is typically much faster than direct determinant computation.


This eigenvalue-based approach also delineates the admissible domain for $\rho$. We need that $1 - \rho \omega_i \neq 0$, which occurs only if $1/\omega_{min} < \rho < 1/\omega_{max}$. For row-standardized matrix, the largest eigenvalues is 1. 

With this new approximation, the new concentrated log-likelihood function is:
\begin{equation}\label{eq:concentrated_ml_sar_2}
  \begin{aligned}
\ell(\rho)& =-\frac{n}{2}-\frac{n}{2}\log(2\pi) - \frac{n}{2}\log\left[\frac{\left(\ve_O - \rho\ve_L\right)^\top\left(\ve_O - \rho\ve_L\right)}{n}\right] + \sum_{i=1}^n\log(1 - \rho\omega_i).
  \end{aligned}
\end{equation}	

An alternative to Ord's approach is the characteristic root method outlined by \citet{smirnov2001fast}. This method facilitates the estimation of spatial lag models for extremely large datasets (> 100,000 observations) in very short computational time. However, it is limited by the requirement that the weight matrix needs to be intrinsically symmetric. This precludes the use of asymmetric weight such as $k$-nearest neighbor weights. For further approximations and methods,  see \citet[][chapter 4]{lesage2010introduction}.

%**************************************
\section{Maximum Likelihood Estimation of SEM}\label{sec:sem-ml}
%*************************************

\subsection{What Are The Consequences of Applying OLS on a SEM Model?}

As discussed in Section \ref{sec:tax_SEM}, one approach to account for spatial autocorrelation in regression models is to model the error term as a spatial process.  The SEM model is given by
\begin{equation}\label{eq:sem_ml}
	\begin{aligned}
	\vy  & = \mX\vbeta_0 + \vu, \\ 
	 \vu & = \lambda_0 \mW \vu + \vepsi, \\
	 \vepsi & \sim \rN(\vzeros, \sigma^2_0\mI_n),
	\end{aligned}
\end{equation}
%
where $\lambda_0$ is the spatial autoregressive coefficient for the error lag term $\mW\vu$ (to distinguish the notation from the spatial autoregressive coefficient $\rho$ in a spatial lag model), $\mW$ is the spatial weight matrix, $\vepsi$ is the error term such that $\vepsi \sim N(\vzeros, \sigma^2_0\mI_n)$. 

The SEM does not require a theoretical justification for the spatial process but is consistent with situations where omitted determinants of the dependent variable are \textbf{spatially autocorrelated}, or with a situation where unobserved shocks follow a spatial pattern \citep{elhorst2014spatial}. In essence, SEM treats spatial correlation primarily as a nuisance. 

When $\lambda_0 > 0$, the error term exhibits positive spatial correlation, implying clustering of similar values. This means the errors for a spatial unit $i$ systematically vary with the errors of nearby observations  $j$. For example so that smaller/larger errors for $i$ would tend to go together with smaller/larger errors for $j$. This violates the typical assumption of no autocorrelation in the error term of the OLS. 

The reduced form of the SEM can be expressed as
\begin{equation*}
	\vy = \mX\vbeta_0 + \left(\mI_n-\lambda_0\mW\right)^{-1}\vepsi = \mX\vbeta_0 + \vu, 
\end{equation*}
%
where $\vu = (\mI_n - \lambda_0\mW)^{-1}\vepsi$. It follows that $\E(\vu| \mW, \mX) = \vzeros$, and the variance-covariance matrix of $\vu$ is given by: 
\begin{equation}\label{eq:cov_error_sem}
\var(\vu| \mW, \mX)=\E(\vu\vu^\top| \mW, \mX) = \sigma^2_0(\mI_n-\lambda_0\mW)^{-1}(\mI-\lambda_0\mW^\top)^{-1}=\sigma^2_0\mOmega_{u}^{-1},
\end{equation}
%
where $\mOmega_{u}=(\mI_n-\lambda_0\mW)(\mI_n-\lambda_0\mW)^\top$. The variance covariance \eqref{eq:cov_error_sem} is a full matrix, implying a spatial autoregressive error process leading to a nonzero error covariance between every pair of observations, but decreasing in magnitude with the order of contiguity \citep{AnselinBera1998}. Furthermore, the complex structure in the inverse matrix in \eqref{eq:cov_error_sem} yields non constant diagonal elements in the error covariance matrix, thus inducing heteroskedasticity in $\vu$, irrespective of the heteroskedasticity of $\vepsi$. Finally, $\vu \sim \rN(\vzeros, \sigma^2_0\mOmega_u^{-1})$.

\begin{remark}\label{remark:ols_sem}
  The OLS estimates of model in Equation (\ref{eq:sem_ml}) are unbiased, but inefficient if $\lambda_0 \neq 0$.
\end{remark}
The inefficiency of OLS in this context arises because the presence of spatially correlated errors invalidates the assumption of spherical error variance. As a result:
\begin{enumerate}
\item \textbf{Inefficient parameter estimation}: OLS does not account for the spatial structure in the errors, leading to less precise estimates of $\vbeta_0$.
\item \textbf{Invalid statistical inference}: The inefficiency in OLS estimation biases the variance and standard error estimates for $\vbeta_0$, rendering significance test unreliable.
\end{enumerate}

To address these issues, generalized least squares (GLS) should be used for more efficient parameter estimation. GLS accounts for the spatial covariance structure in $\mOmega_u^{-1}$, producing valid inference for the SEM model. 

%----------------------------------------
\subsection{Log-likelihood function}
%----------------------------------------

The SEM model defined in Equation \eqref{eq:sem_ml} implies the following transformation for the residual term:
\begin{equation*}
\vepsi = \left(\mI_n-\lambda_0\mW\right)\vy - \left(\mI_n-\lambda_0\mW\right)\mX\vbeta_0 = \mB_0\vy - \mB_0\mX\vbeta_0,
\end{equation*}
%
where $\mB_0 = (\mI_n-\lambda_0\mW)$. 

To derive the log-likelihood function, we first need the joint density function. Using the Transformation Theorem, the joint density of $\vy$ is obtained as:
\begin{equation*}
  f(y_1,\ldots ,y_n|\mX; \vtheta) = f(\vepsi(\vy)| \mX ; \vtheta) \cdot \left|\mJ\right|, 
\end{equation*}
%
where $\mJ$ is the Jacobian of the transformation. The Jacobian is
\begin{equation*}
\mJ = \frac{\partial \vepsi}{\partial \vy} =\mB_0.
\end{equation*}

Thus, the joint density function of $\vepsi$---which is a function of $\vy$---is:
\begin{equation*}
  f(\vepsi(\vy)| \mX ; \vtheta) = (2\pi \sigma^2)^{-n/2}\exp\left[- \frac{\left[\left(\mI_n-\lambda\mW\right)(\vy - \mX\vbeta)\right]^\top \left[\left(\mI_n-\lambda\mW\right)(\vy - \mX\vbeta)\right]}{2\sigma^2}\right],
\end{equation*}
%
and the joint density function of $\vy$, $f(y_1,\ldots,y_n|\mX; \vtheta)$ equals
\begin{equation*}
  f(\vy| \mX ; \vtheta) = (2\pi \sigma^2)^{-n/2}\exp\left[- \frac{(\vy - \mX\vbeta)^\top \mB^\top \mB (\vy - \mX\vbeta)}{2\sigma^2}\right]\cdot \left|\mB\right|
\end{equation*}

Finally, the log-likelihood can be expressed as
\begin{equation}\label{eq:ll_sem}
\ell(\vtheta) = - \frac{n}{2}\log(2\pi) - \frac{n}{2}\log(\sigma^2)-\frac{(\vy - \mX\vbeta)^\top \mOmega(\lambda) (\vy - \mX\vbeta)}{2\sigma^2} + \log\left|\mI_n - \lambda \mW\right|,
\end{equation}
%
where 
\begin{equation*}
\mOmega(\lambda) = \mB^\top \mB = \left(\mI_n-\lambda\mW\right)^\top \left(\mI_n-\lambda\mW\right).
\end{equation*}

Again, we run into complications over the log of the determinant $\left|\mI_n - \lambda \mW\right|$, which is an $n$th-order polynomial that is cumbersome to evaluate. 

\subsection{Score Function and ML Estimates}\label{sec:sem_ml_estimates}

Maximizing the log-likelihood function \eqref{eq:ll_sem} is equivalent to minimizing the sum of the transformed errors, $\vepsi^\top\vepsi$, adjusted by the Jacobian term, $\log\left|\mI_n - \lambda \mW\right|$. This adjustment ensures that the maximum likelihood (ML) estimates differ from the ordinary least squares (OLS) estimates. However, the two coincide as $\lambda \to 0$. 

To derive the ML estimates, we apply the first-order necessary conditions (FONC) to the log-likelihood function  \eqref{eq:ll_sem}. Taking the derivative with respect to $\vbeta$ yields:
\begin{equation}\label{eq:beta_gls_ml}
  \begin{aligned}
\vbeta_{ML}(\lambda) & =\left[\mX^\top\mOmega(\lambda)\mX\right]^{-1}\mX^\top\mOmega(\lambda)\vy,\\
                     & = \left[(\mB\mX)^\top (\mB \mX)\right]^{-1} (\mB\mX)^\top \mB \vy, \\
                     & = \left[\mX(\lambda)^\top  \mX(\lambda)\right]^{-1} \mX(\lambda)^\top \vy(\lambda),
  \end{aligned}
\end{equation}
%
where:
\begin{equation*}
  \begin{aligned}
\mX(\lambda) & = \mB\mX = (\mI - \lambda\mW)\mX = (\mX - \lambda\mW\mX), \\
\vy(\lambda) & = (\vy - \lambda \mW \vy).
  \end{aligned}
\end{equation*}

When $\lambda$ is known, this estimator is equivalent to the generalized least squares (GLS) estimator---$\widehat{\beta}_{ML} = \widehat{\beta}_{GLS}$---and it can be interpreted as the OLS estimate obtained from regressing $\vy(\lambda)$ on $\mX(\lambda)$. In other words, for a known value of the spatial autoregressive coefficient, $\lambda$, this is equivalent to OLS on the transformed variables.

\begin{remark}
In the literature, the transformations:
\begin{eqnarray*}
\mX(\lambda) &=&(\mX - \lambda\mW\mX), \\
\vy(\lambda) &=& (\vy - \lambda \mW \vy),
\end{eqnarray*}
%
are known as the \emph{Cochrane-Orcutt transformation}\index{Cochrane-Orcutt transformation}.
\end{remark}

Similarly, the FONC of \eqref{eq:ll_sem} with respect to $\sigma^2$ gives the MLE for the error variance:
\begin{equation}
	\sigma^2_{ML}(\lambda) = \frac{1}{n}\left(\widehat{\vepsi}^\top\mB^\top\mB \vepsi\right)= \frac{1}{n}\widehat{\vepsi}^\top(\lambda)\widehat{\vepsi}(\lambda),
\end{equation}
%
where $\widehat{\vepsi} = \vy - \mX\widehat{\vbeta}_{ML}$ and $\widehat{\vepsi}(\lambda) = \mB(\lambda)(\vy - \mX\widehat{\vbeta}_{ML}) = \mB(\lambda)\vy - \mB(\lambda)\mX\widehat{\vbeta}_{ML}$. 

First order condition derived from the expression of the likelihood are highly non-linear and therefore the likelihood in Equation \eqref{eq:ll_sem} cannot be directly maximized. Again, a concentrated likelihood approach is necessary.

The estimators for $\vbeta$ and $\sigma^2$ are both functions of the value of $\lambda$. A concentrated log-likelihood can then be obtained as:
\begin{equation}
	\ell(\lambda)= \mbox{const} + \frac{n}{2}\log\left[\frac{1}{n}\widehat{\vepsi}^\top\mB^\top\mB \widehat{\vepsi}\right] + \log\left|\mB\right|, 
\end{equation}
%
where the residual vector indirectly depends on $\lambda$. An iterative optimization procedure is generaly required to estimate all parameters. 

The iterative procedure for ML estimation of the SEM, based on \cite{anselin1988spatial}, can be summarized as follows:
\begin{algorithm}[ML estimation of SEM]
Following \cite{anselin1988spatial}, the procedure can be summarize in the following steps:
\begin{enumerate}
		\item Carry out an OLS of $\mB\mX$ on $\mB\vy$; get $\widehat{\vbeta}_{OLS}$
		\item Compute initial set of residuals $\widehat{\epsilon}_{OLS} = \mB\vy - \mB\mX\widehat{\vbeta}_{OLS}$
		\item Given $\widehat{\epsilon}_{OLS} $, find $\widehat{\lambda}$ that maximizes the concentrated likelihood.
		\item If the convergence criterion is met, proceed, otherwise repeat steps 1, 2 and 3.
		\item Given $\widehat{\lambda}$, estimate $\widehat{\vbeta}(\lambda)$ by GLS and obtain a new vector of residuals, $\widehat{\vepsi}(\lambda)$
		\item Given  $\widehat{\vepsi}(\lambda)$ and $\widehat{\lambda}$, estimate $\widehat{\sigma}(\lambda)$.
\end{enumerate}	
\end{algorithm}


Finally, the asymptotic variance-covariance matrix is:
\begin{equation}\label{eq:asyvar_sem}
\mbox{AsyVar}(\vbeta, \sigma^2, \lambda)  = 
\begin{pmatrix}
 \underset{k \times k}{\frac{\mX(\lambda)\top\mX(\lambda)}{\sigma^2}} & 0 & 0 \\
  0 & \frac{n}{2\sigma^4} & \frac{\tr(\mW_B)}{\sigma^2} \\
 0 & \frac{\tr(\mW_B)}{\sigma^2} & \tr(\mW_B)^2 + \tr(\mW_B^\top\mW_B)
\end{pmatrix}^{-1},
\end{equation}
%
where $\mW_B = \mW(\mI - \lambda\mW)^{-1}$.

%====================================
\section{Asymptotic Properties of SLM}
%====================================

This section reviews the asymptotic properties of Maximum Likelihood (ML) and Quasi-Maximum Likelihood (QML) estimators for the Spatial Lag Model (SLM), following the foundational work of \cite{lee2004asymptotic}. We focus on consistency and asymptotic normality.


%------------------------------------
\subsection{Consistency of QMLE}
%------------------------------------

Consider the true SLM given by:
\begin{equation*}
\vy_n = \mX_n\vbeta_0 + \lambda_0\mW_n\vy_n + \vepsi_n,
\end{equation*}
%
where all variables and the weight matrix $\mW_n$ are indexed by \(n\) to explicitly denote their dependence on the sample size.

\cite{lee2004asymptotic} establishes the asymptotic properties (consistency and asymptotic normality) of the ML and QML estimator under specific regularity conditions. 

To begin, we impose the following assumption on the error terms \(\vepsi_n\):

%----------------------------------------------------------------------------
\begin{assumption}[Errors \citep{lee2004asymptotic}]\label{assu:ml_1}
Assume the following
  \begin{enumerate}
    \item The disturbances $\left\lbrace \epsilon_{i}\right\rbrace, i = 1, \ldots, n$, in $\vepsi_n = (\epsilon_i, \ldots, \epsilon_n)^\top$ are i.i.d with mean zero and variance $\sigma^2$. Its moment $\E\left(|\epsilon|^{4 + \gamma}\right)$ for some $\gamma > 0$ exits.
  \end{enumerate}  
\end{assumption}
%----------------------------------------------------------------------------

Assumption \ref{assu:ml_1} ensures homoskedasticity of the errors and guarantees the existence of finite variances for quadratic forms involving \(\vepsi_n\). This is critical for deriving the asymptotic properties of the QMLE, as it enables the application of the Central Limit Theorem (CLT).

In order to understand the asymptotic behavior of $\mW_n$ under some \textbf{regularity conditions}, we need to understand some useful terminologies.

%------------------------------------------------------------------------
\begin{definition}[Triangular array of constants]\label{def:tria_array_const}
Let $\left\lbrace b_{ni} \right\rbrace, i = 1, \ldots,n$ be a triangular array of constants.

\begin{enumerate}
  \item $\left\lbrace b_{ni} \right\rbrace$ are at most of order $(1/h_n)$, denoted by $O(1/h_n)$ uniformly in $i$ if there exists a finite constant $c$ independent of $i$ and $n$ such that $\left|b_{ni}\right|\leq \frac{c}{h_n}$ for all $i$ and $n$.
  \item $\left\lbrace b_{ni} \right\rbrace$ are bounded away from zero uniformly in $i$ at rate of $h_n$ if there exists a positive sequence $\left\lbrace h_{n} \right\rbrace$ and a constant $c>0$ independent of $i$ and $n$ such that $c\leq \left|b_{ni}\right|/ h_n$ for all $i$ for sufficiently large $n$.
\end{enumerate}
\end{definition}
%------------------------------------------------------------------------

In spatial econometrics, the spatial weight matrix $\mW_n$ is often conceptualized as a triangular array, reflecting the fact that its structure changes as the sample size increases (see Section \ref{sec:triangular-array}). For instance, the element $w_{ij}$ in $\mW_n$ may differ when $n = 50$ versus $n = 55$. This indexing scheme explicitly acknowledges such changes, and the elements of $\mW_n$ are denoted by $w_{n,ij}$.

A natural question arises regarding the boundedness of the elements of $\mW_n$. Definition \ref{def:tria_array_const} provides a precise characterization of sequences that are bounded or diverging. In this context, we impose the following assumption:

%----------------------------------------------------------------------
\begin{assumption}[Weight Matrix \citep{lee2004asymptotic}]\label{assu:ml_2}
	The elements $w_{n,ij}$ of $\mW_n$ are at most of order $h_n^{-1}$, denoted by $O(1/h_n)$, uniformly in all $i,j$, where the rate sequence ${h_n}$ can be \textbf{bounded} or \textbf{divergent}. As a normalization, $w_{n,ii} = 0$ for all $i$.
\end{assumption}
%----------------------------------------------------------------------

Recall that if $X_n = O(b_n)$, then
\begin{equation*}
\lim_{n\to \infty}\frac{X_n}{b_n} = -\infty < c < \infty.
\end{equation*}

This implies that $X_n$ is a bounded sequence of rate $b_n$. Assumption \ref{assu:ml_2} states that the elements of $\mW_{n}$ are sequences that might be bounded or divergent at rate $h_n$. That is, we do not know if $h_nw_{n,ij}$ is bounded or divergent.

%--------------------------------------------------------------
\begin{assumption}[\citep{lee2004asymptotic}]\label{assu:ml_3}
	The ratio $h_n/n \to 0$ as $n$ goes to infinity
\end{assumption}
%--------------------------------------------------------------

Assumptions \ref{assu:ml_2} and \ref{assu:ml_3} establish a direct connection between the spatial weight matrix and the sample size $n$. Intuitively, as the sample size $n$ increases, the row sums of the weight matrix $\mW_n$ are expected to grow, as regions can potentially have more neighbors (see the discussion in Section \ref{sec:triangular-array}). The rate of growth of the spatial weights $w_{n,ij}$ with $n$ can be either bounded (limited number of neighbors) or divergent (unlimited number of neighbors). Assumptions \ref{assu:ml_2} and \ref{assu:ml_3} are intended to cover weight matrices whose elements are not restricted to be nonnegative and those that might not be row-standardized. 

What are the implications of those assumption? These assumptions deal with the row and column sums of $\mW_n$. Specifically, the row and column sums of $\mW_n$,  before row-normalization, should not diverge to infinity at a rate equal to or faster than the rate of the sample size $n$. This contrasts slightly with the conditions in \cite{kelejian1998generalized} and \cite{kelejian1999generalized}, which require that the row and column sums of $\mW$ and $(\mI_n - \rho\mW)^{-1}$, before row-normalization, remain uniformly bounded in absolute value as $n \to \infty$. Both sets of conditions aim to ensure that the cross-sectional correlation remains manageable, meaning that the correlation between two spatial units diminishes as the distance separating them increases to infinity.  


In practical terms, these assumptions often imply that no spatial unit is assumed to have more than a fixed number, say $q$, of neighbors. This limitation ensures that the conditions in \cite{lee2004asymptotic} and \cite{kelejian1998generalized, kelejian1999generalized} are satisfied.  

However, these conditions may not hold if the spatial weight matrix is defined as an inverse distance matrix. To illustrate, consider an infinite number of spatial units arranged linearly. Let the distance between each spatial unit and its nearest left and right neighbors be $d$, its next-nearest neighbors $2d$, and so on. In such cases, the conditions in \cite{kelejian1998generalized, kelejian1999generalized} may be violated. See Figure \ref{fig:example_lineal} for an illustration. 

\begin{figure}[h]
\caption{Distances from R3 to all Regions}
\label{fig:example_lineal}
\centering
\begin{tikzpicture}[scale = 1.5]
\node (R1) at (.5, .5) {R1};
\node (R2) at (1.5,.5) {R2};
\node (R3) [color =  red]  at (2.5,.5) {R3};
\node (R4) at (3.5,.5) {R4};
\node (R5) at (4.5,.5) {R5};
\draw [->] (R3) -- (R2) node [midway, below] {$d$};
\draw [->] (R3) -- (R4) node [midway, below] {$d$};
\draw [->] (R2) -- (R1) node [midway, below] {$2d$};
\draw [->] (R4) -- (R5) node [midway, below] {$2d$};
\end{tikzpicture}
\end{figure}


When $\mW_n$ is an inverse distance matrix, its off-diagonal elements are defined as $w_{ij} = 1/d_{ij}$, where $d_{ij}$ represents the distance between two spatial units $i$ and $j$. The row sum of $\mW_n$ for any given $i$ is:
\begin{equation*}
  \sum_{j = 1}^n w_{ij} = 1/d + 1/d + 1/2d + 1/2d + \cdots = 2 \times (1/d + 1/2d + 1/3d + \cdots),  
\end{equation*}
%
which represents a harmonic series that diverges to infinity. 

This divergence is one of the main reasons why some empirical applications introduce a cutoff distance $d^*$, such that $w_{ij} = 0$ if $d_{ij} > d^*$. Imposing this cutoff ensures that the row sums remain finite and avoids potential numerical issues. 

However, note that even without a cutoff, the ratio of the row sum to the sample size,  
\begin{equation*}  
\frac{2 \times \left( \frac{1}{d} + \frac{1}{2d} + \frac{1}{3d} + \cdots \right)}{n},  
\end{equation*}  
converges to zero as $n \to \infty$. Therefore, the condition proposed by \cite{lee2004asymptotic} is satisfied, implying that an inverse distance matrix without a cutoff does not necessarily violate consistency requirements.  
Assumption \ref{assu:ml_3} excludes cases where the row sums, $\sum_{j=1}^n w_{ij}$ for $i = 1, \ldots, n$, diverge to infinity at a rate equal to or faster than the sample size $n$. In such cases, the maximum likelihood (ML) estimator would likely be inconsistent.  

A special case where the sequence $\{h_n\}$ is bounded arises when the number of neighbors is fixed, as in the $k$-nearest neighbors approach. However, the example of inverse distance weights highlights why these matrices can sometimes lead to numerical problems or unexpected outcomes in empirical applications. This is because, in practice, the sample size $n$ is typically finite and does not approach infinity, leading to potential complications with unbounded row sums.

%Assumptions~\ref{assu:ml_2} is always satisfied if $\left\lbrace h_n \right\rbrace$ is a bounded sequence. Think of $\mW_n$ as spatial weight matrix based on distances that is row-normalized. In this case the $i$th row

%\begin{equation*}
%  w_{i,n} = \frac{(d_{i1}, d_{i2}, ...,d_{in})}{\sum_{j = 1}^nd_{ij}}
%\end{equation*}

%where $d_{ij} > 0$ represents a function of the spatial distance of the $i$th and $j$th units in some space. For a row-normalized $\mW_n$, as $d_{i,j}$ are nonnegative constants and uniformly bounded, if the $\sum_{j = 1}^nd_{ij}, i = 1,...,n$ are uniformly bounded away from zero at the rate $h_n$ in the sense that $\sum_{j = 1}^nd_{ij} =O(h_n)$ uniformly in $i$ and $\lim \inf _{n\to \infty} h_n^{-1}\sum_{j = 1}^nd_{ij} > c$, where $c$ is a positive constant independent of $i$ and $n$, the implied normalized weights matrix will have the property of Assumption \ref{assu:ml_2}.

What happens if $h_n$ is unbounded? In this case $\sum_{j = 1}^nd_{ij}$ is uniformly bounded away from zero at the rate $h_n$, where $\lim_{n\to \infty}h_n = \infty$. This particular case \textbf{rules out} cases where each unit has only a fixed, finite number of neighbors, even as the total number of unit increases to infinity. For example, it excludes scenarios where spatial units correspond to counties, and neighbors are defined as those with contiguous border.

When does $h_n \to \infty$? This case requires that each unit in the limit has infinitely many neighbors. As noted by \cite{lee2002consistency}, in economic applications where (i) the neighbors of each unit are densely distributed in a relevant space, or (ii) each unit is significantly influenced by a large proportion of the total population units, it is likely that $\sum_{j=1}^n d_{ij}$ diverges and $(1/n)\sum_{j=1}^n d_{ij}$ converges as $n \to \infty$.   

For example, consider $d_{ij} = 1 / \left|r_i - r_j\right|$, where $r_i$ is the proportion of state $i$'s population of African descent. Since no state in the U.S. has a zero proportion of African-Americans, $d_{ij}> 0$. Here, $(1/n)\sum_{j = 1}^nd_{ij}$ will remain bounded away from zero, while $\sum_{j = 1}^nd_{ij}$ will diverge at a rate proportional to $n$.

Another case arises when all cross-sectional units are assumed to be neighbors of one another, with equal weights assigned. If all off-diagonal elements of the spatial weights matrix are $w_{ij} = 1$, then the row and column sums become $n - 1$, diverging to infinity as $n \to \infty$. However, in this scenario, $(n - 1)/n \to 1$ rather than converging to zero. Consequently, a spatial weight matrix with equal weights and subsequent row-normalization, $w_{ij} = 1/(n - 1)$, violates both \cite{lee2004asymptotic}'s and \cite{kelejian1998generalized, kelejian1999generalized}'s conditions, making it unsuitable for consistent estimation.  

Alternatively, a group interaction matrix, as introduced by Case (1991), satisfies the conditions. In this setup, ``neighbors'' refer to individuals within the same district. Suppose there are $R$ districts and $m$ individuals per district, with a total sample size $n = mR$. Each neighbor within a district is given equal weight, and the spatial weight matrix is defined as $\mW_n = \mI_R \otimes \mB_m$, where $\mB_m = (\vones_m \vones_m^\top - \mI_m)/(m - 1)$.  

Here, $h_n = m - 1$, and $h_n/n = (m - 1)/(mR) = O(1/R)$. If the sample size $n$ increases through growth in both $R$ and $m$, $h_n \to \infty$ and $h_n/n \to 0$ as $n \to \infty$. Thus, this matrix satisfies \cite{lee2004asymptotic}'s condition.  

\begin{remark}
  The boundedness or divergence of $\{h_n\}$ has significant implications for OLS estimation. When $\{h_n\}$ is bounded, the OLS estimators of $\vbeta$ and $\rho$ are inconsistent. However, when $\{h_n\}$ diverges, these estimators can be consistent \citep[see][]{lee2002consistency}. 
\end{remark}

In summary, when $\{h_n\}$ is a bounded sequence, it implies that each unit has a fixed, small number of neighbors, typically based on geographical proximity. In contrast, when $\{h_n\}$ diverges, it reflects scenarios where each unit interacts with a large number of neighbors, which frequently occurs in empirical studies of social networks, economic spillovers, or cluster sampling data.

%------------------------------------------------------------------------------------------------
\begin{assumption}[Non-singularity of $\mA_n$ \citep{lee2004asymptotic}]\label{assu:ml_4}
	The matrix $\mA_n$ is nonsingular.
\end{assumption}
%------------------------------------------------------------------------------------------------

Under Assumption \ref{assu:ml_4}, the SLM (system) has the reduced form (equilibrium) given by
\begin{equation*}
\vy_n = \mA^{-1}_{n0}(\mX_n\vbeta_0 + \vepsi_n),
\end{equation*}
%
where $\mA^{-1}_{n0} = (\mI_n - \rho_0\mW_n)$ and with the following expectation and variance:
\begin{eqnarray*}
\E(\vy_n)   &=& \left(\mI_n - \rho_0\mW_n\right)^{-1}\mX_n\vbeta  = \mA_{n0}^{-1}\mX_n\vbeta_0,\\
\var(\vy_n) &=& \E(\vy_n\vy_n^\top) = \sigma^2_{0}\left(\mI_n - \rho_0\mW_n\right)^{-1}\left[\left(\mI_n - \rho_0\mW_n\right)^{-1}\right]^\top = \sigma^2_0\mA_{n0}^{-1}(\mA_{n0}^{-1})^\top.
\end{eqnarray*}

For further reference, note that we can write the reduced-form equation as follows:
\begin{equation*}
	\begin{aligned}
	\vy_n & = \mX_n\vbeta_0 + \rho_0\mW_n\vy_n + \vepsi_n, \\
	      & = \mX_n\vbeta_0 + \rho_0\mW_n\left[\mA^{-1}_{n0}\mX_n\vbeta_0 + \mA^{-1}_{n0}\vepsi_n\right] + \vepsi_n, \\
	      & = \mX_n\vbeta_0 + \rho_0\mW_n\mA^{-1}_{n0}\mX_n\vbeta_0 + \rho_0\mW_n\mA^{-1}_{n0}\vepsi_n + \vepsi_n, \\
	      & = \mX_n\vbeta_0 + \rho_0\mW_n\mA^{-1}_{n0}\mX_n\vbeta_0  + \left(\mI_n + \rho_0\mW_n\mA^{-1}_{n0}\right)\vepsi_n, \\
	      & = \mX_n\vbeta_0 + \rho_0\mC_{n0}\mX_n\vbeta_0  + \left(\mI_n + \rho_0\mC_{n0}\right)\vepsi_n, \\
	      & = \mX_n\vbeta_0 + \rho_0\mC_{n0}\mX_n\vbeta_0  +\mA_{n0}^{-1}\vepsi_n,
	\end{aligned}
\end{equation*}
%
because $\mI_n + \rho_0\mC_n = \mA_{n0}^{-1}$ (see Exercise  \ref{lab:4.6}), where $\mC_{n0}=\mW_n\mA_{n0}^{-1}$. 

%-------------------------------------------------------------------------------------
\begin{assumption}[Uniform boundedness  \citep{lee2004asymptotic}]\label{assu:ml_5}
	The sequences of matrices $\left\lbrace \mW_n \right\rbrace$ and $\left\lbrace \mA_n^{-1} \right\rbrace$ are uniformly bounded in both row and column sums 
\end{assumption}
%-------------------------------------------------------------------------------------

The uniform boundedness of the matrices is a condition to limit the spatial correlation to a manageable degree. For example, it guarantees that the variances of $\vy_n$ are bounded as $n$ goes to infinity. See our discussion in Section \ref{sec:bounded-matrices}.

Why do we care about this? Because we need the variance goes to zero when the sample size goes to infinity in order to apply some consistency theorem.\footnote{Equivalently, this assumption rules out the unit root case in time series.}

%-------------------------------------------------------------------------------------

\begin{lemma}[Uniform Boundedness of Matrices in Row and Column Sums]
  Suppose that the spatial weights matrix $\mW_n$ is a non-negative matrix with its $(i,j)$th element being 
  \begin{equation*}
    w_{n,ij} = \frac{d_{ij}}{\sum_{l = 1}^nd_{il}}
  \end{equation*}
  %
  and $d_{ij}>0$ for all $i,j$.
  
  \begin{enumerate}
    \item If the row sums $\sum_{j = 1}^nd_{ij}$ are bounded away from zero at the rate $h_n$ uniformly in $i$, and the column sums $\sum_{i = 1}^nd_{ij}$ are $O(h_n)$ uniformly in $j$, then $\left\lbrace \mW_n \right\rbrace$  are uniformly bounded in column sums.
    \item (Symmetric Matrix) If $d_{ij} = d_{ji}$ for all $i$ and $j$ and the row sums $\sum_{j = 1}^nd_{ij}$ are $O(h_n)$ and bounded away from zero at the rate $h_n$ uniformly in $i$, then $\left\lbrace \mW_n \right\rbrace$  are uniformly bounded in column sums.
  \end{enumerate}
\end{lemma}
%-------------------------------------------------------------------------------------

%-------------------------------------------------------------------------------------

\begin{assumption}[No asymptotic multicollinearity \citep{lee2004asymptotic}]\label{assu:ml_6}
	The elements of $\mX_n$ are uniformly bounded constants for all $n$. The $\lim_{n\to \infty}\mX_n^\top\mX_n/n$ exists and is nonsingular. 
\end{assumption}
%-------------------------------------------------------------------------------------

This rules out multicollinearity among the regressors. Note also that we are assuming that $\mX_n$ is \textbf{nonstochastic}. If $\mX_n$ were stochastic, then we will require:
\begin{equation*}
  \plim_{n\to \infty}\mX_n^\top\mX_n/ n,
\end{equation*}
%
to exists.

%-------------------------------------------------------------------------------------

\begin{assumption}[Uniform Boundedness of ${\mA_n^{-1}(\rho)}$ \cite{lee2004asymptotic}]\label{assu:ml_7}
	${\mA_n^{-1}(\rho)}$ are uniformly bounded in either row or column sums, uniformly in $\rho$ in a compact parameter space $\Gamma$. The true parameter $\rho_0$ is in the interior of $\Gamma$
\end{assumption}
%-------------------------------------------------------------------------------------

This assumption is needed to deal with the nonlinearity of $\log \left|\left(\mI_n - \rho \mW\right)^{-1}\right|$ in the log-likelihood function. Recall that if $\left\| \rho\mW\right\| < 1$, then $\mI_n - \rho\mW_n$ is invertible for all $n$. Then if $\left\| \rho\mW\right\| < 1$, then the sequence of matrices $\left\|\left(\mI_n- \rho\mW_n\right)^{-1}\right\|$ are uniformly bounded in any subset of $(-1, 1)$ bounded away from the boundary. As we previously see, if $\mW_n$ is row-standardized $\left(\mI_n- \rho\mW\right)^{-1}$ is uniformly bounded in row sums norm uniformly in any closed subset of $(-1, 1)$. Therefore, $\Gamma$ from Assumption~\ref{assu:ml_7} can be considered as a single closed set contained in (-1, 1).

What if $\mW_n$ is not row-normalized but its eigenvalues are real? Then, the Jacobian of $\left|\left(\mI_n- \rho\mW\right)^{-1}\right|$ will be positive if $-1/\omega_{min} < \rho < 1/\omega_{max}$, where $\omega_{min}$ and $\omega_{max}$ are the minimum and maximum eigenvalues of $\mW_n$, and $\Gamma$ will be a closed interval contained in $(-1/\omega_{min}, 1/\omega_{max})$ for all $n$. Thus, Assumption~\ref{assu:ml_7} rules out models where $\rho_0$ is close to -1 and 1.  

%-------------------------------------------------------------------------------------
\begin{assumption}[Identification \citep{lee2004asymptotic}]\label{assu:ml_8}
		The
		\begin{equation*}
			\lim_{n\to \infty}\frac{1}{n}\left(\mX_n, \mC_n\mX_n\vbeta_0\right)'\left(\mX_n, \mC_n\mX_n\vbeta_0\right)
		\end{equation*}
		%
		 exists and is nonsingular.
\end{assumption}
%-------------------------------------------------------------------------------------

This is a sufficient condition for global identification of $\vtheta_0$.

%-------------------------------------------------------------------------------------
\begin{theorem}[Consistency]\label{teorem:Consistency_ML}
	Let $\vtheta_0 = (\vbeta_0^\top, \rho_0, \sigma_0^2)^\top$. Under assumption \ref{assu:ml_1}-\ref{assu:ml_8}, $\vtheta_0$ is globally identifiable and $\widehat{\vtheta}_n$ is a consistent estimator of $\vtheta_0$.
\end{theorem}
%-------------------------------------------------------------------------------------


Identification of $\rho_0$ can be based on the maximum values of the concentrated log-likelihood function $Q_n(\rho) / n$. With identification and uniform convergence of $\left[\log L_n(\rho) - Q_n(\rho)\right] / n$ to zero on $\Gamma$, consistency of the QMLE $\widehat{\vtheta}_n$ follows. The sketch of the proof for Theorem \ref{teorem:Consistency_ML} is given in Appendix \ref{appendix-consistency}. 

For a proof without compactness of the parameter space (proving concavity of the log-likelihood function) see \cite{liu2022consistency}.

%=====================================
\subsection{Asymptotic Normality}
%=====================================

To derive the asymptotic distribution of the QML and ML we need the asymptotic behavior of the gradient. Taking a Taylor series expansion around $\vtheta_0$ of $\partial \ell_n(\widehat{\vtheta}_n)/ \partial \vtheta = 0$ at $\vtheta_0$, we get
\begin{equation*}
  \frac{\partial \ell_n(\widehat{\vtheta}_n)}{\partial \vtheta} =  \frac{\partial \ell_n(\vtheta_0)}{\partial \vtheta} + \frac{\partial^2 \ell_n(\widetilde{\vtheta}_n)}{\partial \vtheta \partial \vtheta^\top}(\widehat{\vtheta}_n - \vtheta_0),
\end{equation*}
%
where $\widetilde{\vtheta}_n = \alpha_n \widehat{\vtheta}_n + (1 - \alpha_n)\vtheta_0$ and $\alpha_n\in \left[ 0, 1\right]$, therefore:
\begin{equation}\label{eq:sampling-error-ml}
  \sqrt{n}(\widehat{\vtheta}_n - \vtheta_0) = - \left[\frac{1}{n}\frac{\partial^2 \ell_n(\widetilde{\vtheta}_n)}{\partial \vtheta \partial \vtheta^\top}\right]^{-1}\frac{1}{\sqrt{n}}\frac{\partial \ell_n(\vtheta_0)}{\partial \vtheta}.
\end{equation}

As standard in asymptotic theory of MLE, we need to show that the first element of the rhs of \eqref{eq:sampling-error-ml} converges to some finite matrix. We also need to find the limiting distribution of $\frac{1}{\sqrt{n}}\frac{\partial \ell_n(\vtheta_0)}{\partial \vtheta}$. Recall that the first-order derivatives of the log-likelihood function \textbf{evaluated at $\vtheta_0$} are given by (see Section \ref{sec:score_sml}):
\begin{align}
	\frac{1}{\sqrt{n}}\frac{\partial \ell_n(\vtheta_0)}{\partial \vbeta} & = \frac{1}{\sigma^2_0\sqrt{n}}\mX_n^\top\vepsi_n \\
	\frac{1}{\sqrt{n}}\frac{\partial \ell_n(\vtheta_0)}{\partial \sigma^2} & = \frac{1}{2\sigma_0^4\sqrt{n}}\left(\vepsi_n'\vepsi_n - n\sigma_0^2\right) \\
		\frac{1}{\sqrt{n}}\frac{\partial \ell_n(\vtheta_0)}{\partial \rho} & = \frac{1}{\sigma_0^2\sqrt{n}}(\mC_{n0}\mX_n\vbeta_0)^\top \vepsi_n + \frac{1}{\sigma_0^2\sqrt{n}}(\vepsi_n^\top\mC_{n0}\vepsi_n - \sigma_0^2\tr(\mC_{n0}))\label{eq:asy_der_rho}
\end{align}	

As explained by \citet[][pag. 1905]{lee2004asymptotic}, these are linear and quadratic functions of $\vepsi_n$. In particular, the asymptotic distribution of \eqref{eq:asy_der_rho} may be derived from central limit theorem for linear-quadratic forms. The matrix $\mC_{n0}$ is uniformly bounded in row sums. As the elements of $\mX_n$ are bounded, the elements of $\mC_{n0}\mX_n\vbeta_0$ for all $n$ are uniformly bounded by Lemma \ref{lemma:bounded_lemma}. With the existence of high order moments of $\epsilon$ in Assumption \ref{assu:ml_1}, the central limit theorem for quadratic forms of double arrays of \cite{kelejian2001asymptotic} can be applied and the limit distribution of the score vector follows.

Let $\vtheta = (\vbeta^\top, \rho, \sigma^2)^\top$ be the $k+2$-dimensional vector. Since $\E\left[(1 / \sqrt{n}) \partial \ell_n (\vtheta_0)/\partial \vtheta\right]  = \vzeros$,  the variance matrix of $(1 / \sqrt{n}) \partial \ell_n(\vtheta_0) /\partial \vtheta$ is:
\begin{equation*}
  \E\left[\frac{1}{\sqrt{n}}\frac{\partial \ell_n (\vtheta_0)}{\partial \vtheta}\cdot\frac{1}{\sqrt{n}}\frac{\partial \ell_n (\vtheta_0)}{\partial \vtheta^\top} \right] = - \E\left(\frac{1}{n} \frac{\partial \ell_n(\vtheta_0)}{\partial \vtheta \partial \vtheta^\top}\right) + \mOmega_{\vtheta, n},
\end{equation*}
%
where 
\begin{equation}\label{eq:expected-hessian-slm-asy}
- \E\left(\frac{1}{n} \frac{\partial^2 \ell_n(\vtheta_0)}{\partial \vtheta \partial \vtheta^\top}\right)= 
	\begin{pmatrix}
	\frac{1}{n\sigma^2_0}(\mX_n^\top \mX_n)  & \frac{1}{n\sigma^2_0} \mX_n^\top (\mC_n\mX_n\vbeta_0) & \vzeros^\top \\
	   & \frac{1}{n}\tr(\mC^s_{n0}\mC_{n0}) + \frac{1}{n\sigma^2_0}(\mC_{n0}\mX_n\vbeta_0)^\top(\mC_{n0}\mX_n\vbeta_0) &  \frac{1}{n\sigma^2_0} \tr(\mC_{n0}) \\
		 &  & \frac{1}{2\sigma^4_0}
	\end{pmatrix} 
\end{equation}
%
and $\mC^s_{n0} = \mC_{n0} + \mC_{n0}^\top$. Equation \eqref{eq:expected-hessian-slm-asy} represents the average Hessian matrix (or information matrix when $\vepsi$'s are \textbf{normal}). The matrix $ \mOmega_{\vtheta, n}$ is 
\begin{equation}\label{eq:Omega-slm-ml}
\mOmega_{\vtheta, n}  = \begin{pmatrix} 
                            \vzeros & * & * \\
                            \frac{\mu_{3}}{n\sigma_0^4}\sum_{i = 1}^n\mC_{n, ii}\vx_{i, n} & \frac{2\mu_3}{n\sigma_0^4}\sum_{i = 1}^n\mC_{n, ii}\mC_{n, ii}\mX_m\vbeta_0 + \frac{(\mu_4 - 3\sigma_0^4)}{n\sigma_0^4}\sum_{i = 1}^n\mC_{n, ii}^2 & * \\
                            \frac{1}{n2\sigma_0^6}\left[\mu_3\vones_n^\top\mG_n\mX_n\vbeta_0 + (\mu_4 -3\sigma_0^4)\tr(\mG_n)\right] & \frac{(\mu - 3\sigma_0^4)}{4\sigma_0^8}
                         \end{pmatrix}
\end{equation}
%
which is a symmetric matrix with the second, third, and fourth moments of $\vepsi$. If $\vepsi_n$ is normally distributed, then $\mOmega_{\vtheta, n} = \mZeros$. 

Derivation of \eqref{eq:expected-hessian-slm-asy} is given in Appendix \ref{appendix-EH-sml} and the variance of the score function is given in Appendix \ref{appendix-score}.


%---------------------------------------------------
\begin{theorem}[Asymptotic Normality]\label{teo:asymptotic-normality-slm}
Under Assumptions \ref{assu:ml_1}-\ref{assu:ml_8}, 
\begin{equation}
  \sqrt{n}\left(\widehat{\vtheta}_n- \vtheta_0\right)\dto \rN\left(\vtheta, \mSigma_{\vtheta}^{-1} + \mSigma_{\vtheta}^{-1}\mOmega_{\vtheta}\mSigma_{\vtheta}^{-1}\right),
\end{equation}
%
where $\mOmega_{\vtheta} = \lim_{n\to\infty}\mOmega_{\vtheta, n}$ and
\begin{equation}
  \mSigma_{\vtheta}= - \lim_{n\to\infty} \E\left[\frac{1}{n}\frac{\partial^2 \log L_n(\vtheta_0)}{\partial \vtheta \partial \vtheta^\top}\right],
\end{equation}
which are assumed to exists. If the $\epsilon_i$'s are \textbf{normally distributed}, then:
\begin{equation}
  \sqrt{n}\left(\widehat{\vtheta}_n- \vtheta_0\right)\dto \rN\left(\vtheta, \mSigma_{\vtheta}^{-1}\right).
\end{equation}
\end{theorem}
%-------------------------------------

A sketch of the proof of Theorem \ref{teo:asymptotic-normality-slm} is given in Appendix \ref{appendix-asymptotic normality}.

%=====================================
\section{Computing the Standard Errors For The Marginal Effects}
%=====================================

In section \ref{sec:summary-measures}, we explain how to obtain summary measures for the direct, indirect and total effects. However, we did not explain how to obtain standard errors for such measures. For example, we would like to have confidence intervals for the indirect effects and to be able to say whether they are significant. 

Recall that our three summary measures are:
\begin{eqnarray*}
\bar{M}(\vtheta)_{\mbox{direct}} & = & n^{-1}\tr\left[\mS_r(\vtheta)\right], \\
\bar{M}(\vtheta)_{\mbox{total}} & = & n^{-1}\vones_n^\top\mS_r(\vtheta)\vones_n, \\
\bar{M}(\vtheta)_{\mbox{indirect}} & = & \bar{M}(r)_{\mbox{total}} - \bar{M}(r)_{\mbox{direct}},
\end{eqnarray*}
%
which are highly nonlinear due to $\mS_r(\vtheta)$.\footnote{Note that we have replaced the parameter for the spatially lagged independent variable to let $\vtheta$ be the vector parameters of the model. } Therefore, a procedure such as the Delta Method is difficult to perform. Instead, we can use a Monte Carlo approximation which takes into account the sampling distribution of $\vtheta$. To show this procedure, consider the SDM where:
\begin{equation*}
\mS(\vtheta)_r = \left(\mI_n - \rho\mW\right)^{-1}\left(\mI_n\beta_r + \mW\gamma_r\right).
\end{equation*}

Let $g(\vtheta)= \bar{M}(\vtheta)$ be a function representing the marginal (direct, indirect or total) effect that depends on the population parameters $\vtheta$. If $\rN(\vtheta|\bar{\vtheta}, \mSigma_{\theta})$ denotes the multivariate normal density of $\vtheta$ with mean $\bar{\vtheta}$ and asymptotic variance-covariance matrix $\mSigma_{\theta}$, then the expected value of the marginal effects conditional on the population parameters $\bar{\vtheta}$ and $\mSigma_{\theta}$ is:
\begin{equation*}
\E\left[g(\vtheta)|\bar{\vtheta}, \mSigma_{\theta}\right] = \int_{\vtheta}\E\left[g(\vtheta)|\vy, \mX, \vtheta\right]\rN(\vtheta|\bar{\vtheta}, \mSigma_{\theta})d\vtheta. 
\end{equation*}

A Monte Carlo approximation to this expectation is obtained by calculation of the empirical marginal effects evaluated at pseudo draws of $\vtheta$ from the asymptotic distribution of the estimator. The algorithm is the following:

%------------------------------------------------------------------
\begin{algorithm}[Standard Errors of the Marginal Effects]
Estimate the model using MLE. Consider $s = 1, \ldots, S$, and start with $s = 1$
\begin{enumerate}
  \item Take a random draw of $\vtheta^{s}$ from $\rN(\widehat{\bar{\vtheta}}, \widehat{\mSigma}_{\theta})$, which is the estimated asymptotic distribution of $\widehat{\vtheta}$. 
 \item Compute the marginal effect, but substituting  $\widehat{\vtheta}$ for $\vtheta^s$. 
 \item Update $s = s + 1$, and go back to step 1. 
 \item Repeat for a large number of repetitions S (e.g., S = 1000).  
 \item Calculate the empirical mean of the marginal effects. The standard error of the marginal effect across the $S$ draws is the standard error. 
\end{enumerate}
\end{algorithm}
%------------------------------------------------------------------

%====================================
\section{Spillover Effects on Crime: An Application in R}\label{sec:Anselin-example}
%====================================


\subsection{Estimation of Spatial Models in R}

This example uses the dataset from \citet{anselin1988spatial}, which provides a cross-sectional view of 49 neighborhoods in Columbus, Ohio. The goal is to explain the crime rate as a function of household income and housing values. The dataset includes the following variables:

\begin{itemize}
  \item \texttt{CRIME}: residential burglaries and vehicle thefts per thousand household in the neighborhood.
  \item \texttt{HOVAL}: housing value in US\$1,000.
  \item \texttt{INC}: household income in US\$1,000.
\end{itemize}

We begin the analysis by loading the necessary \proglang{R} packages into the workspace:

<<message = FALSE, warning=FALSE>>=
# Load packages
library("spdep")
library("tmap")
library("spatialreg")
library("memisc")            # Package for tables
library("RColorBrewer") 
library("classInt")
source("getSummary.sarlm.R") # Function for spdep models
@

The dataset is available in the \pkg{spdep} package, and we load it as follows:

<<load-columbus, warning = FALSE>>=
# Load data
columbus <- st_read(system.file("shapes/columbus.gpkg", package="spData")[1], 
                           quiet = TRUE)
col.gal.nb <- poly2nb(columbus)
@

Before estimating models, it is essential to determine whether there is a spatial pattern in the crime variable. To visualize this, we create a choropleth map for \texttt{CRIME}:

<<spatial-crime-false, eval = FALSE >>=
# Spatial distribution of crime
tm_shape(columbus) +
  tm_polygons("CRIME",
              title = "Crime Status",
              parlette = "RdYBu"
              ) +
  tm_layout(frame = FALSE) 
@

The map in Figure~\ref{fig:spatial-crime} suggests a positive spatial autocorrelation in crime rates. To confirm this, we perform Moran's I test using a row-normalized binary contiguity matrix, \texttt{col.gal.nb}, based on the Queen criterion. The Moran test is performed using Monte Carlo simulations with 99 replications:

\begin{figure}[ht]
\caption{Spatial Distribution of Crime in Columbus, Ohio Neighborhoods}
    \label{fig:spatial-crime}
    \centering 
	\begin{minipage}{.9\linewidth}
<<spatial-crime, echo = FALSE, message = FALSE, fig.align='center', out.width = '9cm', out.height = '9cm'>>=
par(mar = c(0.1, 0.1, 0.1, 0.1))
tm_shape(columbus) +
  tm_polygons("CRIME",
              title = "Crime Status",
              parlette = "RdYBu"
              ) +
  tm_layout(frame = FALSE) 
@
\footnotesize
		\emph{Notes:} This graph shows the spatial distribution of crime on the 49 Columbus, Ohio neighborhoods. Darker color indicates greater rate of crime. 
	\end{minipage}	
\end{figure}


<<moran-oldcol>>=
# Moran's I test
set.seed(1234)
listw <- nb2listw(col.gal.nb, style = "W")
moran.mc(columbus$CRIME, listw = listw, 
           nsim = 99, alternative = 'greater')
@

The Moran's I statistic is 0.51 with a p-value of 0.01, providing evidence of positive spatial autocorrelation. This indicates that neighborhoods with high (low) crime rates tend to be surrounded by neighborhoods with similarly high (low) crime rates. 

Next, we estimate various spatial models using functions from the \pkg{spatialreg} package. These models include:

\begin{itemize} 
\item \textbf{OLS}: Estimated using the \texttt{lm} function. 
\item \textbf{SLX}: Estimated with the \texttt{lm} function, using the \texttt{lag.listw} function from \pkg{spdep} to create $\mW\mX$. Alternatively, the \texttt{lmSLX} function from \pkg{spatialreg} can be used. \item \textbf{SLM}: Estimated using the \texttt{lagsarlm} function from \pkg{spatialreg}. 
\item \textbf{SDM}: Estimated with \texttt{lagsarlm} using the \texttt{type = "mixed"} argument. Alternatively, \texttt{type = "Durbin"} may be specified. 
\item \textbf{SEM}: Estimated using the \texttt{errorsarlm} function. The Spatial Durbin Error Model (SDEM) can be estimated with \texttt{type = "emixed"}. 
\item \textbf{SAC}: Estimated with the \texttt{sacsarlm} function. 
\end{itemize}

These models are estimated using Maximum Likelihood (ML) methods, as outlined in the previous section. To compute the determinant of the Jacobian, we follow the approach of \citet{ord1975estimation} and explicitly set \texttt{method = "eigen"} in the spatial model functions, ensuring consistency with Equation~\eqref{eq:Ord-determinant}.
<<models-crime>>=
# Models
columbus$lag.INC   <- lag.listw(listw, 
                         columbus$INC)   # Create spatial lag of INC
columbus$lag.HOVAL <- lag.listw(listw, 
                         columbus$HOVAL) # Create spatial lag of HOVAL
ols <- lm(CRIME ~ INC + HOVAL, 
          data =  columbus)        
slx <- lm(CRIME ~ INC + HOVAL + lag.INC + lag.HOVAL, 
          data =  columbus)
slm <- lagsarlm(CRIME ~ INC + HOVAL, 
                data = columbus,
                listw, 
                method = "eigen")
sdm <- lagsarlm(CRIME ~ INC + HOVAL, 
                data = columbus,
                listw, 
                method = "eigen",
                type = "mixed")
sem <- errorsarlm(CRIME ~ INC + HOVAL, 
                data = columbus,
                listw,
                method = "eigen")
sac <- sacsarlm(CRIME ~ INC + HOVAL, 
                data = columbus,
                listw,
                method = "eigen")
@

Note that the SLX model can also be estimated as follows:

<<slx-spdep, eval = FALSE>>=
slx2 <- lmSLX(CRIME ~ INC + HOVAL, 
              data = columbus, 
              listw)
summary(slx2)
@


The results of the estimations are presented in Table \ref{tab:columbus-models}. Column 1 reports the OLS estimates. The findings suggest that, on average, an increase of one thousand dollars in neighborhood income is associated with a reduction of 1.6 crimes per thousand households. Similarly, an increase of one thousand dollars in housing value is associated with a reduction of 0.3 crimes per thousand households. Both coefficients are statistically significant.\footnote{We use the term "associated" because potential endogeneity issues in either variable could affect causal interpretation.} These results imply that residential burglaries and vehicle thefts are less prevalent in wealthier neighborhoods.

Column 2 shows the results for the SLX model, specified as 
\[
\vy = \mX\vbeta + \mW\mX\vgamma + \vepsi,
\]
where \(\mW\mX\) is a \(49 \times 2\) matrix representing the spatial lags of \texttt{INC} and \texttt{HOVAL}. The coefficient for the spatial lag of income (\texttt{W.INC}) is negative and statistically significant, indicating that crime in a given neighborhood is inversely related to the income levels of its neighboring areas. This suggests that higher income among neighbors is associated with lower crime levels in the focal neighborhood. Conversely, the coefficient for the spatial lag of housing value (\texttt{W.HOVAL}) is positive but not statistically significant, implying no clear relationship between the housing values of neighboring areas and crime.

Column 3 presents the results for the Spatial Lag Model (SLM). The spatial autoregressive parameter \(\rho\) is positive and significant, providing strong evidence of spatial autocorrelation and spillover effects in crime. The coefficients for the explanatory variables are qualitatively similar to the OLS results, though smaller in absolute magnitude, reflecting the influence of incorporating spatial dependence.

The Spatial Durbin Model (SDM) results are reported in column 4. The estimated \(\rho\) parameter remains positive and significant, confirming the presence of endogenous spatial interaction effects. However, the coefficients for the spatially lagged explanatory variables are not statistically significant. This suggests that once endogenous interaction effects in crime are accounted for, the socio-economic characteristics of neighboring areas do not significantly influence crime in the focal neighborhood. Furthermore, the coefficient for the spatial lag of income exhibits an unexpected positive sign, which contradicts the common factor hypothesis. This discrepancy suggests that the primary spatial effect may stem from an omitted spatial lag variable rather than spatial dependence in the error term.

Column 5 contains the results for the Spatial Error Model (SEM). The autoregressive parameter for \(\mW\vu\) is positive and significant, indicating substantial spatial transmission of random shocks. This finding may reflect the omission of spatially correlated variables, which contributes to the propagation of unobserved shocks across neighboring areas.

Finally, the Spatial Autoregressive Combined (SAC) model results are shown in column 6. This model accounts for both endogenous interaction effects and interactions among error terms. The coefficients for \(\mW\vy\) and \(\mW\vu\) are not statistically significant when estimated jointly. However, when these interaction effects are separated, both coefficients become significant. This suggests that the SAC model may be overparameterized, leading to reduced statistical power and diminished significance levels for all variables.


\begin{table}[ht]
\caption{Spatial Models for Crime in Columbus, Ohio Neighborhoods.}\label{tab:columbus-models}
\centering
<<echo = FALSE, results = 'asis', warning=FALSE>>=
table_1 <- mtable("OLS" = ols,
                  "SLX" = slx,
                  "SLM" = slm,
                  "SDM" = sdm,
                  "SEM" = sem, 
                  "SAC" = sac, 
       summary.stats = c("AIC", "N"),
       coef.style = "default")
table_1 <- relabel(table_1,
                   "(Intercept)" = "\\emph{Constant}",
                   "rho" = "$\\rho$",
                   "lambda" = "$\\lambda$",
                   "lag.INC" = "$W.INC$",
                   "lag.HOVAL" = "$W.HOVAL$") 
toLatex(table_1, compact = TRUE, useBooktabs =  TRUE)
@
\end{table}

%---------------------------------------------------
\subsection{Estimation of Marginal Effects in R}
%---------------------------------------------------

In this section, we extend the analysis from Section \ref{sec:lesage-example} by integrating the estimation of marginal effects using a real-world application in \proglang{R}. 

We begin by addressing the following question: What would happen to crime rates across all regions if income increased from 13.906 to 14.906 in the 30th region ($\Delta\texttt{INC} = 1$)? This is analogous to the question posed in the commuting-time example in the previous chapter. Following the approach in Section \ref{sec:lesage-example}, we use the reduced-form predictor defined by the formula:
\begin{equation*}
\widehat{\vy} = \E(\vy| \mX, \mW) = (\mI_n - \widehat{\rho}\mW)^{-1}\mX\widehat{\vbeta},
\end{equation*}
% 
to estimate the predicted values before and after the change in the income variable. Using the observed values of the exogenous variables and the reduced-form predictor, we compute the predicted values for \code{CRIME}, denoted as $\widehat{\vy}^1$, based on the previously estimated SLM model.

<<y-hat-pre>>=
# The predicted values
rho       <- slm$rho                                # Estimated rho from SLM model
beta_hat  <- coef(slm)[-1]                          # Estimated parameters
A         <- invIrW(listw, rho = rho)               # (I - rho*W)^{-1}
X         <- cbind(1, columbus$INC, columbus$HOVAL) # Matrix of observed variables
y_hat_pre <- A %*% crossprod(t(X), beta_hat)        # y hat
@

Next, we increase \code{INC} by 1 in spatial unit 30 and calculate the reduced-form predictions, $\widehat{\vy}^2$, as follows:

<<y-hat-post>>=
# The post-predicted values
col_new <- columbus # copy the data frame

# Change the income value
col_new[30, "INC"] <- 14.906

# The predicted values
X_d        <- cbind(1, col_new$INC, col_new$HOVAL)
y_hat_post <- A %*% crossprod(t(X_d), beta_hat)
@

Finally,  we compute the difference between pre- and post-predictions: $\widehat{\vy}^2 - \widehat{\vy}^1$:
<<diff-predicts>>=
# The difference
delta_y         <- y_hat_post - y_hat_pre
col_new$delta_y <- delta_y

# Show the effects
summary(delta_y)
sum(delta_y)
@

According to the results from \code{sum(delta\_y)}, the predicted effect of this income increase is a decrease of 1.65 crimes per thousand households, accounting for both direct and indirect effects. In other words, increasing income by \$1,000 in region 30 leads to a system-wide adjustment, resulting in a new equilibrium where total crime decreases by approximately 1.7 crimes per thousand households.

Sometimes it is useful to visualize these effects. For instance, we might want to identify regions with high and low impacts due to the increase in \code{INC}. Let us define ``highly impacted regions'' as those where the crime rate decreases by more than 0.05. The following code generates Figure \ref{fig:predicted-effect}, which illustrates these regions:

<<predicted-effect-evalF, eval = FALSE>>=
tm_shape(col_new) +
  tm_polygons(col = "delta_y",
              title = "Impact",
              breaks = c(-Inf,-0.05, Inf),
              palette = c("red", "blue")
              ) +
  tm_layout(frame = FALSE) 
@


\begin{figure}[ht]
  \caption{Effects of a Change in Region 30: Categorization}
    \label{fig:predicted-effect}
    \centering 
	\begin{minipage}{.9\linewidth}
<<predicted-effect, echo = FALSE, message = FALSE, fig.align='center', out.width = '10cm', out.height = '10cm'>>=
# Breaks
tm_shape(col_new) +
  tm_polygons(col = "delta_y",
              title = "Impact",
              breaks = c(-Inf,-0.05, Inf),
              palette = c("red", "blue")
              ) +
  tm_layout(frame = FALSE) 
@
\footnotesize
		\emph{Notes:} This graph shows those regions that had low and high impact due to increase in \code{INC} in 30th. Red-colored regions are those regions with a decrease of crime rate larger than 0.05, whereas blue-colored regions are those regions with lower decrease of crime rate. 
	\end{minipage}	
\end{figure}

Now, we map the magnitude of the changes caused by altering \texttt{INC} in region 30. The following code produces the map, with the resulting visualization shown in Figure \ref{fig:predicted-effect2}.

<<predicted-effect2-evalF, eval = FALSE>>=
# Plot the magnitude of the ME
pal5    <- brewer.pal(6, "Spectral")
cats5   <- classIntervals(col_new$delta_y, n = 5, style = "jenks")
colors5 <- findColours(cats5, pal5)
plot(col_new, col = colors5)
legend("topleft", legend = round(cats5$brks, 2), fill = pal5, bty = "n")
@

\begin{figure}[ht]
  \caption{Effects of a Change in Region 30: Magnitude}
    \label{fig:predicted-effect2}
        \centering 
	\begin{minipage}{.9\linewidth}
<<predicted-effect2, echo = FALSE, message = FALSE, fig.align='center', out.width = '10cm', out.height = '10cm'>>=
tm_shape(col_new) +
  tm_polygons(col = "delta_y",
              title = "Impacts",
              palette = "OrRd", 
              style = "quantile",
              ) +
  tm_layout(frame = FALSE) 
@
\footnotesize
		\emph{Notes:} This graph shows the spatial distribution of the changes caused by altering \texttt{INC} in region 30.
	\end{minipage}
\end{figure}


Next, we use the \code{impacts()} function from the \pkg{spatialreg} package to decompose the total effects of a unit change in each predictor variable into direct (local) effects, indirect (spillover) effects, and total effects. The \code{impacts()} function computes these measures using the reduced-form representation:
\begin{equation*}
  \begin{aligned}
    \vy & = \sum_{r = 1}^K \mA(\mW)^{-1}(\mI_n\beta_r) + \mA(\mW)^{-1}\vepsi \\
     \mA(\mW)^{-1} & = \mI_n + \rho\mW + \rho^2\mW^2 + \ldots
  \end{aligned}
\end{equation*}

If the spatial weights object (\code{listw}) is provided, the exact $\mA(\mW)^{-1}$ is calculated. When traces are computed by powering sparse matrices, the approximation $\mI_n + \rho\mW + \rho^2\mW^2 + \ldots$ is used. Both methods yield similar results, except when the number of powers used is very small or when the spatial coefficient $\rho$ is close to its bounds.

<<using-impacts>>=
spatialreg:::impacts.Sarlm(slm, listw = listw)
@

The output indicates that a \$1,000 increase in income results in a total crime reduction of 1.8 crimes per thousand households. The direct effect of the income variable in the SLM model is -1.123, while the estimated coefficient is -1.074. This implies a feedback effect of: -1.123 - (-1.074) = -0.049, which accounts for 4.5\% of the coefficient estimate. To corroborate these results, we can compute the impacts manually using matrix operations:

<<impacts-by-hand>>=
## Construct S_r(W) = A(W)^-1 (I * beta_r + W * theta_r)
Ibeta <- diag(length(listw$neighbours)) *  coef(slm)["INC"] 
S <- A %*% Ibeta

ADI <- sum(diag(S)) / nrow(A)
ADI

n     <- length(listw$neighbours)
Total <- crossprod(rep(1, n), S) %*% rep(1, n) / n
Total

Indirect <- Total - ADI
Indirect
@

Note that the results obtained here are consistent with those computed using the \code{impact()} function. Additionally, we can calculate the p-values of the impacts by specifying the argument \code{R}, which determines the number of simulations used to generate distributions for the impact measures. This is possible when the fitted model object includes a coefficient covariance matrix. Below, we compute the impacts with p-values:

<<impacts-with-se>>=
# Compute standard errors of impacts
im_obj <- spatialreg:::impacts.Sarlm(slm, listw = listw, R = 200)
summary(im_obj, zstats = TRUE, short = TRUE)
@

The results indicate that the variable with the largest negative direct impact is \code{INC}, implying that \code{INC} has the strongest effect in reducing its own region’s crime rate. The second column of the output presents the indirect effects, which measure the spatial spillovers caused by changes in each variable. Negative indirect effects can be interpreted as spatial benefits, as they reflect reductions in neighboring regions’ crime rates. Conversely, positive indirect effects represent negative externalities, where increases in a variable result in higher crime rates in neighboring regions. From the results, we observe that \code{INC} has the largest and most significant negative indirect effects, highlighting its substantial spatial spillover benefits. 

On the other hand, the indirect effect for \code{HOVAL} is not statistically significant. This weak result may be attributed to the inherent rigidity of the spatial lag model (SLM), which assumes that the ratio of spillover effects to direct effects is the same for all explanatory variables. Such an assumption may limit the model’s ability to accurately capture spillover dynamics.

The total effect combines both the direct and indirect impacts, offering a comprehensive view of each variable’s importance in reducing crime rates. From the results, it is evident that \code{INC} also has the largest total effect, underscoring its overall significance.

To further investigate impacts, we follow an approach that converts the spatial weight matrix into a sparse format and computes its powers using the \code{trW} function:
<<compute-me1>>=
# Impacts using traces. 
W <- as(nb2listw(col.gal.nb, style = "W"), "CsparseMatrix")
trMC <- trW(W, type = "MC")
im <- spatialreg:::impacts.Sarlm(slm, tr = trMC, R = 100)
summary(im, zstats =  TRUE, short = TRUE)
@

Additionally, we can examine cumulative impacts by specifying the \code{Q} argument. When both \code{Q} and \code{tr} are provided in the \code{impacts()} function, the output includes impact components for each step in the traces of the powers of the weight matrix, up to and including the $Q$th power:
<<compute-cumme1>>=
# Cumulative impacts
im2   <- spatialreg:::impacts.Sarlm(slm, tr = trMC, R = 100, Q = 5)
sums2 <- summary(im2, zstats = TRUE, reportQ = TRUE, short =  TRUE)
sums2
@

%-----------------------------------
\section{Programing the SLM in R}
%-----------------------------------

In this section, we demonstrate how to create a custom function to estimate a Spatial Lag Model (SLM) using maximum likelihood (ML) estimation. Two approaches are considered. The first involves a constrained optimization procedure that directly uses the log-likelihood function in Equation \eqref{eq:LL_SLM_2}. The second approach employs the concentrated log-likelihood function, following the steps outlined in Algorithm \eqref{algorithm:SLM}.

%---------------------------
\subsection{First approach}\label{sec:code-full-slm}
%----------------------------

To estimate the SLM via maximum likelihood, we first define a function that computes the log-likelihood, its gradient, and its Hessian. We then use the \code{maxLik} function from the \pkg{maxLik} package \citep{henningsen2011maxlik} to optimize these functions. Below is the implementation of the log-likelihood function:

<<log-like-func>>=
# Create log-likelihood function for SLM ----
sml_ll <- function(theta, y, X, W, gradient = TRUE, hessian = TRUE){
  # Global
  K <- ncol(X)
  N <- nrow(X)
  
  # Extract parameters
  betas  <- theta[1:K]
  rho    <- theta[K + 1]
  sig.sq <- theta[K + 2]
  
  # Make residuals
  A   <- diag(N) -  rho * W
  Ay  <- A %*% y
  Xb  <- X %*% betas
  res <- Ay - Xb
  
  # Make log-likelihood
  detA <- det(A)
  ll   <- -0.5 * N * log(2 * pi * sig.sq) - 0.5 * crossprod(res) / sig.sq + log(detA)
  
  # Gradient
  if (gradient){
    C           <-  W %*% solve(A)
    grad.betas  <- (1 / sig.sq) * t(X) %*% res
    grad.rho    <- - sum(diag(C)) + (1 / sig.sq) * t(res) %*% W %*% y
    grad.sig.sq <- (1 / (2 * sig.sq ^2 )) * (t(res) %*% res - N * sig.sq)
    attr(ll, 'gradient') <- c(grad.betas, grad.rho, grad.sig.sq)
  }
  # Hessian
  if (hessian){
    H    <- matrix(NA, nrow = (K + 2), ncol = (K + 2))
    h_bb <- - (1 / sig.sq) * t(X) %*% X
    h_bs <- - (1 / sig.sq ^ 2) * t(X) %*% res
    h_br <- - (1 / sig.sq) * t(X) %*% W %*% y 
    h_ss <- (N / (2 * sig.sq ^ 2)) - (1 / sig.sq ^ 3) * t(res) %*% res
    h_sr <-  - t(res) %*% W %*% y / sig.sq ^ 2
    h_rr <- - sum(diag(C %*% C)) - (1 / sig.sq) * (t(y) %*% t(W) %*% W %*% y)
    H[1:K, 1:K]     <- h_bb
    H[1:K, K + 1]   <- h_br
    H[1:K, K + 2]   <- h_bs
    H[K + 1, 1:K]   <- t(h_br)
    H[K + 1, K + 1] <- h_rr
    H[K + 1, K + 2] <- h_sr
    H[K + 2, 1:K]   <- t(h_bs)
    H[K + 2, K + 1] <- h_sr
    H[K + 2, K + 2] <- h_ss
    attr(ll, 'hessian') <- H
  }
  return(ll)
}
@

The function \code{sml\_ll} has the following arguments: \code{theta} is a vector log length $k + 2$, where the $K + 1$ and $K + 2$ elements are $\rho$ and $\sigma^2$, respectively; \code{y} is the $n\times 1$ vector of dependent variables; \code{X} is the $n\times k$ matrix of independent variables; \code{W} is the spatial weight matrix of \code{matrix} class; the arguments \code{gradient} and \code{hessian} indicate whether the analytical gradient and Hessian, respectively, should be use in the numerical optimization algorithm. 

Note that the function does not approximate the Jacobian matrix during computation. This may not be the most efficient method for large sample sizes. The log-likelihood object \code{ll} corresponds to Equation \eqref{eq:LL_SLM_2}. The gradient and Hessian are implemented following Equations \eqref{eq:full_agradient} and \eqref{eq:hessian_sml}, respectively.


Now we define the main function, \code{slm.ml}, which estiamtes the SLM using MLE with constrained optimization via the \code{maxLik} function.

<<slm-ml-function, message=FALSE>>=
library("maxLik")
slm.ml <- function(formula, data, W, 
                   gradient = TRUE, 
                   hessian  = TRUE, ...){
  require("maxLik")
  # Model Frame: This part is standard in R to obtain
  #              the variables using formula and data argument.
  callT    <- match.call(expand.dots = TRUE)
  mf       <- callT
  m        <- match(c("formula", "data"), names(mf), 0L)
  mf       <- mf[c(1L, m)]
  mf[[1L]] <- as.name("model.frame")
  mf       <- eval(mf, parent.frame()) # final model frame
  nframe   <- length(sys.calls())
  
  # Get variables and globals
  y  <- model.response(mf)        # Get dependent variable from mf
  X  <- model.matrix(formula, mf) # Get X from mf
  K  <- ncol(X)
  
  # Starting values
  ols.init    <- lm(y ~ X - 1)
  b.init      <- coef(ols.init)
  sigma2.init <- sum(residuals(ols.init)^2) / ols.init$df.residual
  rho.init    <- cor(W %*% y, y)
  start       <- c(b.init, rho.init, sigma2.init)
  names(start) <- c(colnames(X), "rho", "sig.sq")
  
  # Optimization default controls if not added by user
  if (is.null(callT$method))  callT$method  <- 'bfgs'
  if (is.null(callT$iterlim)) callT$iterlim <- 100000
    
  # Restricted optimization if BFGS: A %*% theta + B >= 0: Constraint rho and sigma2
  if (callT$method == "bfgs"){
    sym          <- all(W == t(W))
    omega        <- eigen(W, only.values = TRUE, symmetric = sym)
    lambda_space <- if (is.complex(omega$values)) 1 / range(Re(omega$values)) else 1 / range(omega$values)
    A <- rbind(c(rep(0, K), 1, 0),
               c(rep(0, K), -1, 0), 
               c(rep(0, K), 0, 1))
    B <- c(-1L * (lambda_space[1] + sqrt(.Machine$double.eps)), 
                  lambda_space[2] - sqrt(.Machine$double.eps), 
           -1L * sqrt(.Machine$double.eps))
   callT$constraints <- list(ineqA = A, ineqB = B)
  }

  # Optimization
  opt <- callT
  m <- match(c('method', 'print.level', 'iterlim',
               'tol', 'ftol', 'steptol', 'fixed', 'constraints', 
               'control', 'finalHessian', 'reltol', 'rho', 
               'outer.iterations', 'outer.eps'),
             names(opt), 0L)
  opt <- opt[c(1L, m)]
  opt$start     <- start
  opt[[1]]      <- as.name('maxLik')
  opt$logLik    <- as.name('sml_ll')
  opt$gradient  <- gradient
  opt$hessian   <- hessian
  opt[c('y', 'W', 'X')] <- list(as.name('y'), 
                                as.name('W'), 
                                as.name('X'))
  out <- eval(opt, sys.frame(which = nframe))
  return(out)
}
@

The procedure uses numerical optimization via the \code{maxLik} function. Initial values for $\widehat{\vbeta}$ and $\widehat{\sigma}^2$ are derived from the OLS estimator, while the starting value for $\widehat{\rho}$ is based on the simple correlation between $\mW\vy$ and $\vy$. By default, the optimization procedure is based on BFGS algorithm with inequality constraints. These constrainsts ensure that $\rho < 1 /\omega_{\textrm{max}} - e$, $\rho > 1 /\omega_{\textrm{min}} + e$, and $\sigma^2>e$. The value $e$ is a very small number, defined as \code{sqrt(.Machine\$double.eps)}.

In matrix form, the inequality constraints are expressed as:
\begin{equation*}
 \mA\vtheta + \vb \geq \vzeros, 
\end{equation*}
%
where $\vtheta = (\vbeta ^\top, \rho, \sigma^2)^\top$, and 
\begin{equation*}
\mA  = \begin{pmatrix}
      \vzeros^\top& 1 & 0 \\
      \vzeros^\top&  -1 & 0 \\
      \vzeros^\top& 0& 1
      \end{pmatrix}, \quad\quad \\
\vb  = \begin{pmatrix}
       - (1 /\omega_{\textrm{min}}  + e) \\
       1 / \omega_{\textrm{max}} -e \\
       - e
      \end{pmatrix}, 
\end{equation*}
%
where $\vzeros$ is an $k\times 1$ vector of zeros. This implies:
\begin{align*}
\rho & \geq 1 /\omega_{\textrm{min}} + e, \\
-\rho & \geq - (1 /\omega_{\textrm{max}} - e), \\
\sigma^2 & \geq e.
\end{align*}

Currently, only the BFGS optimizer supports inequality constraints. If an alternative optimization method such as Newton-Raphson (specified via \code{method = "nr"}) is used, the procedure will not enforce these constraints, and the optimization will proceed globally.

The function outputs an object of class \code{maxLik}, making it compatible with existing methods in the \code{maxLik} package without requiring additional S3 methods.

To evaluate the function, we create an artificial dataset based on a data-generating process (DGP) inspired by \cite{lee2007gmm}. The DGP generates spatially lagged data for testing the ML implementation. The following lines demonstrate the function's application:
<<generate-DGP-ml>>=
# Generate DGP
set.seed(1)
n      <- 529
rho    <- 0.6
W.nb2  <- cell2nb(sqrt(n), sqrt(n))
W      <- nb2mat(W.nb2)

# Exogenous variables
x1     <- rnorm(n)
x2     <- rnorm(n)
x3     <- rnorm(n)

# DGP parameters
b0 <- 0 ; b1 <- -1; b2 <- 0; b3 <- 1
sigma2 <- 2
epsilon <- rnorm(n, mean = 0, sd = sqrt(sigma2))

# Simulate the dependent variable
y <- solve(diag(n) -  rho * W) %*% (b0 + b1*x1 + b2*x2 + b3*x3 + epsilon)

# Data as data.frame
data <- as.data.frame(cbind(y, x1, x2, x3))
names(data) <- c("y", "x1", "x2", "x3")
@

Two models are tested: one using the MLE approach with inequality constraints, and the other using the Newton-Raphson algorithm without constraints:

<<check-func1, message = FALSE, cache = TRUE>>=
# Use our function 
start <- Sys.time()
sml.mle <- slm.ml(y ~ x1 + x2 +  x3, data = data, W = W)
summary(sml.mle)
print(Sys.time()- start)

start <- Sys.time()
sml.mle.nr <- slm.ml(y ~ x1 + x2 +  x3, data = data, W = W, method = "nr")
summary(sml.mle.nr)
print(Sys.time()- start)
@

Both optimization procedures yield similar estimates and standard errors, but the Newton-Raphson algorithm is faster. However, the second approach produces a warning due to a negative value of $\sigma^2$ in one of the iterations.

%---------------------------
\subsection{Second approach}\label{sec:code-condentrated-slm}
%----------------------------

Now, we create a function that estimates the parameters of the SLM using the concentrated log-likelihood and  the steps in Algorithm \eqref{algorithm:SLM}.

The concentrated log-likelihood function is created as follows:
<<conc-ml>>=
logLik_sar <- function(rho, e_0, e_L, omega, n)
{
  # This function returns the concentrated log L for maximization
  
  #Generate determinant using Ord's approximation
  det    <- if (is.complex(omega)) Re(prod(1 - rho * omega)) else prod(1 - rho * omega)
  e_diff <- e_0 - rho * e_L
  sigma2 <- crossprod(e_diff) / n
  
  #Log-Likelihood function
  l_c    <- - (n / 2) - (n / 2) * log(2 * pi) - (n / 2) * log(sigma2) + log(det)
  return(l_c)
}
@

The \code{logLik\_sar} function uses the Ord's approximation for the Jacobian. 

The main function is the following:
<<secondfunc>>=
sar.mle.con <- function(formula, data, W)
{
  # Model Frame: This part is standard in R to obtain
  # the variables using formula and data argument.
  callT <- match.call(expand.dots = TRUE)
  mf <- callT
  m  <- match(c("formula", "data"), names(mf), 0L)
  mf <- mf[c(1L, m)]
  mf[[1L]] <- as.name("model.frame")
  mf <- eval(mf, parent.frame()) # final model frame
  
  # Get variables and globals
  y  <- model.response(mf)        # Get dependent variable from mf
  X  <- model.matrix(formula, mf) # Get X from mf
  n  <- nrow(X)                   # Number of spatial units
  k  <- ncol(X)                   # Number of regressors
  Wy <- W %*% y                   # Spatial lag
  
  # Generate auxiliary regressions 
  # See Algorithm 3.1
  ols_0 <- lm(y ~ X - 1)
  ols_L <- lm(Wy ~ X - 1)
  e_0   <- residuals(ols_0)
  e_L   <- residuals(ols_L)
  
  # Get eigenvalues to constraint the optimization
  sym          <- all(W == t(W))
  omega        <- eigen(W, only.values = TRUE, symmetric = sym)
  
  # Maximize concentrated log-likelihood
  rho_space <- if (is.complex(omega$values)) 1 / range(Re(omega$values)) else 1 / range(omega$values)
  opt_lc <- optimize(f = logLik_sar,   # This function is below
                     lower = rho_space[1] + .Machine$double.eps,
                     upper = rho_space[2] - .Machine$double.eps,
                     maximum = TRUE, 
                     e_0 = e_0, e_L = e_L, omega = omega$values, n = n, 
                     tol = .Machine$double.eps)
  # Obtain rho_hat from concentrated log-likelihood
  rho_hat <- opt_lc$maximum
  
  # Generate estimates
  A          <- (diag(n) - rho_hat * W)
  Ay         <- crossprod(t(A), y)
  beta_hat   <- solve(crossprod(X)) %*% crossprod(X, Ay) 
  error      <- Ay - crossprod(t(X), beta_hat)
  sigma2_hat <- crossprod(error) / n                    
  
  # Save results
  out <- structure(
    list(
      callT = callT,
      rho_hat = rho_hat, 
      beta_hat = beta_hat,
      sigma2_hat = sigma2_hat, 
      A = A, 
      W = W,
      X = X, 
      omega = omega 
    ),
    class = "slmc.mle"
  )
  
return(out)
}
@

The following code creates the S3 method

<<vcov.slmc.mle>>=
vcov.slmc.mle <- function(object, ...){
  rho_hat    <- object$rho_hat
  beta_hat   <- object$beta_hat
  sigma2_hat <- object$sigma2_hat
  A          <- object$A
  W          <- object$W
  X          <- object$X
  omega      <- object$omega$values
  k          <- ncol(X)
  
  # Hessian
  C       <- crossprod(t(W), solve(A)) # C = WA^{-1}
  alpha   <-  sum(omega ^ 2 / ((1 - rho_hat * omega) ^ 2))
  if (is.complex(alpha)) alpha <- Re(alpha)
  b_b     <- drop(1 / sigma2_hat) * crossprod(X) # k * k
  b_rho   <- drop(1 / sigma2_hat) * (t(X) %*% C %*% X %*% beta_hat) # k * 1
  sig_sig <- n / (2 * sigma2_hat ^ 2) # 1 * 1
  sig_rho <- drop(1 / sigma2_hat) * sum(diag(C)) # 1 * 1
  rho_rho <- sum(diag(crossprod(C))) +  alpha +
    drop(1 / sigma2_hat) * crossprod(C %*% X %*% beta_hat) # 1*1
  row_1   <- cbind(b_b, rep(0, k), b_rho)
  row_2   <- cbind(t(rep(0, k)), sig_sig, sig_rho)
  row_3   <- cbind(t(b_rho), sig_rho, rho_rho)
  Hessian <- rbind(row_1, row_2, row_3)

  return(solve(Hessian))
}
@


<<summary-smlc.mle>>=
# S3 methods for summary
summary.slmc.mle <- function(object, 
                              table = TRUE, 
                              digits = max(3, .Options$digits - 3), 
                              ...){
    X       <- object$X
    n       <- nrow(X)
    k       <- ncol(X)
    df      <- n - (k + 1)
    b       <- c(object$beta_hat, object$sigma2_hat, object$rho_hat)
    names(b) <- c(colnames(X), "sigma2", "Wy")
    std.err <- sqrt(diag(vcov(object)))
    z       <- b / std.err
    p       <- 2 * pt(-abs(z), df = df)
    CoefTable <- cbind(b, std.err, z, p)
    colnames(CoefTable) <- c("Estimate", "Std.Error", "t-value", "Pr(>|t|)")
    result <- structure(
      list(
        CoefTable = CoefTable, 
        digits    = digits, 
        call      = object$call),
      class = 'summary.slmc.mle'
    )
    return(result)
}

print.summary.slmc.mle <- function(x, 
                                   digits = x$digits, 
                                   na.print = "", 
                                   symbolic.cor = p > 4, 
                                   signif.stars = getOption("show.signif.stars"), 
                                   ...)
{
  cat("\nCall:\n")
  cat(paste(deparse(x$call), sep = "\n", collapse = "\n"), "\n\n", sep = "")
      
  cat("\nCoefficients:\n")
  printCoefmat(x$CoefTable, digit = digits, P.value = TRUE, has.Pvalue = TRUE)
  invisible(NULL)
}
@


<<>>=
slm2 <- sar.mle.con(y ~ x1 + x2 + x3, data = data, W = W)
summary(slm2)
@

\begin{table}[ht]
\caption{Comparing coefficients for SLM.}\label{tab:mle-slms}
\centering
<<echo = FALSE, results = 'asis', warning=FALSE>>=
sarlm <- lagsarlm(y~ x1 + x2 + x3, 
                   data = data, 
                   listw = mat2listw(W, style = "W"), 
                   method = "eigen")
coefs <- cbind(sml.mle$estimate, 
            sml.mle.nr$estimate, 
           c(slm2$beta_hat, slm2$rho_hat, slm2$sigma2_hat),
           c(sarlm$coefficients, sarlm$rho, sarlm$s2)
           )
ses <- cbind(sqrt(diag(vcov(sml.mle))), 
             sqrt(diag(vcov(sml.mle.nr))),
             sqrt(diag(vcov(slm2)))[c(1, 2, 3, 4, 6, 5)], 
             c(sqrt(diag(vcov(sarlm)))[c(2, 3, 4, 5, 1)], 0)
             )
Tab <- cbind(coefs, ses)
rownames(Tab) <- c("b0", "b1", "b2", "b3", "rho", "sigma2")
colnames(Tab) <- c("F-BFGS", "F-NR", "CMLE", "R", 
                   "F-BFGS", "F-NR", "CMLE", "R")
toLatex(Tab, compact = TRUE, useBooktabs =  TRUE, digits = 5)
@
\end{table}


%-----------------------
\section{Exercises}
%-------------------

\begin{exercises}
    \exercise Consider the concentrated log-likelihood in Equation \eqref{eq:concentrated_ml_sar_1}. Find the first and second derivative respect to $\rho$.
    % \textbf{Solution}:
% 
% Taking the first derivate:
% 
% \begin{eqnarray}
% 	\frac{\partial \ell^c(\rho)}{\partial \rho} &=& \frac{-2\ve_0'\ve_L + 2\rho\ve_L'\ve_L}{\frac{\ve_0'\ve_0 - 2\rho\ve_L'\ve_0 + \rho^2 \ve_L'\ve_L}{N}} +\frac{2}{N}\sum_{i=1}^n\frac{\omega_i}{1-\rho\omega_i} \\
% 	&=& 2\left(\frac{ \rho\ve_L'\ve_L -\ve_0'\ve_L}{\widehat{\sigma}^2(\rho)}\right) +\frac{2}{N}\sum_{i=1}^n\frac{\omega_i}{1-\rho\omega_i} 
% \end{eqnarray}
% 
% The second derivative is:
% 
% 
% \begin{eqnarray}
% 	\frac{\partial^2 \ell^c(\rho)}{\partial \rho ^2}  &=& \frac{2}{N}\sum_{i=1}^n\frac{\omega_i^2}{(1-\rho\omega_i)^2} + 2\left(\frac{\ve_L'\ve_L\cdot \sigma ^2(\rho)-N^{-1}(-2\ve_0'\ve_L + 2\rho\ve_L'\ve_L)(\rho\ve_L'\ve_L -\ve_0'\ve_L)}{\widehat{\sigma} ^4(\rho)}\right) \\
% 	 &=& \frac{2}{N}\sum_{i=1}^n\frac{\omega_i^2}{(1-\rho\omega_i)^2} + 2\frac{\ve_L'\ve_L}{\sigma ^2(\rho)} - 4 \frac{(\rho\ve_L'\ve_L -\ve_0'\ve_L)^2}{\widehat{\sigma} ^4(\rho)}
% \end{eqnarray}

	
%\end{enumerate}
    \exercise Consider the Spatial Lag Model:
  
      \begin{eqnarray*}
      \vy & = & \rho\mW\vy +\mX\vbeta + \vepsi \\
      \vepsi & \sim &\rN(\vzeros, \sigma^2\mI_n)
      \end{eqnarray*}
      
      Let $\vz = \mA\vy$. Show that $\widehat{\sigma}^2_{ML}$ can be written as:
      
      \begin{equation*}
      \widehat{\sigma}^2_{ML} = \frac{1}{N}\vz^\top\mM\vz
      \end{equation*}
      
      where $\mM = \mI - \mX(\mX^\top\mX)^{-1}\mX^\top$. 
    \exercise Consider the Spatial Error Model:
	
	\begin{eqnarray*}
		\vy    & = & \mX\vbeta + \vu \label{eq:sac_eq_1}\\
		\vu & = &\lambda\mW\vu + \vepsi\label{eq:sac_eq_2} \\
		\vepsi & \sim &\rN(\vzeros, \sigma^2\mI_n)
	\end{eqnarray*}
	
	\begin{enumerate}
	  \item Show that the OLS estimates $\widehat{\vbeta}$ is unbiased, but inefficient.
	  \item Derived the ML estimates.
	  \item Derived the concentrated log-likelihood function.
	  \item Derive the asymptotic variance-covariance matrix of the estimates given in Equation \eqref{eq:asyvar_sem}.
	\end{enumerate}
 
 \exercise  Consider the following SAC model with heteroskedastic errors:
 	\begin{eqnarray}
		\vy    & = & \rho \mW_1\vy + \mX\vbeta + \vu \\
		\vu & = &\lambda\mW_2\vu + \vepsi \\
		\vepsi & \sim &\rN(\vzeros, \mOmega)
	\end{eqnarray}
	
	The matrix $\mOmega$ is the variance-covariance matrix of the error terms, which is assummed to be known a priori. For example, we can assume that:
	
	\begin{equation}
		\var(\epsilon_i) = \sigma^2_i = \vz_i^\top\valpha
	\end{equation}
	
	or
	
	\begin{equation}
		\var(\epsilon_i) = \sigma^2_i = \exp(\vz_i^\top\valpha)
	\end{equation}
	
	or more general,
	
	\begin{equation}
		\var(\epsilon_i) = \sigma^2_i = \vh(\vz_i^\top\valpha)
	\end{equation}
	
	where $\vh(\cdot)$ is any function, $\vz_i$ is a vector of covariates for each spatial unit, and $\valpha$ is a vector of parameters with element $\alpha_p, p = 0, 1,..., P$.  Therefore, the diagonal elements of the error covariance  matrix $\mOmega$ are:
	
	
	\begin{equation}
		\mOmega_{ii} = \sigma^2_i = \vh_i(\vz^\top_i\valpha), \quad \vh_i > 0
	\end{equation}
	
	Note that the model has $2 + K + P$ unknown parameters:
	
	\begin{equation}
		\vtheta = (\rho, \vbeta^\top, \lambda, \valpha^\top)^\top.
	\end{equation}
	
   \begin{enumerate}
     \item Find the Log-likelihood function.
     \item Find the first order conditions
   \end{enumerate}
 
 \exercise Consider the model:

\begin{equation}
	\vy = \mX\vbeta + \rho_1\mW_1\vy + \rho_2\mW_2\vy + \vu,
\end{equation}
%
where $\vu$ has mean and VC matrix of $\vzeros$ and $\sigma^2\mI_n$, respectively, and $\mW_1$ and $\mW_2$, are observed exogenous weighting matrices. 
   \begin{enumerate}
    \item Obtain the likelihood function, and then determine the first order conditions for $\vbeta$.
    \item Assume that $\mW_1$ and $\mW_2$ are row-normalized. Give a condition which is sufficient for the model to be solved for $\vy$ in terms of $\mX$ and $\vepsi$.
   \end{enumerate}
   
   \exercise  Show that $\mI_n + \rho_0\mC_n = \mA_n^{-1}$.\label{exercise:expansion-res1}
   \exercise  Consider the following DGP:

\begin{equation}
	\begin{aligned}
		y_i & = \alpha + \beta x_i + u_i \\
		u_i & = \lambda \sum_{j =  1}^n w_{ij}u_{j} + \epsilon_i \\
		\epsilon_i &\sim \rN(0, 1) 
	\end{aligned}
\end{equation}
%
where $\lambda = 0.8$, $\alpha = 0.5$, $\beta = 1$ and $x_i \sim \rN(0, 2^2)$. Using a Monte Carlo experiment, show that the $\widehat{\beta}_{OLS}$ is unbiased, but inefficient. For experiment create 100 datasets with 225 spatial units. Set the seed at 123.


\end{exercises}   
    
    

\section*{Appendix}
\begin{subappendices}

%-------------------------------------------------------------------------
 \section{Consistency of SLM Model}\label{appendix-consistency}
%-------------------------------------------------------------------------

The consistency $\widehat{\vtheta}_n$ can be established from the uniform convergence of the concentrated log-likelihood of $\rho$ with the QMLE of $\vbeta_0$ and $\sigma_0^2$ concentrated out, because one can have the QMLEs of $\vbeta$ and $\sigma^2$ once $\rho$ is given. Thus, we show that $\frac{1}{n}\ell_n(\rho) - \frac{1}{n}Q_n(\rho)$ converges in probability to zero uniformly on $\Gamma$ (where $Q_n(\rho)$ is the expectation of the concentrated log-likelihood), and the identification-uniqueness condition holds. 

\paragraph{Uniform convergence}

In this first part, we need to show that
\begin{equation*}
  \frac{1}{n}\ell_n(\rho) - \frac{1}{n}Q_n(\rho) \pto 0,
\end{equation*}
%
uniformly on $\Gamma$, where $\ell_n(\rho)$ is the concentrated log-likelihood and $Q_n(\rho)$ is the expectation of the log-likelihood function evaluated in the optimal values of $\vbeta$ and $\sigma^2$, $Q_n(\rho) = \underset{\vbeta, \sigma^2}{\max}\; \E\left[\ell_n(\vtheta)\right]$.

For simplicity in the notation, let 
\begin{equation*}
 \mA_n = \mA_n(\rho) = (\mI - \rho\mW) \quad \mbox{and} \quad  \mA_{n0} = \mA_n(\rho_0) = (\mI - \rho_0\mW). 
\end{equation*}

For further reference, recall that the log-likelihood function is 
\begin{equation*}
\ell_n(\vtheta) = -\frac{n}{2}\ln (2\pi) - \frac{n}{2}\ln(\sigma^2) - \frac{1}{2\sigma^2}\vepsi_n^\top\vepsi_n + \ln\left|\mA_n\right|,
\end{equation*}
with $\vepsi_n = \mA_n\vy_n - \mX_n\vbeta$. The concentrated log-likelihood can be written as
\begin{equation}\label{eq:concentrated-mle-consistency}
 \ell_n(\rho) =-\frac{n}{2}\left(\ln(2\pi) + 1\right) - \frac{n}{2}\ln\left[\widetilde{\sigma}^2_n(\rho)\right] + \ln\left|\mA_n\right|,
\end{equation}
%
where
\begin{equation}\label{eq:sigma-mle-consistency}
\begin{aligned}
\widetilde{\sigma}^2_n(\rho) & = \frac{1}{n}\left[\mA_n\vy_n - \mX_n\widehat{\vbeta}_n(\rho)\right]^\top \left[\mA_n\vy_n - \mX_n\widehat{\vbeta}_n(\rho)\right],  \\
& = \frac{1}{n}\vy_n^\top\mA_n^\top(\mI_n - \mP_{nX})\mA_n\vy_n, \\
& = \frac{1}{n}\vy_n^\top\mA_n^\top\mM_n\mA_n\vy_n,
\end{aligned}
\end{equation}
%
with $\widehat{\vbeta}_n(\rho)$ being the MLE of $\vbeta_0$ wich depends on $\rho$, $\mP_{nX} = \mX_n\left(\mX_n^\top\mX_n\right)^{-1}\mX_n^\top$, and $\mM_n = \mI_n - \mP_{nX}$ is the projection and annihilator matrix, respectively.

The QMLE $\widehat{\rho}_n$ of $\rho_0$ is the maximizer of the concentrated likelihood in Equation \eqref{eq:concentrated-mle-consistency}. If $\widehat{\rho}_n$ is consistent, the consistency of $\widehat{\vbeta}_n(\widehat{\rho}_n)$ and $\widetilde{\sigma}^2(\widehat{\rho})_n$ follows from their closed form formula. It is important to highlight that we do not need to have a compact parameter space for $\vbeta_0$ and $\sigma_0^2$.

The expectation of $\ell_n(\vtheta)$ is:
\begin{equation}\label{eq:expectation-log-slm}
  \begin{aligned}
    \E\left[\ell_n(\vtheta)\right] & = -\frac{n}{2}\ln (2\pi) - \frac{n}{2}\ln(\sigma^2) - \frac{1}{2\sigma^2}\E\left(\vepsi_n^\top\vepsi_n\right) + \ln\left|\mA_n\right|.
  \end{aligned}
\end{equation}

We need to work on $\E\left(\vepsi_n^\top\vepsi_n\right)$. To do so, note that under the true DGP $\vy_n = \mA_{n0}^{-1}\mX_n\vbeta_0 + \mA_{n0}^{-1}\vepsi_n$, then
\begin{equation}\label{eq:equality-for-consistency}
\begin{aligned}
\E(\vy_n) & = \vmu_{\vy}     = \mA_{n0}^{-1}\mX_n\vbeta_0, \\
\var(\vy_n)& = \mSigma_{\vy}  = \sigma_0^2\mA_{n0}^{-1}(\mA_{n0}^{-1})^\top, \\
\tr\left(\mA_n^\top(\rho)\mA_n(\rho)\mSigma_{\vy}\right) & = \sigma_0^2\tr\left[\mA_n^\top\mA_n\mA_{n0}^{-1}(\mA_{n0}^{-1})^\top\right], \\
& = \sigma_0^2\tr\left[\mD_n(\rho_0, \rho)\right], \\
\vmu_{\vy}^\top\mA_n^\top(\rho)\mA_n(\rho)\vmu_{\vy} & = \vbeta_0^\top\mX_n^\top(\mA_{n0}^{-1})^\top\mA_n^\top\mA_n\mA_{n0}^{-1}\mX_n\vbeta_0, \\
& = \vbeta_0^\top\mX_n^\top\mD_n(\rho_0, \rho)\mX_n\vbeta_0,
\end{aligned}
\end{equation}
%
where $\mD_n(\rho_0, \rho) = (\mA_{n0}^{-1})^\top\mA_n^\top\mA_n\mA_{n0}^{-1}$.

Then, using Lemma \ref{lemma:second-mom-lee} for the expectation of quadratic forms and the results in Equation \eqref{eq:equality-for-consistency}
\begin{equation*}
\begin{aligned}
\E(\vepsi_n^\top\vepsi_n) & = \E\left[\left(\mA_n\vy_n - \mX_n\vbeta\right)^\top\left(\mA\vy_n - \mX_n\vbeta\right)\right], \\
& = \E\left[\vy_n^\top\mA_n^\top\mA_n\vy_n - 2\vbeta^\top\mX_n^\top\mA_n\vy_n + \vbeta^\top\mX_n^\top\mX_n\vbeta\right], \\
& = \E\left[\vy_n^\top\mA_n^\top\mA_n\vy_n \right]- \E\left[2\vbeta^\top\mX_n^\top\mA_n\vy_n\right] + \E\left[\vbeta^\top\mX_n^\top\mX_n\vbeta\right], \\
& = \tr\left(\mA_n^\top\mA_n\mSigma_{\vy}\right) + \vmu_{\vy}^\top\mA_n^\top\mA_n\vmu_{\vy} - 2\vbeta^\top\mX_n^\top\mA_n\vmu_{\vy} + \vbeta^\top\mX_n^\top\mX_n\vbeta, \\
& = \sigma_0^2\tr\left[\mD_n(\rho_0, \rho)\right] + \vbeta_0^\top\mX_n^\top\mD_n(\rho_0, \rho)\mX_n\vbeta_0 -2\vbeta^\top\mX_n^\top\mA_n\mA_{n0}^{-1}\mX_n\vbeta_0 + \vbeta^\top\mX_n^\top\mX_n\vbeta.
\end{aligned}
\end{equation*}

Then, the FONC for the optimal solutions of $\E(\ell_n(\vtheta))$ in Equation \eqref{eq:expectation-log-slm} are
\begin{equation*}
\begin{aligned}
\frac{\partial  \E\left[\ell_n(\vtheta)\right]}{\partial \vbeta} & = -2\mX_n^\top\mA_n\mA_{n0}^{-1}\mX_n\vbeta_0 +  2\mX_n^\top\mX_n\vbeta, \\
\frac{\partial  \E\left[\ell_n(\vtheta)\right]}{\partial \sigma^2} & = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4}\E\left(\vepsi_n^\top\vepsi_n\right), 
\end{aligned}
\end{equation*}
%
such that
\begin{equation*}
\vbeta_n^*  = \left(\mX_n^\top\mX_n\right)^{-1}\mX_n^\top\mA_n\mA_{n0}^{-1}\mX_n\vbeta_0,
\end{equation*}
and 
\begin{equation}\label{eq:sigma-star-consistency}
\begin{aligned}
\sigma^{2*}_n  &  = \frac{1}{n}\E\left(\vepsi_n^\top\vepsi_n\right), \\
               & =  \frac{1}{n}\E\left(\left[\mA_n\vy_n - \mX_n\vbeta_n^*\right]^\top \left[\mA_n\vy_n - \mX_n\vbeta_n^*\right]\right), \\
               &= \frac{1}{n}\left\lbrace (\rho_0 - \rho)^2(\mC_n\mX_n\vbeta_0)^\top \mM_n(\mC_n\mX_n\vbeta_0) + \sigma_0^2\tr(\mD_n(\rho_0, \rho)) \right\rbrace, \\
               & = \frac{1}{n}\left\lbrace (\rho_0 - \rho)^2(\mC_n\mX_n\vbeta_0)^\top \mM_n(\mC_n\mX_n\vbeta_0))\right\rbrace + \sigma_n^2(\rho), 
\end{aligned}
\end{equation}
%
where $\sigma_n^2(\lambda) = \frac{1}{n}\sigma_0^2\tr(\mD_n(\rho_0, \rho))$.

Since $\vbeta^*_n$ and $\sigma_n^{2*}$ represent the maximum values of the expected value of the log-likelihood function, $Q_n(\rho) = \max_{\vbeta, \sigma^2}\E(\ell_n(\vtheta))$ is 
\begin{equation*}
  Q_n(\rho) = -\frac{n}{2}\left(\ln(2\pi) + 1\right) - \frac{n}{2}\ln(\sigma^{2*}_n(\rho)) + \log\left|\mA_n\right|.
\end{equation*}
%
and
\begin{equation*}
\frac{1}{n}\left[\ell_n(\rho) - Q_n(\rho)\right] = -\frac{1}{2}\left[\ln( \widetilde{\sigma}^{2}) - \ln(\sigma^{2*}_n(\rho)) \right],
\end{equation*}

To prove that $\frac{1}{n}\left[\ell_n(\rho) - Q_n(\rho)\right]\pto 0$, we need to show that $\widetilde{\sigma}^{2}\pto \sigma^{2*}_n$. 

We can show that $\mA_n\mA_{n0}^{-1} = \mI_n + (\rho_0 - \rho)\mC_n(\rho_0)$, then we can also write
\begin{equation*}
\begin{aligned}
  (\mI_n - \mP_{nX})\mA_n\vy_n & = \mM_n\mA_n\vy_n, \\
                               & = \mM_n\mA_n\mA_{n0}^{-1}\mX_n\vbeta_0 + \mM_n\mA_n\mA_{n0}^{-1}\vepsi_n, \\
& = \mM_n\left(\mI_n + (\rho_0 - \rho)\mC_n(\rho_0)\right)\mX_n\vbeta_0 + \mM_n\mA_n\mA_{n0}^{-1}\vepsi_n,       \\
& = (\rho_0 - \rho)\mM_n\mC_{n0}\mX_n\vbeta_0 + \mM_n\mA_n\mA_{n0}^{-1}\vepsi_n, 
\end{aligned}
\end{equation*}

From \eqref{eq:sigma-mle-consistency}, we can write
\begin{equation*}
\begin{aligned}
\widetilde{\sigma}(\rho)  = & \frac{1}{n}\vy_n^\top\mA_n^\top(\mI_n - \mP_{nX})\mA_n\vy_n, \\
 = & \frac{1}{n}\left((\rho_0 - \rho)\mM_n\mC_{n0}\mX_n\vbeta_0 + \mM_n\mA_n\mA_{n0}^{-1}\vepsi_n \right)^\top \left((\rho_0 - \rho)\mM_n\mC_{n0}\mX_n\vbeta_0 + \mM_n\mA_n\mA_{n0}^{-1}\vepsi_n \right), \\
 = & \frac{1}{n}(\rho_0 - \rho)^2(\mC_{n0}\mX_n\vbeta_0)^\top\mM_n(\mC_{n0}\mX_n\vbeta_0) + 2(\rho_0 - \rho)\underbrace{\frac{1}{n}(\mC_{n0}\mX_n\vbeta_0)^\top\mM_n\mA_n\mA_{n0}^{-1}\vepsi_n}_{B_{n1}} +  \\
& \underbrace{\frac{1}{n}\vepsi_n^\top(\mA_{n0}^\top)^{-1}\mA_n^\top\mM_n\mA_n\mA_{n0}^{-1}\vepsi_n}_{B_{n2}} \\
= & \sigma^{2*}_n + B_{n1} + (B_{n2} - \sigma^2_n(\lambda)), 
\end{aligned}
\end{equation*}
%
where the last equality comes from Equation \eqref{eq:sigma-star-consistency}. 

It can be shown that 
\begin{equation}
    \begin{aligned}
      \frac{1}{n}\mX_n^\top\mC_n^\top\vepsi_n \pto \vzeros \\
      \frac{1}{n}\mX_n^\top\mC_n^\top\mC_n\vepsi_n \pto \vzeros. 
    \end{aligned}
\end{equation}

This implies that $\frac{1}{n}\mX_n^\top\mC_n^\top\vepsi_n  =o_p(1)$ and $\frac{1}{n}\mX_n^\top\mC_n^\top\mC_n\vepsi_n = o_p(1)$.

$B_{n1}$ can be expanded as
\begin{equation*}
\begin{aligned}
B_{n1}  = &  \frac{1}{n}(\mC_{n0}\mX_n\vbeta_0)^\top\mM_n\mA_n\mA_{n0}^{-1}\vepsi_n \\
        = & \frac{1}{n}(\mC_{n0}\mX_n\vbeta_0)^\top\mM_n\left(\mI_n + (\rho_0 - \rho)\mC_n (\rho_0)\right)\vepsi_n \\
        = & \frac{1}{n}(\mC_{n0}\mX_n\vbeta_0)^\top\mM_n\vepsi_n + (\rho_0 - \rho)\frac{1}{n}(\mC_{n0}\mX_n\vbeta_0)^\top\mM_n\mC_n(\rho_0)\vepsi_n 
\end{aligned} 
\end{equation*}

The first and second moments of the first element of $B_{n1}$ are
\begin{equation*}
\begin{aligned}
\E\left[\frac{1}{n}(\mC_{n0}\mX_n\vbeta_0)^\top\mM_n\vepsi_n\right] & = \frac{1}{n}(\mC_{n0}\mX_n\vbeta_0)^\top\mM_n\E(\vepsi_n) = \vzeros \\
\var\left[\frac{1}{n}(\mC_{n0}\mX_n\vbeta_0)^\top\mM_n\vepsi_n\right] & = \frac{1}{n^2}(\mC_{n0}\mX_n\vbeta_0)^\top\mM_n\E(\vepsi_n\vepsi_n\top)\mM_n^\top(\mC_{n0}\mX_n\vbeta_0) = o(1)
\end{aligned}
\end{equation*}

Since $(\mC_{n0}\mX_n\vbeta_0)^\top\mM_n$ is uniformly bounded, then by Chebyshev's inequality \ref{definition:chebyshev_ineq} $n^{-1}(\mC_{n0}\mX_n\vbeta_0)^\top\mM_n\vepsi_n\pto \vzeros$. A similar reasoning can be applied to the second element of element of $B_{n1}$. Then
\begin{equation*}
  B_{n1} =  o_p(1) + o_p(1) = o_p(1)\quad \mbox{uniformly in $\rho \in \Gamma$.}
\end{equation*}

$B_{n2}$ can be expanded as
\begin{equation*}
\begin{aligned}
B_{n2} - \sigma^2_n(\lambda) = & \frac{1}{n}\vepsi_n^\top(\mA_{n0}^\top)^{-1}\mA_n^\top\mM_n\mA_n\mA_{n0}^{-1}\vepsi_n -  \frac{1}{n}\sigma_0^2\tr(\mD_n(\rho_0, \rho))\\
= & \frac{1}{n}\vepsi_n^\top(\mA_{n0}^\top)^{-1}\mA_n^\top(\mI_n - \mP_{nX})\mA_n\mA_{n0}^{-1}\vepsi_n -  \frac{1}{n}\sigma_0^2\tr(\mD_n(\rho_0, \rho)) \\
= & \left[\frac{1}{n}\vepsi_n^\top(\mA_{n0}^\top)^{-1}\mA_n^\top\mA_n\mA_{n0}^{-1}\vepsi_n -   \frac{1}{n}\sigma_0^2\tr(\mD_n(\rho_0, \rho)\right] - \frac{1}{n}\vepsi_n^\top(\mA_{n0}^\top)^{-1}\mA_n^\top\mP_{nX}\mA_n\mA_{n0}^{-1}\vepsi_n 
\end{aligned}
\end{equation*}

Using Lemma \ref{lemma:second-mom-lee} to the first element of $B_{n2}$ yields
\begin{equation*}
\begin{aligned}
\E\left[\frac{1}{n}\vepsi_n^\top(\mA_{n0}^\top)^{-1}\mA_n^\top\mA_n\mA_{n0}^{-1}\vepsi_n\right] &  = \frac{1}{n}\sigma_0^2\tr(\mD_{n}(\rho_0, \rho)) \\
\var\left[\frac{1}{n}\vepsi_n^\top(\mA_{n0}^\top)^{-1}\mA_n^\top\mA_n\mA_{n0}^{-1}\vepsi_n\right]& = \frac{1}{n^2}\sigma_0^4\left[\tr\left[(\mA_{n0}^\top)^{-1}\mA_n^\top\mA_n\mA_{n0}^{-1})(\mA_{n0}^\top)^{-1}\mA_n^\top\mA_n\mA_{n0}^{-1})^\top\right] +  \right. \\
& \left. \tr\left[(\mA_{n0}^\top)^{-1}\mA_n^\top\mA_n\mA_{n0}^{-1})^2\right]\right] = o(1)
\end{aligned}
\end{equation*}

Then by Theorem \ref{teo:chebyshev}, $\frac{1}{n}\vepsi_n^\top(\mA_{n0}^\top)^{-1}\mA_n^\top\mA_n\mA_{n0}^{-1}\vepsi_n\pto \frac{1}{n}\sigma_0^2\tr(\mD_{n}(\rho_0, \rho))$. The second element can be written as
\begin{equation*}
\frac{1}{n}\left(\underbrace{\frac{1}{\sqrt{n}}\mX_n^\top\mA_n\mA_{n0}^{-1}\vepsi}_{O_p(1)}\right)^\top\left(\underbrace{\frac{\mX_n^\top\mX_n}{n}}_{O(1)}\right)^{-1}\left(\underbrace{\frac{1}{\sqrt{n}}\mX_n^\top\mA_n\mA_{n0}^{-1}\vepsi}_{O_p(1)}\right) = n^{-1}O_p(n^0) = O_p(n^{1})= o_p(1)
\end{equation*}
%
where we use the fact that if $Z_n = O(n^k)$ then $Z_n = o_p(n^{k + \delta})$ for all $\delta>0$. Then $B_{n2}  - \sigma^2_n(\lambda) = o_p(1)$ uniformly in $\rho \in \Gamma$. Then 
\begin{equation*}
\widetilde{\sigma}(\rho)  \pto \sigma^{2*}_n
\end{equation*}

Consequently, 
\begin{equation*}
\begin{aligned}
\ln( \widetilde{\sigma}^{2}) - \ln(\sigma^{2*}_n(\rho)) & = \ln\left(\frac{\widetilde{\sigma}^{2}}{\sigma^{2*}_n(\rho)}\right) \\
& = \ln\left(\frac{\sigma^{2*}_n + B_{n1} + (B_{n2} - \sigma^2_n(\lambda)) }{\sigma^{2*}_n(\rho)}\right) \\
& = \ln\left(1 + \frac{B_{n1}) }{\sigma^{2*}_n(\rho)} + \frac{(B_{n2} - \sigma^2_n(\lambda)) }{\sigma^{2*}_n(\rho)}\right) \\
& = o_p(1)
\end{aligned}
\end{equation*}
%
and
\begin{equation*}
\sup_{\rho\in \Gamma} \left\lbrace \frac{1}{n}\left|  \ell_n(\rho) - Q_n(\rho)\right|\right\rbrace = o_p(1)
\end{equation*}

\paragraph{Identification}

Here, we need to show that for any $\epsilon>0$, $\lim\sup_{n \to \infty}\left[\max_{\rho\in\bar{N}_{\epsilon}(\rho_0)}\frac{1}{n}Q_n(\rho) - \frac{1}{n}Q_n(\rho_0)\right]<0$, where $\bar{N}_{\epsilon}(\rho_0)$ is the complement of an open neighborhood of $\rho$ in $\Gamma$ with radius $\epsilon$.

Consider the log-likelihood function of a pure SLM process $\vy_n = \rho\mW_n\vy_n + \vepsi_n$, where $\vepsi_n \sim \rN(\vzeros, \sigma_0^2\mI_n)$ is
\begin{equation*}
\ell_{p, n}(\rho, \sigma^2)= -\frac{n}{2}\ln(2\pi) - \frac{n}{2}\ln\sigma^2 + \ln\left|\mA_n(\rho)\right|-\frac{1}{2\sigma^2}\vy_n^\top\mA_n(\rho)^\top\mA_n(\rho)\vy_n.
\end{equation*}

Consider $Q_{p,n}(\rho) = \max_{\sigma^2}\E_p\left[\ell_{p, n}(\rho, \sigma^2)\right]$. Then
\begin{equation*}
\begin{aligned}
  Q_{p,n}(\rho)  = &\max_{\sigma^2} \E_p\left[\ell_{p, n}(\rho, \sigma^2)\right] \\
                 \leq & \E_p\left[\max_{\sigma^2}\ell_{p, n}(\rho, \sigma^2)\right],\quad \mbox{by Jensen's inequality} \\
                  = & Q_{p, n}(\rho_0), \quad \mbox{for all $\rho$}. 
\end{aligned}
\end{equation*}

This implies that 
\begin{equation*}
  \frac{1}{n}\left(Q_{p,n}(\rho) - Q_{p, n}(\rho_0)\right)\leq 0 \quad \mbox{for all $\rho$}. 
\end{equation*}

It is also clear that:
\begin{equation}
\ln(\sigma_n^2(\rho)) \leq \ln\sigma_n^{2*}(\rho)
\end{equation}

At $\rho_0, \sigma^{2*}_n(\rho) = \sigma_0^2$ (see Equation \eqref{eq:sigma-star-consistency}). Then, 
\begin{equation*}
\begin{aligned}
\frac{1}{n}Q_n(\rho) - \frac{1}{n}Q_n(\rho_0) & = -\frac{1}{2}(\ln \sigma_n(\rho) - \ln \sigma_0^2) + \frac{1}{n}\left(\ln \left|\mA_n\right| - \ln \left|\mA_0\right|\right)- \frac{1}{2}\left[\ln \sigma_n^{2*}(\rho) - \ln \sigma_n^2(\rho)\right] \\
& = \frac{1}{n}\left(Q_{p,n}(\rho) - Q_{p, n}(\rho_0)\right)- \frac{1}{2}\left[\ln \sigma_n^{2*}(\rho) - \ln \sigma_n^2(\rho)\right] 
\end{aligned}
\end{equation*}

It follows that 
\begin{equation*}
\frac{1}{n}Q_n(\rho) - \frac{1}{n}Q_n(\rho_0) \leq 0.
\end{equation*}




%Note also that 
%\begin{equation}
%\begin{aligned}
%\mP(\rho_0, \rho)  & = \left[\mI_n + (\rho_0 - \rho)\mC_n(\rho_0)\right]^\top\left[\mI_n + (\rho_0 - \rho)\mC_n%(\rho_0)\right] \\
%& = \mI + 2(\rho_0 - \rho)\mC_n(\rho_0) + (\rho_0 - \rho)^2\mC_n(\rho_0) \mC_n(\rho_0)^\top
%\end{aligned}
%\end{equation}


%It is also important to note that:

%\begin{equation}
%\begin{aligned}
%\sigma^2(\rho_0, \rho) & = \sigma_0^2\tr\left[\mP_n(\rho_0, \rho)\right] \\
%                       & = \sigma_0^2\tr\left[\mI_n + 2(\rho_0 - \rho)\mC_n(\rho_0) + (\rho_0 - \rho)^2\mC_n(\rho_0) %\mC_n(\rho_0)^\top\right] \\
%                       & = \sigma_0^2\left[1 + 2(\rho_0 - \rho)\tr(\mC_n(\rho_0)) + (\rho_0 - \rho)^2\tr(\mC_n(\rho_0) \mC_n(\rho_0)^\top)\right]
%\end{aligned}
%\end{equation}

%-------------------------------------------------------------------------
 \section{Expected Value of Hessian for SLM}\label{appendix-EH-sml}
%-------------------------------------------------------------------------

In this Section we drop the subindex $n$ and use the results from Section \ref{sec:Hessian}. The following definitions and relations for the Spatial Lag Model are very useful:

Since $\vepsi  =  \mA_0\vy - \mX\vbeta_0$, it follows that
\begin{align*}
	\E\left(\vepsi\right)      & = \vzeros \\
	\E\left(\vepsi\vepsi^\top\right) &= \sigma^2_0\mI_n, \\
	\E\left(\vepsi^\top\vepsi\right) &= \E(\tr(\vepsi^\top\mI_n\vepsi)) = n\E(\vepsi\vepsi^\top)
\end{align*}

Using Lemma \ref{lemma:second-mom-lee} for the expectation of quadratic forms, yields
\begin{equation}
\begin{aligned}
	\vy & =  \mA_0^{-1}\mX\vbeta_0 + \mA_0^{-1}\vepsi, \\
	\E\left(\vy\right)   & = \vmu_{\vu} = \mA_0^{-1}\mX\vbeta_0, \\
	\E\left(\vy^\top\vy\right) & = \mSigma_{\vy} = \mA_0^{-1}\sigma^2_0\mI_n(\mA_0^{-1})^\top, \\
	\E\left(\vy\vy^\top\right) & = \left(\mA_0^{-1}\mX\vbeta_0\right) \left(\mA_0^{-1}\mX\vbeta_0\right)^\top + \mA_0^{-1}\sigma^2_0\mI_n(\mA_0^{-1})^\top.
\end{aligned}
\end{equation}

We now derive the most difficult expectations. From \eqref{eq:sar_der_beta_rho} and letting $\mC_0 = \mW\mA^{-1}_0$
\begin{equation}\label{eq:E_der_beta_rho}
\begin{aligned}
\E\left(\frac{\partial^2  \ell(\vtheta_0)}{\partial \vbeta \partial \rho}\right) & = - \frac{1}{\sigma^2_0} \mX^\top\E\left(\mW\vy\right) \\
& = - \frac{1}{\sigma^2_0} \mX^\top\mW\mA^{-1}_0\mX\vbeta_0 \\
& = - \frac{1}{\sigma^2_0} \mX^\top(\mC_0\mX\vbeta_0)
\end{aligned}	
\end{equation}

For \eqref{eq:sar_der_sigma_sigma} we obtain:
\begin{equation}\label{eq:E_der_sigma_sigma}
  \begin{aligned}
\E\left(\frac{\partial^2 \ell(\vtheta_0)}{\partial (\sigma^2)^2}\right) & = \E\left[\frac{n}{2(\sigma^2_0) ^2} - \frac{1}{(\sigma^2_0)^3} \vepsi^\top \vepsi\right] \\
& = \frac{n}{2\sigma^4_0} - \frac{1}{\sigma^6_0}\E\left[\vepsi^\top \vepsi\right] \\
& = \frac{n}{2\sigma^4_0} - \frac{n}{\sigma^6_0} \sigma^2_0 \mI_n \\
& = - \frac{n}{2\sigma^4_0} 
  \end{aligned}
\end{equation}

From (\ref{eq:sar_der_sigma_rho}):
\begin{equation}\label{eq:E_der_sigma_rho}
	\begin{aligned}
	\E\left[\frac{\partial^2 \ell(\vtheta_0)}{\partial \sigma^2 \partial \rho}\right] &= \E\left[-\frac{\vepsi^\top\mW\vy}{\sigma^4_0}\right] \\
	& = - \frac{1}{\sigma^4_0}\E\left[\vepsi^\top\mW(\mA^{-1}_0\mX\vbeta_0 + \mA^{-1}_0\vepsi)\right] \\
	& = - \frac{1}{\sigma^4_0}\E\left[\vepsi^\top\mC_0\mX\vbeta_0 + \vepsi^\top\mC_0\vepsi\right] \\
	& = - \frac{1}{\sigma^4_0}\E\left[\vepsi^\top\mC_0\vepsi\right] \\
	& = - \frac{1}{\sigma^4_0}\tr\left(\mC_0\right) \E\left(\vepsi\vepsi^\top\right) \\
	& = - \tr(\mC_0)/\sigma^2_0
	\end{aligned}
\end{equation}

From \eqref{eq:sar_der_rho_rho}:
\begin{equation}\label{eq:E_der_rho_rho}
  \begin{aligned}
\E\left[\frac{\partial^2 \ell(\vtheta_0)}{\partial \rho ^2}\right]  & =  \E\left[- \tr\left[(\mW\mA_0^{-1})^2\right] - \frac{1}{\sigma^2_0}(\vy^{\top}\mW^{\top}\mW\vy)\right] \\
  & =  - \tr\left[(\mW\mA^{-1}_0)^2\right] - \frac{1}{\sigma^2_0} \E\left[\vy^{\top}\mW^{\top}\mW\vy\right] \\
  & = - \tr(\mC_0^2) -\frac{1}{\sigma^2_0} \left(\tr(\mW^{\top}\mW\mSigma_{\vy}) + \mu_{\vy}^\top\mW^{\top}\mW\vmu_{\vy}\right) \\
  & = - \tr(\mC_0^2) -\frac{1}{\sigma^2_0}\left(\sigma_0^2\tr(\mW^{\top}\mW\mA_0^{-1}(\mA_0^{-1})^\top) + (\mA_0^{-1}\mX\vbeta_0)^\top\mW^{\top}\mW\mA_0^{-1}\mX\vbeta_0\right) \\
 & = - \tr(\mC_0^2) - \frac{1}{\sigma^2_0}\left(\vbeta_0^\top\mX^\top\mC_0^\top\mC_0\mX\vbeta_0 + \sigma_0^2\tr(\mC_0^\top\mC_0)\right) \\
 & = - \tr(\mC_0^2) - \frac{1}{\sigma^2_0}(\mC_0\mX\vbeta_0)^\top(\mC_0\mX\vbeta_0) - \tr(\mC_0^\top\mC_0) \\
 & = - \tr(\mC_0^s\mC_0) - \frac{1}{\sigma^2_0}(\mC_0\mX\vbeta_0)^\top(\mC_0\mX\vbeta_0)
\end{aligned}
\end{equation}
%
where $\mC_0^s = \mC_0 + \mC_0^\top$. 

Let $\mH(\vtheta) = \frac{\partial \ell^2(\vtheta)}{\partial \vtheta \partial \vtheta^\top}$. Thus, minus the expected value of the Hessian evaluated at $\vtheta_0$ is:
\begin{equation}
	- \E\left[\mH(\vtheta_0)\right]= 
\begin{pmatrix}
	\frac{1}{n\sigma^2_0}(\mX^\top \mX)  & \frac{1}{\sigma^2_0} \mX^\top (\mC\mX\vbeta_0) & \vzeros^\top \\
	   & \tr(\mC^s_{0}\mC_{0}) + \frac{1}{\sigma^2_0}(\mC_{0}\mX\vbeta_0)^\top(\mC_{0}\mX\vbeta_0) &  \frac{1}{\sigma^2_0} \tr(\mC_{0}) \\
		 &  & \frac{n}{2\sigma^4_0}
	\end{pmatrix} 
\end{equation}

By multiplying $-\E\left[\mH(\vtheta_0)\right]$ by $(1/n)$, we obtain Equation \eqref{eq:expected-hessian-slm-asy}. 


% The asymptotic variance matrix follows as the inverse of the information matrix:
% 
% \begin{equation}\label{eq:asyvar_slm}
% 	\var(\vbeta, \sigma^2, \rho)= 
% 	\begin{pmatrix}
% 	\frac{1}{\sigma^2}(\mX^\top \mX) & \vzeros^\top & \frac{1}{\sigma^2} \mX^\top (\mC\mX\vbeta) \\
% 		 &  \frac{n}{2\sigma^4} & \frac{1}{\sigma^2} \tr(\mC)\\
% 		 &  & \tr(\mC^s\mC) + \frac{1}{\sigma^2}(\mC\mX\vbeta)^\top(\mC\mX\vbeta)
% 	\end{pmatrix} ^{-1}
% \end{equation}
% 
% An important feature is that the covariance between $\vbeta$ and the error variance is zero, as in the standard regression model, this is not the case for $\rho$ and the error variance. This lack of block diagonality in the information matrix for the spatial lag model will lead to some interesting results on the structure of specification test.
% 
% However, we can use the eigenvalues approximation. Recall that 
% 
% \begin{equation}
% \left(\frac{\partial }{\partial \rho}\right)\log \left|\mA \right| = -\sum_{i=1}^n\frac{\omega_i}{1-\rho\omega_i}, 
% \end{equation}
% %
% so that, 
% 
% \begin{equation*}
% 	\var(\vbeta, \sigma^2, \rho)= 
% 	\begin{pmatrix}
% 	\frac{1}{\sigma^2}(\mX^\top \mX) & \vzeros' & \frac{1}{\sigma^2} \mX^\top (\mC\mX\vbeta) \\
% 		. &  \frac{n}{2\sigma^4} & \frac{1}{\sigma^2} \tr(\mC)\\
% 		. &  .& \alpha + \tr(\mC\mC) + \frac{1}{\sigma^2}(\mC\mX\vbeta)^\top(\mC\mX\vbeta)
% 	\end{pmatrix} ^{-1}
% \end{equation*}
% %
% where $\alpha = \sum_{i=1}^n\left(\frac{\omega_i^2}{(1-\rho\omega_i)^2}\right)$. Note that while the covariance between $\vbeta$ and the error variance is zero, as in the standard regression model, this is not the case for $\rho$ and the error variance. 

%-------------------------------------------------------------------------
 \section{Variance of the Score Function}\label{appendix-score}
%-------------------------------------------------------------------------

In this Appendix, we will derive the variance of the score function. That is
\begin{equation}
\begin{aligned}
 \var\left[\frac{1}{\sqrt{n}}\frac{\partial \ell_n (\vtheta_0)}{\partial \vtheta}\right] = \E\left[\frac{1}{\sqrt{n}}\frac{\partial \ell_n (\vtheta_0)}{\partial \vtheta}\cdot\frac{1}{\sqrt{n}}\frac{\partial \ell_n (\vtheta_0)}{\partial \vtheta^\top}\right] & = \mJ_{\theta, n} = \mSigma_{\theta, n} + \mOmega_{\theta, n}
  \end{aligned}
\end{equation}
%
where $\mSigma_{\theta, n}= - \E\left[(1/n)\mH(\vtheta_0)\right]$ is given in Equation \eqref{eq:expected-hessian-slm-asy}. The following results are important. For i.i.d $\vepsi_n = (\epsilon_1, ..., \epsilon_n)$, it can be shown that
\begin{equation}
\begin{aligned}
\E(\epsilon_i\epsilon_j) & = 
\begin{cases}
  \sigma_0^2 & \mbox{for $i = j$}, \\
  0          &  \mbox{for $i\neq j$}
\end{cases} \\
\E(\epsilon_i\epsilon_j\epsilon_s) & =
\begin{cases}
  \mu_3 & \mbox{for $i = j = s$}, \\
  0          &  \mbox{otherwise}
\end{cases} \\
\E(\epsilon_i\epsilon_j\epsilon_s\epsilon_t) & =
\begin{cases}
  \mu_4 & \mbox{for $i = j = s = t$}, \\
   \sigma_0^4          &  \mbox{for $i=j \neq s=t$ or $i =s\neq j= t$ or $i = t \neq s = t$}, \\
   0 & \mbox{otherwise}
\end{cases} \\
\end{aligned}
\end{equation}

If $\epsilon_i \sim \rN(0, \sigma_0^2)$, we have $\E(\epsilon_i^3) = \mu_3 = 0$ and $\E(\epsilon_i^4) = \mu_4 = 3\sigma_0^4$. Thus
\begin{equation}\label{eq:moments-linear-cuadratic}
\begin{aligned}
  \E(\vepsi_n^\top\mA_n\vepsi_n) & = \E\left[\sum_{i = 1}^n\sum_{j = 1}^na_{ij}\epsilon_i\epsilon_j\right] = \sum_{i = 1}^na_{ii}\E(\epsilon_i\epsilon_i) = \sigma^2_0\sum_{i = i}a_{ii} = \sigma_0^2\tr(\mA_n), \\
  \E(\vepsi_n\vepsi_n^\top\mA_n\vepsi_n) & = \sum_{s = 1}^n\sum_{i = 1}^n\sum_{j = 1}^n a_{ij}\E(\epsilon_i\epsilon_j\epsilon_s) = \sum_i a_{ii}\E(\epsilon_i^3), \\
  & =  \mu_3\tr(\mA_n) \\
\E(\vepsi_n^\top\mA_n^\top\vepsi_n\vepsi_n^\top\mA^\top\vepsi_n) & = \sum_i\sum_j\sum_s\sum_ta_{ij}a_{st}\E(\epsilon_i\epsilon_j\epsilon_s\epsilon_t), \\
& = (\mu_4 - 3\sigma^4)\sum_{i = 1}^na_{ii}^2 + \sigma^4\left[\tr^2(\mA_n) + \tr(\mA_n\mA_n^\top)+\tr(\mA_n^2)\right], \\
& = (\mu_4 - 3\sigma^4)\sum_{i = 1}^na_{ii}^2 + \sigma^4\left[\tr^2(\mA_n) + \tr(\mA_n^s\mA_n)\right] 
\end{aligned}
\end{equation}
%
where $\mA_n^s = (\mA_n + \mA_n^\top)$.

Then, the expectation for the elements of $  \frac{1}{\sqrt{n}}\frac{\partial \ell_n (\vtheta_0)}{\partial \vtheta}\cdot\frac{1}{\sqrt{n}}\frac{\partial \ell_n (\vtheta_0)}{\partial \vtheta^\top}$ are
\begin{equation}
\begin{aligned}
\E\left[\left(\frac{1}{\sqrt{n}}\frac{\partial \ell_n(\vtheta_0)}{\partial \vbeta}\right) \left(\frac{1}{\sqrt{n}}\frac{\partial \ell_n(\vtheta_0)}{\partial \vbeta}\right)^\top\right] & = \frac{1}{\sigma_0^4}\frac{1}{n}\E(\mX_n^\top\vepsi\vepsi^\top\mX_n), \\
& = \frac{1}{\sigma_0^2}\frac{1}{n}\mX_n^\top\mX_n.
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\E\left[\left(\frac{1}{\sqrt{n}}\frac{\partial \ell_n(\vtheta_0)}{\partial \vbeta}\right) \left(\frac{1}{\sqrt{n}}\frac{\partial \ell_n(\vtheta_0)}{\partial \rho}\right)^\top\right]  = &  \frac{1}{\sigma_0^4}\frac{1}{n}\left[\E(\mX_n^\top\vepsi_n\vepsi_n^\top(\mC_{n0}\mX_n\vbeta_0)) + \E(\mX_n^\top\vepsi_n\vepsi_n^\top\mC_{n0}\vepsi_n) - \right. \\
& \left. \sigma_0^2\E(\mX_n^\top\vepsi_n\tr(\mC_{n0})) \right], \\
& = \frac{1}{\sigma_0^4}\frac{1}{n}\left[\mX_n^\top\E(\vepsi_n\vepsi_n^\top)(\mC_{n0}\mX_n\vbeta_0) + \mX_n^\top\E(\vepsi_n\vepsi_n^\top\mC_{n0}\vepsi_n)\right], \\
& = \frac{1}{\sigma_0^2}\frac{1}{n}\mX_n^\top(\mC_{n0}\mX_n\vbeta_0) + \frac{1}{\sigma_0^4}\frac{\mu_3}{n}\mX^\top\diag(\mC_{n0}).
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\E\left[\left(\frac{1}{\sqrt{n}}\frac{\partial \ell_n(\vtheta_0)}{\partial \vbeta}\right) \left(\frac{1}{\sqrt{n}}\frac{\partial \ell_n(\vtheta_0)}{\partial \sigma^2}\right)^\top\right] & = \frac{1}{\sigma_0^6}\frac{1}{n}\E\left[\frac{1}{2}\mX_n^\top\vepsi_n\vepsi_n^\top\vepsi_n - \frac{n\sigma_0^2}{2}\mX_n^\top\vepsi_n\right], \\
& = \frac{\mu_3}{\sigma_0^6}\frac{1}{2n}\mX_n^\top\vones_n.
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\E\left[\left(\frac{1}{\sqrt{n}}\frac{\partial \ell_n(\vtheta_0)}{\partial \sigma^2}\right) \left(\frac{1}{\sqrt{n}}\frac{\partial \ell_n(\vtheta_0)}{\partial \rho}\right)^\top\right] & =  \frac{1}{n\sigma_0^2}\tr(\mC_{n0})+ \frac{1}{2n\sigma_0^6}\left[(\mu_4 - 3\sigma_0^4)\tr(\mC_{n0}) + \mu_3\vones^\top(\mC_{n0}\mX_n\vbeta_0)\right] \\
\E\left[\left(\frac{1}{\sqrt{n}}\frac{\partial \ell_n(\vtheta_0)}{\partial \sigma^2}\right) \left(\frac{1}{\sqrt{n}}\frac{\partial \ell_n(\vtheta_0)}{\partial \sigma^2}\right)^\top\right] & = \frac{1}{2\sigma_0^4}+ \frac{(\mu_4 - 3\sigma_0^4)}{4\sigma_0^8}
\end{aligned}
\end{equation}

Note that:
\begin{align*}
\left(\frac{\partial \ell_n(\vtheta_0)}{\partial \rho}\right)\left(\frac{\partial \ell_n(\vtheta_0)}{\partial \rho}\right)^\top  = &  (\mC_{n0}\mX_n\vbeta_0)^\top \vepsi_n\vepsi_n^\top(\mC_{n0}\mX_n\vbeta_0) + (\vepsi_n^\top\mC_{n0}\vepsi)^2 + \sigma_{0}^4\tr^2(\mC_{n0}) \\
& + 2(\mC_{n0}\mX_n\vbeta_0)^\top \vepsi_n\vepsi_n^\top\mC_{n0}^\top\vepsi_n - 2\sigma_{0}^2\tr(\mC_{n0})(\mC_{n0}\mX_n\vbeta_0)^\top \vepsi_n  \\
& - 2\sigma_{0}^2\tr(\mC_{n0})\vepsi_n^\top\mC_{n0}\vepsi_n
\end{align*}
Taking the expectation for each element of this gives and given the results in Equation \eqref{eq:moments-linear-cuadratic}
\begin{align*}
 \E\left[(\mC_{n0}\mX_n\vbeta_0)^\top \vepsi_n\vepsi_n^\top(\mC_{n0}\mX_n\vbeta_0)\right] & = \sigma_0^2(\mC_{n0}\mX_n\vbeta_0)^\top(\mC_{n0}\mX_n\vbeta_0) \\
 \E\left[(\vepsi_n^\top\mC_{n0}\vepsi)^2\right] & = (\mu_4 - 3\sigma^4_0)\sum_{i = 1}^nc_{n,ii}^2 + \sigma^4_0\left[\tr^2(\mC_{n0}) + \tr(\mC_{n0}^s\mC_{n0})\right] \\
  \E\left[\sigma_{0}^4\tr^2(\mC_{n0})\right] & = \sigma_0^4\tr^2(\mC_{n0}) \\
  \E\left[2(\mC_{n0}\mX_n\vbeta_0)^\top \vepsi_n\vepsi_n^\top\mC_{n0}^\top\vepsi_n\right] & = 2\mu_3(\mC_{n0}\mX_n\vbeta_0)^\top\diag(\mC_{n0}) \\
  \E\left[2\sigma_{0}^2\tr(\mC_{n0})(\mC_{n0}\mX_n\vbeta_0)^\top \vepsi_n\right] & = \vzeros \\
  \E\left[2\sigma_{0}^2\tr(\mC_{n0})\vepsi_n^\top\mC_{n0}\vepsi_n\right] & = 2\sigma_0^4\tr^2(\mC_{n0})
\end{align*}

Using these results, we obtain 
\begin{equation}
\begin{aligned}
  \E\left[\left(\frac{1}{\sqrt{n}}\frac{\partial \ell_n(\vtheta_0)}{\partial \rho}\right) \left(\frac{1}{\sqrt{n}}\frac{\partial \ell_n(\vtheta_0)}{\partial \rho}\right)^\top\right] & = \frac{1}{\sigma_0^4n}\left[\sigma_0^2(\mC_{n0}\mX_n\vbeta_0)^\top(\mC_{n0}\mX_n\vbeta_0) - \sigma_0^4\tr^2(\mC_{n0})\right. \\
  & + \left. (\mu_4 - 3\sigma^4_0)\sum_{i = 1}^nc_{n,ii}^2 + \sigma^4_0\left[\tr^2(\mC_{n0}) + \tr(\mC_{n0}^s\mC_{n0})\right] \right. \\
  & + \left.2\mu_3(\mC_{n0}\mX_n\vbeta_0)^\top\diag(\mC_{n0})\right] \\
 & =   \frac{1}{\sigma_0^2}\frac{1}{n}(\mC_{n0}\mX_n\vbeta_0)^\top(\mC_{n0}\mX_n\vbeta_0) + \frac{1}{n}\tr(\mC_{n0}^s\mC_{n0}) \\
 &  + \frac{1}{\sigma_0^4}\frac{1}{n}(\mu_4 - 3\sigma^4_0)\sum_{i = 1}^nc_{ii,0}^2 + \frac{1}{\sigma_0^4}\frac{2\mu_3}{n}(\mC_{n0}\mX_n\vbeta_0)^\top\diag(\mC_{n0})
\end{aligned}
\end{equation}

Then, 
\begin{equation}
\mJ_{\theta, n} = 
\begin{pmatrix}
  \frac{1}{\sigma_0^2}\frac{1}{n}\mX_n^\top\mX_n & \frac{1}{n\sigma_0^2}\mX_n^\top(\mC_{n0}\mX_n\vbeta_0) + \frac{\mu_3}{n\sigma_0^4}\mX^\top\diag(\mC_{n0}) & \frac{\mu_3}{2n\sigma_0^6}\mX_n^\top\vones_n \\
  *  & \frac{1}{n\sigma_0^2}(\mC_{n0}\mX_n\vbeta_0)^\top(\mC_{n0}\mX_n\vbeta_0) + \frac{1}{n}\tr(\mC_{n0}^s\mC_{n0}) +  J_{22, n} & \frac{1}{n\sigma_0^2}\tr(\mC_{n0})+ J_{23,n} \\
  * & * & \frac{1}{2\sigma_0^4}+ \frac{(\mu_4 - 3\sigma_0^4)}{4\sigma_0^8}
\end{pmatrix}
\end{equation}
%
where 
\begin{align*}
J_{22, n} & = \frac{1}{\sigma_0^4}\frac{1}{n}(\mu_4 - 3\sigma^4_0)\sum_{i = 1}^nc_{ii,0}^2 + \frac{1}{\sigma_0^4}\frac{2\mu_3}{n}(\mC_{n0}\mX_n\vbeta_0)^\top\diag(\mC_{n0}), \\
J_{23, n} &= \frac{1}{2n\sigma_0^6}\left[(\mu_4 - 3\sigma_0^4)\tr(\mC_{n0}) + \mu_3\vones^\top(\mC_{n0}\mX_n\vbeta_0)\right].
\end{align*}

Thus, we can write $\mJ_{\theta, n} = \mSigma_{\theta, n} + \mOmega_{\theta, n}$ with
\scriptsize
\begin{equation}
\mOmega_{\theta, n} = 
\begin{pmatrix}
  \mZeros & \frac{\mu_3}{n\sigma_0^4}\mX^\top\diag(\mC_{n0}) & \frac{\mu_3}{2n\sigma_0^6}\mX_n^\top\vones_n \\
  * & \frac{1}{n\sigma_0^4}(\mu_4 - 3\sigma^4_0)\sum_{i = 1}^nc_{ii,0}^2 + \frac{2\mu_3}{n\sigma_0^4}(\mC_{n0}\mX_n\vbeta_0)^\top\diag(\mC_{n0}) & \frac{1}{2n\sigma_0^6}\left[(\mu_4 - 3\sigma_0^4)\tr(\mC_{n0}) + \mu_3\vones^\top(\mC_{n0}\mX_n\vbeta_0)\right] \\
  * & * & \frac{(\mu_4 - 3\sigma_0^4)}{4\sigma_0^8}
\end{pmatrix}.
\end{equation}
\normalsize


%-------------------------------------------------------------------------
 \section{Proof of Asymptotic Normality}\label{appendix-asymptotic normality}
%-------------------------------------------------------------------------

From Equation \eqref{eq:sampling-error-ml}, we know
\begin{equation*}
  \sqrt{n}(\widehat{\vtheta}_n - \vtheta_0) = - \left[\frac{1}{n}\frac{\partial^2 \ell_n(\widetilde{\vtheta}_n)}{\partial \vtheta \partial \vtheta^\top}\right]^{-1}\frac{1}{\sqrt{n}}\frac{\partial \ell_n(\vtheta_0)}{\partial \vtheta}.
\end{equation*}

The sketch consists in the following steps: 
\begin{enumerate}
  \item First, we need to show that:
    \begin{equation*}
    \mSigma_{\vtheta} = - \lim_{n\to\infty} \E\left[\frac{1}{n}\frac{\partial^2 \ell_n(\vtheta_0)}{\partial \vtheta \partial \vtheta^\top}\right],
    \end{equation*}
    %
    is non-singular. To show this is beyond the scope of this class notes. We will take this as given. 
  \item Now we will show that 
   \begin{equation*}
    \frac{1}{n}\frac{\partial^2 \ell_n(\widetilde{\vtheta}_n)}{\partial \vtheta \partial \vtheta^\top}\pto \frac{1}{n}\frac{\partial^2 \ell_n(\vtheta_0)}{\partial \vtheta \partial \vtheta^\top}
   \end{equation*}
   
   By Assumption \ref{assu:ml_6} (No asymptotic multicolinearity), we know that  $\lim_{n\to\infty}\frac{1}{n}\mX_n^\top\mX_n$ exists, therefore $\mX_n^\top\mX_n = O(n)$ so that $\mX_n^\top\mX_n/n = O(1)$ and $\widetilde{\sigma}^2_n\pto \sigma_0^2$ from consistency, then from Equation (\ref{eq:sar_der_beta_beta}), we have:\footnote{Since $\lim_{n\to\infty}\frac{1}{n}\mX_n^\top\mX_n$ exists, then, each of its elements is o(1) and hence O(1). In other words, $\frac{1}{n}\mX_n^\top\mX_n$ is a bounded matrix.} 
   \begin{equation*}
    \begin{aligned}
    \frac{1}{n}\frac{\partial^2 \ell_n(\widetilde{\vtheta}_n)}{\partial \vbeta \partial \vbeta^\top} - \frac{1}{n}\frac{\partial^2 \ell_n(\vtheta_0)}{\partial \vbeta \partial \vbeta^\top} & = - \frac{1}{\widetilde{\sigma}^2_n}\frac{(\mX^\top_n \mX_n)}{n} + \frac{1}{\sigma^2_0}\frac{(\mX^\top_n \mX_n)}{n}, \\
    & = \underbrace{\left(\frac{1}{\sigma^2_0} - \frac{1}{\widetilde{\sigma}^2_n}\right)}_{o_p(1)}\underbrace{\frac{(\mX^\top_n \mX_n)}{n}}_{O(1)}, \\
    & = o_p(1)O(1),  \\
    & = o_p(1).
    \end{aligned}
   \end{equation*}
   
   It can also be shown that
   \begin{equation}\label{eq:important-bounded-forms}
    \begin{aligned}
      \frac{1}{n}\mX_n^\top\mW_n^\top\vy_n & = \frac{1}{n}\mX_n^\top\mC_n\mX_n\vbeta_0 + o_p(1) = O_p(1),\\
      \frac{1}{n}\vy_n^{\top}\mW_n^{\top}\vepsi_n &= \frac{1}{n}\vepsi_n^\top\mC_n^{\top}\vepsi_n + o_p(1) = O_p(1),\\
      \frac{1}{n}\vy_n^\top\mW_n^\top\mW_n\vy_n & = \frac{1}{n}\left(\mX_n\vbeta_0\right)^\top\mC_n^\top\mC_n\mX_n\vbeta_0 + \frac{1}{n}\vepsi_n^\top\mC_n^\top\mC_n\vepsi_n + o_p(1) = O_p(1).
    \end{aligned}
   \end{equation}
   
   It follows from Equation \eqref{eq:sar_der_beta_rho}:
   \begin{equation*}
    \begin{aligned}
    \frac{1}{n}\frac{\partial^2 \ell_n(\widetilde{\vtheta}_n)}{\partial \vbeta \partial \rho} - \frac{1}{n}\frac{\partial^2 \ell_n(\vtheta_0)}{\partial \vbeta \partial \rho} & = - \frac{1}{\widetilde{\sigma}^2_n}\frac{(\mX^\top_n \mW_n\vy_n)}{n} + \frac{1}{\sigma^2_0}\frac{(\mX^\top_n \mW_n\vy_n)}{n}, \\
    & = \underbrace{\left(\frac{1}{\sigma^2_0} - \frac{1}{\widetilde{\sigma}^2_n}\right)}_{o_p(1)}\underbrace{\frac{(\mX^\top_n \mW_n\vy_n)}{n}}_{O_p(1)}, \\
    & = o_p(1)O_p(1)  = o_p(1).
    \end{aligned}
   \end{equation*} 
   
   The residuals as function of the true error term is:
   \begin{equation}\label{eq:error_slm_lee}
    \begin{aligned}
    \vepsi(\widetilde{\vdelta}_n) & = \vy_n - \mX_n\widetilde{\vbeta}_n - \widetilde{\rho}_n\mW_n\vy_n +(\vepsi(\vdelta_0) -  \vepsi(\vdelta_0)), \\
    & = \vy_n - \mX_n\widetilde{\vbeta}_n - \widetilde{\rho}_n\mW_n\vy_n - \vy_n + \mX_n\vbeta_0 - \rho_0\mW_n\vy_n + \vepsi_n(\vdelta_0), \\
    & = \mX_n(\vbeta_0 - \widetilde{\vbeta}_n) + (\rho_0 - \widetilde{\rho}_n)\mW_n\vy_n + \vepsi_n(\vdelta_0). 
    \end{aligned}
   \end{equation}
   
   Then, taking into account Equation \eqref{eq:sar_der_beta_sigma} and using our result in Equation (\ref{eq:error_slm_lee}) yields:
\begin{equation*}
    \begin{aligned}
    \frac{1}{n}\frac{\partial^2 \ell_n(\widetilde{\vtheta}_n)}{\partial \vbeta \partial \sigma^2} - \frac{1}{n}\frac{\partial^2 \ell_n(\vtheta_0)}{\partial \vbeta \partial \sigma^2}  = & - \frac{1}{\widetilde{\sigma}^4_n} \frac{\mX_n^\top \vepsi(\widetilde{\vdelta}_n)}{n}+  \frac{1}{\sigma^4_0} \frac{\mX^\top_n \vepsi_n(\vdelta_0)}{n}, \\
     = & - \frac{1}{\widetilde{\sigma}^4_n}\frac{1}{n}\mX_n^\top\left[\mX_n(\vbeta_0 - \widetilde{\vbeta}_n) + (\rho_0 - \widetilde{\rho}_n)\mW_n\vy_n + \vepsi_n(\vdelta_0) \right]+  \\
    & + \frac{1}{\sigma^4_0} \frac{\mX_n^\top \vepsi_n(\vdelta_0)}{n}, \\
   = & \frac{1}{\widetilde{\sigma}^4_n}\frac{1}{n}\mX_n^\top\mX_n(\vbeta_0 - \widetilde{\vbeta}_n) - \frac{1}{\widetilde{\sigma}^4_n}\frac{1}{n}\mX_n^\top\mW_n\vy_n(\rho_0 - \widetilde{\rho}_n)  \\
   & - \frac{1}{\widetilde{\sigma}^4_n}\frac{1}{n}\mX_n^\top \vepsi_n(\vdelta_0)  + \frac{1}{\sigma^4_0} \frac{\mX_n^\top \vepsi_n(\vdelta_0)}{n}, \\
   = & \left(\frac{1}{\sigma^4_0} - \frac{1}{\widetilde{\sigma}^4_n}\right)\frac{\mX^\top_n \vepsi_n(\vdelta_0)}{n} + \frac{\mX^\top_n\mX_n}{n\widetilde{\sigma}^4_n}(\vbeta_0 - \widetilde{\vbeta}_n)  \\
   & + \frac{\mW_n^\top\mW_n\vy_n}{n\widetilde{\sigma}^4_n}(\rho_0 - \widetilde{\rho}_n), \\
   = & o_p(1)O_p(1) + O(1)o_p(1) + O_p(1)o_p(1), \\
   = & o_p(1).
    \end{aligned}
   \end{equation*}
   
From Equation \eqref{eq:sar_der_rho_rho}, we know that:
   \begin{equation*}
   \frac{\partial \ell_n(\vtheta) }{\partial \rho ^2} = - \tr\left[(\mC_n(\rho))^2\right] - \frac{1}{\sigma^2}(\vy_n^{\top}\mW^{\top}_n\mW_n\vy_n)\quad \mbox{where}\quad \mC_n(\rho) = \mW_n\mA_n(\rho)^{-1}.
   \end{equation*}
   
   From Mean Value Theorem around of  $\tr\left[(\mC_n(\widetilde{\rho}_n))^2\right]$ around $\rho_0$:
   \begin{equation*}
    \begin{aligned}
    \tr\left[(\mC_n(\widetilde{\rho}_n))^2\right] & = \tr\left[(\mC_n(\rho_0))^2\right]+ 2\tr\left[(\mC_n(\bar{\rho}))^3\right](\rho_0 - \widetilde{\rho}_n), \\
    \tr\left[(\mC_n(\widetilde{\rho}_n))^2\right] - \tr\left[(\mC_n(\rho_0))^2\right] & = 2\tr\left[(\mC_n(\bar{\rho}))^3\right](\rho_0 - \widetilde{\rho}_n).
    \end{aligned}
   \end{equation*}
   
Then:
\begin{equation*}
    \begin{aligned}
    \frac{1}{n}\frac{\partial^2 \ell_n(\widetilde{\vtheta}_n)}{\partial \rho^2} - \frac{1}{n}\frac{\partial^2 \ell_n(\vtheta_0)}{\partial\rho^2} & = \underbrace{2\frac{1}{n}\tr\left[(\mC_n(\bar{\rho}))^3\right]}_{O(1)} \underbrace{(\rho_0 - \widetilde{\rho}_n)}_{o_p(1)} + \underbrace{\left(\frac{1}{\sigma^2_0} - \frac{1}{\widetilde{\sigma}^2_n}\right)}_{o_p(1)}\underbrace{\frac{\vy_n^{\top}\mW^{\top}_n\mW_n\vy_n}{n}}_{O_p(1)}, \\
    & = o_p(1).
    \end{aligned}
\end{equation*}
   
   Note that $\mC_n(\bar{\rho})$ is uniformly bounded in row and column sums uniformly in a neighborhood of $\rho_0$ by  Assumption \ref{assu:ml_5} and \ref{assu:ml_7}. Note that $\tr\left[(\mC_n(\bar{\rho}))^3\right] = O(n)$.
   
  Considering Equation \eqref{eq:sar_der_sigma_rho}:
    \begin{equation*}
    \begin{aligned}
    \frac{1}{n}\frac{\partial^2 \ell_n(\widetilde{\vtheta}_n)}{\partial \sigma^2 \partial \rho} - \frac{1}{n}\frac{\partial^2 \ell_n(\vtheta_0)}{\partial \sigma^2 \partial \rho}& = -\frac{1}{\widetilde{\sigma}^4}\frac{1}{n}\vy_n^\top\mW_n^\top\vepsi_n(\widetilde{\vdelta}_n) + \frac{1}{\sigma^4}\frac{1}{n}\vy_n^\top\mW_n^\top\vepsi_n(\vdelta_n),\\
    & = -\frac{1}{\widetilde{\sigma}^4}\frac{1}{n}\vy_n^\top\mW_n^\top\left[\mX_n(\vbeta_0 - \widetilde{\vbeta}_n) + (\rho_0 - \widetilde{\rho}_n)\mW_n\vy_n + \vepsi_n(\vdelta_0)\right] + \\
    &  \frac{1}{\sigma^4}\frac{1}{n}\vy_n^\top\mW_n^\top\vepsi_n(\vdelta_n), \\
    &= -\frac{1}{\widetilde{\sigma}^4}\frac{1}{n}\vy_n^\top\mW_n^\top\mX_n(\vbeta_0 - \widetilde{\vbeta}_n)  + \frac{1}{\widetilde{\sigma}^4}\frac{1}{n}\vy_n^\top\mW_n^\top\mW_n\vy_n(\widetilde{\rho}_n-\rho_0) + \\
    & \left(\frac{1}{\sigma^4}-\frac{1}{\widetilde{\sigma}^4}\right)\frac{1}{n}\vy_n^\top\mW_n^\top\vepsi_{n}, \\
    & = o_p(1).
    \end{aligned}
   \end{equation*}
   
Note the following:
   \begin{equation*}
    \begin{aligned}
    \frac{1}{n}\vepsi(\widetilde{\vdelta})^\top \vepsi(\widetilde{\vdelta}) & = \left(\widetilde{\vbeta}_n - \vbeta_0\right)^\top\frac{\mX_n^\top\mX_n}{n} + (\widetilde{\rho}_n -\rho_0)^2\frac{\vy_n^\top\mW_n^\top\mW_n\vy_n}{n} + \frac{\vepsi_n^{\top}\vepsi_n}{n} + \\
    & 2(\widetilde{\rho}_n -\rho_0)\left(\widetilde{\vbeta}_n - \vbeta_0\right)^\top\frac{\mX_n^\top\mW_n\vy_n}{n} + 2\left(\vbeta_0-\widetilde{\vbeta}_n\right)^\top\frac{\mX_n^\top\vepsi_n}{n} + 2(\rho_0-\widetilde{\rho}_n)\frac{\vy_n^\top\mW_n^\top\vepsi_n}{n}, \\
    & = \frac{\vepsi_n^{\top}\vepsi_n}{n} + o_p(1).
    \end{aligned}
   \end{equation*}
   
   
Finally, considering the second derivative in Equation \eqref{eq:sar_der_sigma_sigma}
   \begin{equation*}
    \begin{aligned}
      \frac{1}{n}\frac{\partial^2 \ell_n(\widetilde{\vtheta})}{\partial (\sigma^2)^2} -	\frac{1}{n}\frac{\partial^2 \ell_n(\vtheta_0)}{\partial (\sigma^2)^2}  & = \frac{1}{2(\widetilde{\sigma}^2) ^2} - \frac{1}{(\widetilde{\sigma}^2)^3} \frac{1}{n}\vepsi(\widetilde{\vdelta})^\top \vepsi(\widetilde{\vdelta}) -\frac{1}{2(\sigma^2) ^2} + \frac{1}{(\sigma^2)^3} \frac{1}{n}\vepsi^\top \vepsi, \\
      & = \frac{1}{2}\left(\frac{1}{\widetilde{\sigma}^2) ^2}-\frac{1}{(\sigma^2) ^2}\right)- \frac{1}{(\widetilde{\sigma}^2)^3} \frac{1}{n}\vepsi(\widetilde{\vdelta})^\top \vepsi(\widetilde{\vdelta}) + \frac{1}{(\sigma^2)^3} \frac{1}{n}\vepsi^\top \vepsi, \\
      & = \frac{1}{2}\left(\frac{1}{\widetilde{\sigma}^2) ^2}-\frac{1}{(\sigma^2) ^2}\right)- \frac{1}{(\widetilde{\sigma}^2)^3} \left(\frac{\vepsi_n^{\top}\vepsi_n}{n} + o_p(1)\right)+ \frac{1}{(\sigma^2)^3} \frac{1}{n}\vepsi^\top \vepsi, \\
      & = \frac{1}{2}\left(\frac{1}{\widetilde{\sigma}^2) ^2}-\frac{1}{(\sigma^2) ^2}\right)+ \left(\frac{1}{(\sigma^2)^3} -\frac{1}{(\widetilde{\sigma}^2)^3}\right)\frac{\vepsi_n^{\top}\vepsi_n}{n} + o_p(1), \\
      & = o_p(1).
    \end{aligned}
   \end{equation*}
   
   \item Now, we need to show that:
   \begin{equation*}
     \frac{1}{n}\frac{\partial^2 \ell_n(\vtheta_0)}{\partial \vtheta \partial \vtheta^\top}\pto \E\left[\frac{1}{n}\frac{\partial^2 \ell_n(\vtheta_0)}{\partial \vtheta \partial \vtheta^\top}\right]
   \end{equation*}
   
   From Section \ref{sec:Hessian}, we know that 
\begin{equation}\label{eq:hessian_sml2}
\frac{1}{n}\frac{\partial^2 \ell_n(\vtheta_0)}{\partial \vtheta \partial \vtheta^\top}= 
	\begin{pmatrix}
	- \frac{1}{n\sigma^2_0}(\mX_n^\top \mX_n)  & - \frac{1}{n\sigma^2_0} \mX_n^\top \mW\vy_n & - \frac{1}{(n\sigma^2_0)^2} \mX_n^\top \vepsi_n\\
	& - \frac{1}{n}\tr\left[(\mW\mA_{n0}^{-1})^2\right] - \frac{1}{n\sigma^2_0}(\vy_n^{\top}\mW_n^{\top}\mW_n\vy_n) & -\frac{\vepsi_n^\top\mW_n\vy_n}{n\sigma^4_0} \\
		  & &  \frac{1}{2(\sigma^2_0) ^2} - \frac{1}{n(\sigma^2_0)^3} \vepsi_n^\top \vepsi_n &  \\
	\end{pmatrix} 
\end{equation}

Then, using the results in Equation \eqref{eq:important-bounded-forms}
\begin{equation}
\begin{aligned}
	- \frac{1}{n\sigma^2_0}(\mX_n^\top \mX_n) \pto \E\left[\frac{1}{n}\frac{\partial^2  \ell_n(\vtheta_0)}{\partial \vbeta \partial \vbeta^\top}\right] & = -\frac{1}{\sigma^2_0}\frac{1}{n}(\mX_n^\top\mX_n) \\
	- \frac{1}{n\sigma^2_0} \mX_n^\top \mW\vy_n \pto \E\left[\frac{1}{n}\frac{\partial^2  \ell_n(\vtheta_0)}{\partial \vbeta \partial \rho} \right] & = - \frac{1}{\sigma^2_0} \frac{1}{n}\mX_n^\top\mC_{n0}\mX_n\vbeta_0 \\
	- \frac{1}{(n\sigma^2_0)^2} \mX_n^\top \vepsi_n\pto \E\left[\frac{1}{n}\frac{\partial^2  \ell_n(\vtheta_0)}{\partial \vbeta \partial \sigma^2}\right] & = \vzeros \\
	- \frac{\tr\left[(\mW\mA_{n0}^{-1})^2\right]}{n} - \frac{(\vy_n^{\top}\mW_n^{\top}\mW_n\vy_n)}{n\sigma^2_0} \pto  \E\left[\frac{1}{n}\frac{\partial^2 \ell_n(\vtheta_0)}{\partial \rho ^2}\right] & = - \frac{\tr(\mC^s_{n0}\mC_{n0})}{n} - \frac{(\mC_{n0}\mX_n\vbeta_0)^\top(\mC_{n0}\mX_n\vbeta_0)}{n\sigma^2_0} \\
	-\frac{\vepsi_n^\top\mW_n\vy_n}{n\sigma^4_0} \pto  \E\left[\frac{1}{n}\frac{\partial^2 \ell_n(\vtheta_0)}{ \partial \rho \partial \sigma^2}\right] & = - \frac{1}{n}\tr(\mC_{n0})/\sigma^2_0 \\
	\frac{1}{2(\sigma^2_0) ^2} - \frac{1}{n(\sigma^2_0)^3} \vepsi_n^\top \vepsi_n \pto  \E\left[\frac{1}{n}\frac{\partial^2 \ell_n(\vtheta_0)}{\partial (\sigma^2_0)^2}\right] & = - \frac{1}{2\sigma^4_0} 
	\end{aligned}
\end{equation}
   
   All these expectations exist in the limit by Assumption \ref{assu:ml_8} and Lemma \ref{lemma:O-lemma-lee}. Then, by nonsingularity we can say that 
   \begin{equation*}
     \left[\frac{1}{n}\frac{\partial^2 \ell_n(\widetilde{\vtheta})}{\partial \vtheta \partial \vtheta^\top}\right]^{-1}\pto \E\left[\frac{1}{n}\frac{\partial^2 \ell_n(\vtheta_0)}{\partial \vtheta \partial \vtheta^\top}\right]^{-1}.
   \end{equation*}
   

   % Lemma \ref{lemma:second-mom-lee} implies that $\E(\vepsi_n^\top\mC_n^\top\vepsi_n)=\sigma_0^2\tr(\mC_n)$ and, under normality assumption, 
   % 
   % \begin{equation}
   % \var\left(\frac{1}{n}\vepsi_n^\top\mC_n^\top\vepsi_n\right) = \frac{\sigma_{0}^4}{n^2}\left[\tr\left(\mC_n\mC_n^\top\right) +\tr(\mC_n^2)\right] =O\left(\frac{1}{nh_n}\right)
   % \end{equation}
   % 
   % Similarly, $\E(\vepsi_n^\top\mC_n^\top\mC_n\vepsi_n)=\sigma_0^2\tr(\mC_n^\top\mC_n)$ and, under normality assumption, 
   % 
   % \begin{equation}
   % \var\left(\frac{1}{n}\vepsi_n^\top\mC_n^\top\mC_n\vepsi_n\right) =O\left(\frac{1}{nh_n}\right)
   % \end{equation}
   % 
   % By the law of large numbers $\frac{1}{n}\vepsi_n^\top\vepsi_n\pto \sigma^2_0$. With these properties the convergence results follows. 
   
   
   \item Recall that the first-order derivatives of the log-likelihood function at $\vtheta_0$ are given by:
   
  \begin{equation*}
   \frac{1}{\sqrt{n}}\frac{\partial \ell_n(\vtheta_0)}{\partial \vtheta} = \begin{pmatrix}
   \frac{1}{\sigma^2_0\sqrt{n}}\mX_n^\top\vepsi_n\\
   \frac{1}{2\sigma_0^4\sqrt{n}}\left(\vepsi_n'\vepsi_n - n\sigma_0^2\right) \\
   \frac{1}{\sigma_0^2\sqrt{n}}(\mC_n\mX_n\vbeta_0)^\top \vepsi_n + \frac{1}{\sigma_0^2\sqrt{n}}(\vepsi_n^\top\mC_n\vepsi_n - \sigma_0^2\tr(\mC_n))
   \end{pmatrix}
  \end{equation*}


As explained by \citet[][pag. 1905]{lee2004asymptotic}, these are linear and quadratic functions of $\vepsi_n$. In particular, the asymptotic distribution of $\frac{1}{\sqrt{n}}\frac{\partial \log L_n(\vtheta_0)}{\partial \vtheta}$ may be derived from central limit theorem for linear-quadratic forms. The matrix $\mC_n$ is uniformly bounded in row sums. As the elements of $\mX_n$ are bounded, the elements of $\mC_n\mX_n\vbeta_0$ for all $n$ are uniformly bounded by Lemma \ref{lemma:bounded_lemma}. With the existence of high order moments of $\epsilon$ in Assumption \ref{assu:ml_1}, the central limit theorem for quadratic forms of double arrays of \cite{kelejian2001asymptotic} can be applied. 

Then 
\begin{equation*}
  \frac{1}{\sqrt{n}}\frac{\partial \ell_n(\vtheta_0)}{\partial \vtheta} \dto \rN(\vzeros, \mSigma_0 + \mOmega_0)
\end{equation*}
%
where 
\begin{equation}
\mSigma_0 = \lim_{n\to \infty}\mSigma_{n, 0}\quad \mbox{and} \quad \mOmega_0 = \lim_{n\to \infty}\mOmega_{n, 0}
\end{equation}
\end{enumerate}

Then, the proof is complete by using Slutsky's theorem. 
\end{subappendices}

